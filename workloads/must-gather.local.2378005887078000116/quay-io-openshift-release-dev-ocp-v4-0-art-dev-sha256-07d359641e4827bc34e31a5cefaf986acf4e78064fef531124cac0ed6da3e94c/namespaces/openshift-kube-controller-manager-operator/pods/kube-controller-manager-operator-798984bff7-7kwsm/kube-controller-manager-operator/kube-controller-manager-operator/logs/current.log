2023-01-09T04:42:13.516796289Z I0109 04:42:13.516697       1 cmd.go:209] Using service-serving-cert provided certificates
2023-01-09T04:42:13.517270151Z I0109 04:42:13.517249       1 observer_polling.go:159] Starting file observer
2023-01-09T04:42:13.529808344Z I0109 04:42:13.529769       1 builder.go:262] kube-controller-manager-operator version 4.12.0-202301042354.p0.g9243e02.assembly.stream-9243e02-9243e022c42c6d55e1d97a15ed51831f6080984a
2023-01-09T04:42:13.806527002Z W0109 04:42:13.806494       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:13.806527002Z W0109 04:42:13.806512       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:13.809730409Z I0109 04:42:13.809699       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2023-01-09T04:42:13.809758062Z I0109 04:42:13.809744       1 shared_informer.go:255] Waiting for caches to sync for RequestHeaderAuthRequestController
2023-01-09T04:42:13.809812751Z I0109 04:42:13.809784       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-01-09T04:42:13.809825788Z I0109 04:42:13.809814       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:13.809835743Z I0109 04:42:13.809819       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2023-01-09T04:42:13.809870057Z I0109 04:42:13.809838       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:13.810223084Z I0109 04:42:13.810186       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:13.811899377Z I0109 04:42:13.811867       1 secure_serving.go:210] Serving securely on [::]:8443
2023-01-09T04:42:13.812714646Z I0109 04:42:13.812683       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2023-01-09T04:42:13.813420219Z I0109 04:42:13.813350       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock...
2023-01-09T04:42:13.826008080Z I0109 04:42:13.825970       1 leaderelection.go:258] successfully acquired lease openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock
2023-01-09T04:42:13.826177296Z I0109 04:42:13.826134       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator-lock", UID:"e614bd73-4c23-48fa-b416-02d3e662094e", APIVersion:"v1", ResourceVersion:"9277", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-controller-manager-operator-798984bff7-7kwsm_43af47d6-7678-4cc4-808c-203b22a0ff61 became leader
2023-01-09T04:42:13.826177296Z I0109 04:42:13.826162       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator-lock", UID:"71e91033-7208-4ecb-8445-8b4c69458192", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"9278", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' kube-controller-manager-operator-798984bff7-7kwsm_43af47d6-7678-4cc4-808c-203b22a0ff61 became leader
2023-01-09T04:42:13.828739443Z E0109 04:42:13.828712       1 static_resource_controller.go:208] missing informer for namespace "openshift-infra"; no dynamic wiring added, time-based only.
2023-01-09T04:42:13.829188902Z E0109 04:42:13.829168       1 static_resource_controller.go:214] missing informer for namespace "openshift-infra"; no dynamic wiring added, time-based only.
2023-01-09T04:42:13.833740709Z I0109 04:42:13.833688       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.836337537Z I0109 04:42:13.836295       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.836426548Z I0109 04:42:13.836390       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.837542354Z I0109 04:42:13.837512       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.837695166Z I0109 04:42:13.837666       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.837863892Z I0109 04:42:13.837827       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.841374363Z I0109 04:42:13.841349       1 base_controller.go:67] Waiting for caches to sync for GarbageCollectorWatcherController
2023-01-09T04:42:13.841565099Z I0109 04:42:13.841532       1 base_controller.go:67] Waiting for caches to sync for CertRotationController
2023-01-09T04:42:13.841604676Z I0109 04:42:13.841588       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2023-01-09T04:42:13.841708810Z I0109 04:42:13.841695       1 base_controller.go:67] Waiting for caches to sync for SATokenSignerController
2023-01-09T04:42:13.841782109Z I0109 04:42:13.841769       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2023-01-09T04:42:13.841831130Z I0109 04:42:13.841820       1 base_controller.go:67] Waiting for caches to sync for WorkerLatencyProfile
2023-01-09T04:42:13.841887108Z I0109 04:42:13.841858       1 base_controller.go:67] Waiting for caches to sync for PruneController
2023-01-09T04:42:13.841887108Z I0109 04:42:13.841877       1 base_controller.go:67] Waiting for caches to sync for GuardController
2023-01-09T04:42:13.841917452Z I0109 04:42:13.841891       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2023-01-09T04:42:13.841917452Z I0109 04:42:13.841907       1 base_controller.go:67] Waiting for caches to sync for NodeController
2023-01-09T04:42:13.841929049Z I0109 04:42:13.841916       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2023-01-09T04:42:13.842022482Z I0109 04:42:13.842004       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2023-01-09T04:42:13.842082043Z I0109 04:42:13.842060       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2023-01-09T04:42:13.842131213Z I0109 04:42:13.842109       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2023-01-09T04:42:13.842131213Z I0109 04:42:13.842125       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-01-09T04:42:13.842156453Z I0109 04:42:13.842138       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2023-01-09T04:42:13.842156453Z I0109 04:42:13.842144       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2023-01-09T04:42:13.842195597Z I0109 04:42:13.842086       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-controller-manager
2023-01-09T04:42:13.842254175Z I0109 04:42:13.842241       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2023-01-09T04:42:13.842301192Z I0109 04:42:13.842289       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2023-01-09T04:42:13.842404044Z I0109 04:42:13.842368       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2023-01-09T04:42:13.842808336Z I0109 04:42:13.842775       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:13.909940873Z I0109 04:42:13.909895       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:13.909969727Z I0109 04:42:13.909905       1 shared_informer.go:262] Caches are synced for RequestHeaderAuthRequestController
2023-01-09T04:42:13.910560949Z I0109 04:42:13.910524       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:13.942899579Z I0109 04:42:13.942861       1 base_controller.go:73] Caches are synced for CertRotationController 
2023-01-09T04:42:13.942899579Z I0109 04:42:13.942876       1 base_controller.go:73] Caches are synced for GarbageCollectorWatcherController 
2023-01-09T04:42:13.942899579Z I0109 04:42:13.942883       1 base_controller.go:110] Starting #1 worker of CertRotationController controller ...
2023-01-09T04:42:13.942899579Z I0109 04:42:13.942890       1 base_controller.go:110] Starting #1 worker of GarbageCollectorWatcherController controller ...
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942892       1 base_controller.go:73] Caches are synced for PruneController 
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942906       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942860       1 base_controller.go:73] Caches are synced for WorkerLatencyProfile 
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942917       1 base_controller.go:110] Starting #1 worker of WorkerLatencyProfile controller ...
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942919       1 base_controller.go:73] Caches are synced for SATokenSignerController 
2023-01-09T04:42:13.942932333Z I0109 04:42:13.942925       1 base_controller.go:110] Starting #1 worker of SATokenSignerController controller ...
2023-01-09T04:42:13.942958584Z I0109 04:42:13.942932       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-01-09T04:42:13.942958584Z I0109 04:42:13.942940       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2023-01-09T04:42:13.942958584Z I0109 04:42:13.942943       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-01-09T04:42:13.942958584Z I0109 04:42:13.942946       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2023-01-09T04:42:13.943079345Z I0109 04:42:13.943056       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2023-01-09T04:42:13.943079345Z I0109 04:42:13.943070       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2023-01-09T04:42:13.943243992Z I0109 04:42:13.943227       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-controller-manager 
2023-01-09T04:42:13.943257516Z I0109 04:42:13.943241       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-controller-manager controller ...
2023-01-09T04:42:13.943352666Z I0109 04:42:13.942914       1 base_controller.go:73] Caches are synced for NodeController 
2023-01-09T04:42:13.943364691Z I0109 04:42:13.943353       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2023-01-09T04:42:13.943857745Z I0109 04:42:13.943797       1 base_controller.go:73] Caches are synced for ConfigObserver 
2023-01-09T04:42:13.943903628Z I0109 04:42:13.943891       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2023-01-09T04:42:13.943937134Z I0109 04:42:13.943908       1 base_controller.go:73] Caches are synced for RevisionController 
2023-01-09T04:42:13.943963803Z I0109 04:42:13.943954       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2023-01-09T04:42:13.944795236Z I0109 04:42:13.944769       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.947683335Z E0109 04:42:13.947659       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.957676181Z E0109 04:42:13.957647       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.970425118Z E0109 04:42:13.970399       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.997354359Z E0109 04:42:13.997319       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.040236904Z E0109 04:42:14.040201       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.042186956Z I0109 04:42:14.042164       1 base_controller.go:73] Caches are synced for GuardController 
2023-01-09T04:42:14.042186956Z I0109 04:42:14.042173       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2023-01-09T04:42:14.042207635Z I0109 04:42:14.042187       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2023-01-09T04:42:14.042207635Z I0109 04:42:14.042178       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2023-01-09T04:42:14.042620393Z I0109 04:42:14.042593       1 base_controller.go:73] Caches are synced for InstallerController 
2023-01-09T04:42:14.042620393Z I0109 04:42:14.042605       1 base_controller.go:73] Caches are synced for InstallerStateController 
2023-01-09T04:42:14.042620393Z I0109 04:42:14.042610       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2023-01-09T04:42:14.042686675Z I0109 04:42:14.042610       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2023-01-09T04:42:14.042701331Z I0109 04:42:14.042598       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2023-01-09T04:42:14.042724961Z I0109 04:42:14.042713       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2023-01-09T04:42:14.043122253Z I0109 04:42:14.043096       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.043151417Z E0109 04:42:14.043137       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.044418779Z E0109 04:42:14.044401       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.044432120Z E0109 04:42:14.044420       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.044441401Z E0109 04:42:14.044434       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.044646202Z E0109 04:42:14.044632       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.048726583Z I0109 04:42:14.048691       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.048815658Z E0109 04:42:14.048798       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.052557734Z E0109 04:42:14.052536       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.052578339Z E0109 04:42:14.052554       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.052578339Z E0109 04:42:14.052565       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.059655082Z I0109 04:42:14.059621       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.059787466Z E0109 04:42:14.059758       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.060709175Z E0109 04:42:14.060685       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:42:14.061673508Z I0109 04:42:14.061650       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.061803275Z I0109 04:42:14.061780       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.061857595Z E0109 04:42:14.061791       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.062231897Z I0109 04:42:14.062208       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.069167951Z I0109 04:42:14.067876       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:14.073364388Z E0109 04:42:14.073328       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.073364388Z E0109 04:42:14.073348       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.073364388Z E0109 04:42:14.073358       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.080323956Z I0109 04:42:14.080290       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.080457950Z E0109 04:42:14.080439       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.081514454Z E0109 04:42:14.081485       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.082339425Z E0109 04:42:14.082299       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.082339425Z I0109 04:42:14.082302       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.082339425Z I0109 04:42:14.082315       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.082680220Z I0109 04:42:14.082659       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.088504584Z I0109 04:42:14.088461       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:14.103556815Z E0109 04:42:14.103527       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.103556815Z E0109 04:42:14.103547       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.103584337Z E0109 04:42:14.103557       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.103730978Z E0109 04:42:14.103715       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.123242498Z E0109 04:42:14.123214       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.146347653Z E0109 04:42:14.146307       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.146347653Z E0109 04:42:14.146338       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.146383852Z E0109 04:42:14.146352       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.146574863Z E0109 04:42:14.146558       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.162212850Z I0109 04:42:14.162160       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.162307340Z E0109 04:42:14.162285       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.215445265Z E0109 04:42:14.215407       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.215502470Z E0109 04:42:14.215491       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.215560898Z E0109 04:42:14.215533       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.215849351Z E0109 04:42:14.215828       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.230491700Z E0109 04:42:14.230454       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.230553763Z E0109 04:42:14.230542       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.230592353Z E0109 04:42:14.230582       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.242102740Z E0109 04:42:14.242065       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.242599159Z E0109 04:42:14.242568       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:42:14.245378956Z I0109 04:42:14.245334       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.245620517Z I0109 04:42:14.245603       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.245953050Z I0109 04:42:14.245919       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.252574549Z I0109 04:42:14.252525       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:14.287754346Z E0109 04:42:14.287718       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.484073492Z I0109 04:42:14.484021       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.484246811Z E0109 04:42:14.484225       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.570117539Z E0109 04:42:14.570074       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.570117539Z E0109 04:42:14.570109       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.570150625Z E0109 04:42:14.570126       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.578294982Z E0109 04:42:14.578262       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:14.579032747Z E0109 04:42:14.578984       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:14.579130463Z I0109 04:42:14.579104       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:14.579219823Z I0109 04:42:14.579198       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.579277626Z I0109 04:42:14.579248       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.584906048Z I0109 04:42:14.584871       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:14.611073585Z E0109 04:42:14.611043       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.642914241Z I0109 04:42:14.642880       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2023-01-09T04:42:14.643005472Z I0109 04:42:14.642962       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2023-01-09T04:42:14.666781878Z E0109 04:42:14.665921       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:14.666781878Z E0109 04:42:14.665952       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:14.666781878Z E0109 04:42:14.665977       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:14.666781878Z E0109 04:42:14.666290       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:15.038666126Z I0109 04:42:15.038626       1 request.go:601] Waited for 1.195670925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services?limit=500&resourceVersion=0
2023-01-09T04:42:15.189303278Z E0109 04:42:15.189256       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:15.189303278Z E0109 04:42:15.189295       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:15.189348126Z E0109 04:42:15.189312       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:15.204812884Z E0109 04:42:15.204775       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:42:15.205374911Z I0109 04:42:15.205348       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:15.205666996Z E0109 04:42:15.205646       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:15.205728144Z I0109 04:42:15.205705       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:15.205905014Z I0109 04:42:15.205875       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:15.214949390Z I0109 04:42:15.214900       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:15.215226385Z I0109 04:42:15.215174       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:15.219591191Z E0109 04:42:15.219566       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:15.222201858Z E0109 04:42:15.222178       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:15.222223820Z E0109 04:42:15.222198       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:15.222223820Z E0109 04:42:15.222209       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:15.231175377Z E0109 04:42:15.231149       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:15.231732794Z I0109 04:42:15.231705       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:15.232237153Z E0109 04:42:15.232053       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:15.232263222Z I0109 04:42:15.232067       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:15.232542455Z I0109 04:42:15.232523       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:15.238844783Z I0109 04:42:15.238813       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:15.242232129Z I0109 04:42:15.242206       1 base_controller.go:73] Caches are synced for TargetConfigController 
2023-01-09T04:42:15.242283688Z I0109 04:42:15.242271       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2023-01-09T04:42:15.242334123Z I0109 04:42:15.242283       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2023-01-09T04:42:15.242334123Z I0109 04:42:15.242321       1 base_controller.go:110] Starting #1 worker of KubeControllerManagerStaticResources controller ...
2023-01-09T04:42:15.242354546Z I0109 04:42:15.242255       1 base_controller.go:73] Caches are synced for BackingResourceController 
2023-01-09T04:42:15.242354546Z I0109 04:42:15.242338       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2023-01-09T04:42:15.253960492Z E0109 04:42:15.253934       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:15.765577014Z I0109 04:42:15.765523       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:15.765643300Z E0109 04:42:15.765623       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:16.220435582Z E0109 04:42:16.220399       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:16.220435582Z E0109 04:42:16.220429       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:16.220477450Z E0109 04:42:16.220444       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:16.220700954Z E0109 04:42:16.220683       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:16.438698084Z I0109 04:42:16.438658       1 request.go:601] Waited for 1.191034079s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T04:42:16.537483615Z E0109 04:42:16.537448       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:16.640875032Z I0109 04:42:16.640813       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:16.640982119Z E0109 04:42:16.640959       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:17.152022648Z E0109 04:42:17.151959       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:17.152633461Z E0109 04:42:17.152606       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:17.152685285Z E0109 04:42:17.152673       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:17.152976348Z E0109 04:42:17.152954       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:17.156054785Z E0109 04:42:17.156028       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:17.156110985Z E0109 04:42:17.156094       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:17.156125362Z E0109 04:42:17.156117       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:17.156349595Z E0109 04:42:17.156296       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:17.237277156Z E0109 04:42:17.237229       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:17.237378589Z E0109 04:42:17.237334       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:17.237378589Z E0109 04:42:17.237363       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:17.248186333Z E0109 04:42:17.247917       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:42:17.249445769Z E0109 04:42:17.249419       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:17.256600136Z I0109 04:42:17.254728       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:17.256600136Z I0109 04:42:17.256099       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:17.256808332Z I0109 04:42:17.256784       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:17.265729715Z I0109 04:42:17.264391       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:17.265729715Z I0109 04:42:17.264627       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:17.268130331Z E0109 04:42:17.268104       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:18.341275191Z E0109 04:42:18.341221       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:18.341275191Z E0109 04:42:18.341257       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:18.341339041Z E0109 04:42:18.341271       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:18.350391263Z E0109 04:42:18.350356       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:18.351413894Z E0109 04:42:18.351379       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:18.351505916Z I0109 04:42:18.351473       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:18.351590179Z I0109 04:42:18.351558       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:18.352167917Z I0109 04:42:18.352134       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:18.364178011Z I0109 04:42:18.364135       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:42:18.364748836Z I0109 04:42:18.364718       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nGarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:18.369331678Z E0109 04:42:18.369300       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:18.441301321Z I0109 04:42:18.441232       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:18.441364234Z E0109 04:42:18.441346       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:19.101077674Z E0109 04:42:19.101041       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:19.234352055Z E0109 04:42:19.234311       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:19.234352055Z E0109 04:42:19.234343       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:19.234395943Z E0109 04:42:19.234361       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:19.234688965Z E0109 04:42:19.234659       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:20.240569584Z I0109 04:42:20.240517       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:20.240759675Z E0109 04:42:20.240733       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:20.355151734Z E0109 04:42:20.355117       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:20.355151734Z E0109 04:42:20.355141       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:20.355190486Z E0109 04:42:20.355151       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:20.355411571Z E0109 04:42:20.355391       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:21.840800455Z I0109 04:42:21.840750       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:21.840946618Z E0109 04:42:21.840929       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:23.367928441Z E0109 04:42:23.367884       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:23.367928441Z E0109 04:42:23.367921       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:23.367969585Z E0109 04:42:23.367940       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:23.368343493Z E0109 04:42:23.368269       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:23.441056722Z I0109 04:42:23.440973       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:23.441170377Z E0109 04:42:23.441148       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:24.224815641Z E0109 04:42:24.224781       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:24.640686886Z I0109 04:42:24.640628       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:24.640782807Z E0109 04:42:24.640763       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:25.229101268Z E0109 04:42:25.229061       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.229101268Z E0109 04:42:25.229091       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.229137470Z E0109 04:42:25.229106       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.229419337Z E0109 04:42:25.229392       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.509019606Z E0109 04:42:25.508964       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.509045186Z E0109 04:42:25.509025       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.509056225Z E0109 04:42:25.509044       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.509406524Z E0109 04:42:25.509364       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.840860840Z I0109 04:42:25.840803       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:25.840962229Z E0109 04:42:25.840943       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:26.840440700Z I0109 04:42:26.840385       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:26.840551778Z E0109 04:42:26.840532       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:27.641315381Z I0109 04:42:27.641266       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:27.641505571Z E0109 04:42:27.641486       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:28.363324319Z E0109 04:42:28.363249       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:28.363324319Z E0109 04:42:28.363286       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:28.363324319Z E0109 04:42:28.363303       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:28.363923795Z E0109 04:42:28.363543       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:28.441018148Z I0109 04:42:28.440951       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:28.441161080Z E0109 04:42:28.441143       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:29.640672116Z I0109 04:42:29.640617       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:29.640755737Z E0109 04:42:29.640735       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:34.469653257Z E0109 04:42:34.469611       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:34.768176577Z I0109 04:42:34.768120       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:34.768347622Z E0109 04:42:34.768325       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:36.246548572Z I0109 04:42:36.246495       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: client-ca
2023-01-09T04:42:36.246590432Z E0109 04:42:36.246566       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: configmaps: client-ca
2023-01-09T04:42:40.906603362Z E0109 04:42:40.906566       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:40.906603362Z E0109 04:42:40.906594       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:40.906650668Z E0109 04:42:40.906608       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:40.906855295Z E0109 04:42:40.906837       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:42.883792469Z E0109 04:42:42.883752       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:42.883879252Z E0109 04:42:42.883853       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:42.883901616Z E0109 04:42:42.883878       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:42.884209561Z E0109 04:42:42.884184       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:43.776497849Z I0109 04:42:43.776440       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:43.776533275Z E0109 04:42:43.776515       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:44.623325372Z E0109 04:42:44.623285       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:44.623325372Z E0109 04:42:44.623312       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:44.623361534Z E0109 04:42:44.623328       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:44.623595171Z E0109 04:42:44.623573       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:45.774297060Z I0109 04:42:45.774244       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:42:45.774398945Z E0109 04:42:45.774384       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:54.955147458Z E0109 04:42:54.955107       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:57.980856673Z I0109 04:42:57.980800       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/client-ca -n openshift-kube-controller-manager because it was missing
2023-01-09T04:42:57.981573944Z E0109 04:42:57.981521       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:42:57.981855969Z I0109 04:42:57.981812       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:01.729884808Z I0109 04:43:01.729830       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:01.730015902Z E0109 04:43:01.729978       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:04.131597398Z I0109 04:43:04.131537       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/serviceaccount-ca -n openshift-kube-controller-manager:
2023-01-09T04:43:04.131597398Z cause by changes in data.ca-bundle.crt
2023-01-09T04:43:04.135820994Z I0109 04:43:04.135722       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "configmap/serviceaccount-ca has changed"
2023-01-09T04:43:04.731164528Z I0109 04:43:04.731113       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:05.000739831Z I0109 04:43:05.000707       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca","reason":"GarbageCollector_Error::GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:05.007177436Z I0109 04:43:05.007129       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded changed from False to True ("GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca")
2023-01-09T04:43:05.358238189Z I0109 04:43:05.358165       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-manager-pod-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:05.933128381Z I0109 04:43:05.933079       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:06.530427063Z I0109 04:43:06.530373       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/cluster-policy-controller-config-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:07.141765737Z I0109 04:43:07.141714       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/controller-manager-kubeconfig-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:07.729935659Z I0109 04:43:07.729882       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-cert-syncer-kubeconfig-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:08.331494829Z I0109 04:43:08.331428       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:08.931165758Z I0109 04:43:08.931125       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/service-ca-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:09.530878471Z I0109 04:43:09.530829       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/recycler-config-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:10.131142819Z I0109 04:43:10.131079       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/service-account-private-key-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:10.930917620Z I0109 04:43:10.930844       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:11.528470777Z I0109 04:43:11.528418       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:11.528580962Z E0109 04:43:11.528561       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:11.731625505Z I0109 04:43:11.731570       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-5 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:11.739833542Z I0109 04:43:11.739787       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 4 created because configmap/serviceaccount-ca has changed
2023-01-09T04:43:11.741006106Z I0109 04:43:11.740953       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:11.741867361Z I0109 04:43:11.741831       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "configmap/serviceaccount-ca has changed"
2023-01-09T04:43:11.743314398Z W0109 04:43:11.743296       1 staticpod.go:38] revision 5 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:43:11.751819772Z E0109 04:43:11.751796       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 5
2023-01-09T04:43:11.752292448Z I0109 04:43:11.752251       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:11.752850785Z I0109 04:43:11.752829       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nRevisionControllerDegraded: conflicting latestAvailableRevision 5","reason":"GarbageCollector_Error::GuardController_SyncError::InstallerController_Error::RevisionController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:11.760107747Z I0109 04:43:11.759595       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nRevisionControllerDegraded: conflicting latestAvailableRevision 5"
2023-01-09T04:43:11.763200061Z I0109 04:43:11.763167       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:11.764019497Z I0109 04:43:11.763956       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca","reason":"GarbageCollector_Error::GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:11.770048557Z I0109 04:43:11.770008       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca\nRevisionControllerDegraded: conflicting latestAvailableRevision 5" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca"
2023-01-09T04:43:12.728437490Z I0109 04:43:12.728392       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal static pod not found and needs new revision 5
2023-01-09T04:43:12.728481405Z I0109 04:43:12.728440       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:12.728481405Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:43:12.728481405Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:12.728481405Z  TargetRevision: (int32) 5,
2023-01-09T04:43:12.728481405Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:12.728481405Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:12.728481405Z  LastFailedReason: (string) "",
2023-01-09T04:43:12.728481405Z  LastFailedCount: (int) 0,
2023-01-09T04:43:12.728481405Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:12.728481405Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:12.728481405Z }
2023-01-09T04:43:12.737418056Z I0109 04:43:12.737359       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 0 to 5 because node ip-10-0-199-219.us-east-2.compute.internal static pod not found
2023-01-09T04:43:12.738188423Z I0109 04:43:12.738148       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:12.738729376Z I0109 04:43:12.738688       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca","reason":"GarbageCollector_Error::GuardController_SyncError::InstallerController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:12.745093708Z I0109 04:43:12.745041       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5"
2023-01-09T04:43:12.762903213Z I0109 04:43:12.762864       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:12.763464255Z I0109 04:43:12.763431       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:12.769245342Z I0109 04:43:12.769202       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: configmaps: client-ca" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:12.926806500Z I0109 04:43:12.926772       1 request.go:601] Waited for 1.185699682s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:43:13.529259362Z I0109 04:43:13.529201       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:13.529416854Z E0109 04:43:13.529395       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:13.562842502Z E0109 04:43:13.562802       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:13.562842502Z E0109 04:43:13.562836       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:13.562904492Z E0109 04:43:13.562853       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:13.563147438Z E0109 04:43:13.563129       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:13.926934958Z I0109 04:43:13.926894       1 request.go:601] Waited for 1.163539723s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.024415917Z E0109 04:43:15.024375       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.024415917Z E0109 04:43:15.024405       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:15.024468349Z E0109 04:43:15.024423       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.024652598Z E0109 04:43:15.024636       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:15.126651202Z I0109 04:43:15.126614       1 request.go:601] Waited for 1.197810993s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods
2023-01-09T04:43:15.132700149Z I0109 04:43:15.132653       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:15.134692215Z E0109 04:43:15.134667       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.134692215Z E0109 04:43:15.134688       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:15.134715018Z E0109 04:43:15.134699       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.134883695Z E0109 04:43:15.134867       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:15.141480988Z E0109 04:43:15.141460       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.141500159Z E0109 04:43:15.141479       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:15.141500159Z E0109 04:43:15.141489       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.141652563Z E0109 04:43:15.141639       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:15.151614886Z E0109 04:43:15.151580       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:15.151614886Z E0109 04:43:15.151608       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.151636118Z E0109 04:43:15.151624       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.160038798Z E0109 04:43:15.160006       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:43:15.160790766Z I0109 04:43:15.160761       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:15.161313378Z I0109 04:43:15.161284       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:15.167739232Z I0109 04:43:15.167688       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:43:15.689244008Z E0109 04:43:15.689207       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.689244008Z E0109 04:43:15.689234       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:15.689285577Z E0109 04:43:15.689247       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.697260736Z E0109 04:43:15.697224       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:15.697873808Z I0109 04:43:15.697843       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:15.698380135Z I0109 04:43:15.698356       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:15.704755469Z I0109 04:43:15.704696       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:15.928454408Z I0109 04:43:15.928400       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:15.928546875Z E0109 04:43:15.928530       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:16.126672131Z I0109 04:43:16.126632       1 request.go:601] Waited for 1.197742381s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:16.329236033Z I0109 04:43:16.329197       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:43:17.326924004Z I0109 04:43:17.326888       1 request.go:601] Waited for 1.398281509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:43:18.527079457Z I0109 04:43:18.527042       1 request.go:601] Waited for 1.398413275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2023-01-09T04:43:18.729259773Z I0109 04:43:18.729195       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:18.729422554Z E0109 04:43:18.729402       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:19.095845667Z E0109 04:43:19.095796       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:19.095845667Z E0109 04:43:19.095824       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:19.095845667Z E0109 04:43:19.095839       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:19.103853122Z E0109 04:43:19.103816       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:43:19.104828663Z I0109 04:43:19.104789       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:19.105209594Z I0109 04:43:19.105185       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:19.113447683Z I0109 04:43:19.112590       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:43:19.128430323Z I0109 04:43:19.128394       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:20.127404046Z I0109 04:43:20.127365       1 request.go:601] Waited for 1.023078942s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:43:21.326503211Z I0109 04:43:21.326465       1 request.go:601] Waited for 1.197189626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/default/endpoints/kubernetes
2023-01-09T04:43:21.328581452Z I0109 04:43:21.328530       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:21.328729508Z E0109 04:43:21.328710       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:21.529179656Z I0109 04:43:21.529135       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:27.529098966Z I0109 04:43:27.529047       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:27.529220672Z E0109 04:43:27.529201       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:32.013171979Z E0109 04:43:32.013121       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:32.013171979Z E0109 04:43:32.013150       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:32.013206066Z E0109 04:43:32.013168       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:32.013584718Z E0109 04:43:32.013459       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:43:35.919274430Z E0109 04:43:35.919239       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:43:44.194372678Z E0109 04:43:44.194336       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:44.194372678Z E0109 04:43:44.194362       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:44.194403679Z E0109 04:43:44.194372       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:44.204474892Z E0109 04:43:44.204439       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:44.210950622Z I0109 04:43:44.210919       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:44.212396705Z I0109 04:43:44.212347       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:44.215764086Z I0109 04:43:44.215735       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:44.220493072Z I0109 04:43:44.220439       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:44.221624681Z E0109 04:43:44.221260       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:44.221624681Z I0109 04:43:44.221373       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:45.433833543Z E0109 04:43:45.433796       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:45.433911399Z E0109 04:43:45.433901       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:45.433960192Z E0109 04:43:45.433939       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:45.434341243Z E0109 04:43:45.434319       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:50.832924550Z E0109 04:43:50.832891       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:50.832924550Z E0109 04:43:50.832916       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:50.832958140Z E0109 04:43:50.832927       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:50.843539771Z E0109 04:43:50.843509       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:50.844786418Z I0109 04:43:50.844754       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:50.844861308Z I0109 04:43:50.844842       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:50.852827474Z I0109 04:43:50.852773       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:51.172147543Z E0109 04:43:51.172110       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:51.172147543Z E0109 04:43:51.172135       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:51.172179519Z E0109 04:43:51.172146       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:51.172321109Z E0109 04:43:51.172305       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:52.006889100Z I0109 04:43:52.006851       1 request.go:601] Waited for 1.162395331s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:43:53.011428747Z I0109 04:43:53.011376       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:53.184551259Z E0109 04:43:53.184512       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:53.184551259Z E0109 04:43:53.184534       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:53.184551259Z E0109 04:43:53.184546       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:53.194215445Z E0109 04:43:53.194178       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:43:53.195322602Z I0109 04:43:53.195288       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:53.195765722Z I0109 04:43:53.195736       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:53.203538579Z I0109 04:43:53.203496       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]"
2023-01-09T04:43:53.206405873Z I0109 04:43:53.206365       1 request.go:601] Waited for 1.395861857s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/default/endpoints/kubernetes
2023-01-09T04:43:53.209749858Z I0109 04:43:53.209690       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:53.209883740Z E0109 04:43:53.209856       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:53.813625856Z I0109 04:43:53.813574       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-controller-manager version "kube-controller-manager" changed from "" to "1.25.4"
2023-01-09T04:43:53.813625856Z I0109 04:43:53.813604       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-controller-manager version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:43:53.813885334Z I0109 04:43:53.813866       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"kube-controller-manager","version":"1.25.4"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:43:53.822109616Z I0109 04:43:53.822044       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: status.versions changed from [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"}] to [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"} {"kube-controller-manager" "1.25.4"} {"operator" "4.12.0-0.nightly-2023-01-08-142418"}]
2023-01-09T04:43:53.823300693Z I0109 04:43:53.822333       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-controller-manager version "kube-controller-manager" changed from "" to "1.25.4"
2023-01-09T04:43:53.823300693Z I0109 04:43:53.822355       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-controller-manager version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:43:53.823300693Z I0109 04:43:53.822897       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"kube-controller-manager","version":"1.25.4"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:43:53.834403183Z E0109 04:43:53.834376       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:43:54.206728678Z I0109 04:43:54.206691       1 request.go:601] Waited for 1.025288939s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:55.207062575Z I0109 04:43:55.207023       1 request.go:601] Waited for 1.393325937s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:55.612530976Z E0109 04:43:55.612181       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:43:55.612566970Z I0109 04:43:55.612544       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:43:56.410055410Z I0109 04:43:56.410017       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:43:57.206431658Z I0109 04:43:57.206397       1 request.go:601] Waited for 1.056023013s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:59.213362186Z E0109 04:43:59.213325       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:59.213362186Z E0109 04:43:59.213351       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:59.213441125Z I0109 04:43:59.213409       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:43:59.223752831Z E0109 04:43:59.223716       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:59.224343082Z I0109 04:43:59.224311       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:59.224959427Z I0109 04:43:59.224926       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:59.233133680Z I0109 04:43:59.233087       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:59.612166060Z I0109 04:43:59.612126       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:00.406036895Z I0109 04:44:00.405980       1 request.go:601] Waited for 1.181813259s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:44:01.021400185Z I0109 04:44:01.021354       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:01.022216452Z I0109 04:44:01.022174       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: ","reason":"GarbageCollector_Error::GuardController_SyncError::StaticPods_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:01.030580265Z I0109 04:44:01.030515       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: "
2023-01-09T04:44:01.406709377Z I0109 04:44:01.406671       1 request.go:601] Waited for 1.571606229s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:44:02.010900788Z I0109 04:44:02.010846       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:02.011107320Z E0109 04:44:02.011074       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:02.213056635Z E0109 04:44:02.213016       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:02.213056635Z E0109 04:44:02.213045       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:02.213341931Z E0109 04:44:02.213314       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:02.406717555Z I0109 04:44:02.406683       1 request.go:601] Waited for 1.38632904s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:03.606102380Z I0109 04:44:03.606054       1 request.go:601] Waited for 1.594801387s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:44:04.212012527Z I0109 04:44:04.211953       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:04.606935478Z I0109 04:44:04.606882       1 request.go:601] Waited for 1.595685836s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2023-01-09T04:44:05.209783041Z I0109 04:44:05.209737       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:05.210030500Z E0109 04:44:05.209977       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:05.411112191Z E0109 04:44:05.411074       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:05.411112191Z E0109 04:44:05.411099       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:05.411329488Z E0109 04:44:05.411315       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:05.806840934Z I0109 04:44:05.806807       1 request.go:601] Waited for 1.593699707s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:07.006791244Z I0109 04:44:07.006750       1 request.go:601] Waited for 1.195403592s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:07.610576433Z E0109 04:44:07.610537       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:07.610667190Z E0109 04:44:07.610654       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:07.611065982Z E0109 04:44:07.611033       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:08.011085521Z I0109 04:44:08.011044       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:08.621399764Z I0109 04:44:08.621360       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:08.622455253Z I0109 04:44:08.622424       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:08.630197225Z I0109 04:44:08.629470       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"cluster-policy-controller\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-cert-syncer\" is waiting: ContainerCreating: \nStaticPodsDegraded: pod/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal container \"kube-controller-manager-recovery-controller\" is waiting: ContainerCreating: " to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:09.806264443Z I0109 04:44:09.806222       1 request.go:601] Waited for 1.184044908s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:44:10.410468276Z I0109 04:44:10.410400       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:10.410556098Z E0109 04:44:10.410534       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:10.806283553Z I0109 04:44:10.806243       1 request.go:601] Waited for 1.195249528s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:11.609726766Z I0109 04:44:11.609686       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:44:11.609726766Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:44:11.609726766Z  CurrentRevision: (int32) 5,
2023-01-09T04:44:11.609726766Z  TargetRevision: (int32) 0,
2023-01-09T04:44:11.609726766Z  LastFailedRevision: (int32) 0,
2023-01-09T04:44:11.609726766Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:44:11.609726766Z  LastFailedReason: (string) "",
2023-01-09T04:44:11.609726766Z  LastFailedCount: (int) 0,
2023-01-09T04:44:11.609726766Z  LastFallbackCount: (int) 0,
2023-01-09T04:44:11.609726766Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:44:11.609726766Z }
2023-01-09T04:44:11.609726766Z  because static pod is ready
2023-01-09T04:44:11.620850293Z I0109 04:44:11.620794       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 0 to 5 because static pod is ready
2023-01-09T04:44:11.623311385Z I0109 04:44:11.623270       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:11.623927711Z I0109 04:44:11.623895       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:11.640070422Z I0109 04:44:11.640022       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5",Available changed from False to True ("StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5")
2023-01-09T04:44:12.206745701Z I0109 04:44:12.206706       1 request.go:601] Waited for 1.051709543s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:13.406924878Z I0109 04:44:13.406873       1 request.go:601] Waited for 1.596484579s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller
2023-01-09T04:44:13.810822226Z E0109 04:44:13.810784       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:13.810822226Z E0109 04:44:13.810809       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:13.811038683Z E0109 04:44:13.811023       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:14.410608762Z I0109 04:44:14.410557       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:14.410787999Z E0109 04:44:14.410755       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:14.607073233Z I0109 04:44:14.607028       1 request.go:601] Waited for 1.397516364s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:14.610575806Z I0109 04:44:14.610538       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal static pod not found and needs new revision 5
2023-01-09T04:44:14.610615411Z I0109 04:44:14.610577       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:44:14.610615411Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:44:14.610615411Z  CurrentRevision: (int32) 0,
2023-01-09T04:44:14.610615411Z  TargetRevision: (int32) 5,
2023-01-09T04:44:14.610615411Z  LastFailedRevision: (int32) 0,
2023-01-09T04:44:14.610615411Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:44:14.610615411Z  LastFailedReason: (string) "",
2023-01-09T04:44:14.610615411Z  LastFailedCount: (int) 0,
2023-01-09T04:44:14.610615411Z  LastFallbackCount: (int) 0,
2023-01-09T04:44:14.610615411Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:44:14.610615411Z }
2023-01-09T04:44:14.621703330Z I0109 04:44:14.621653       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 0 to 5 because node ip-10-0-160-211.us-east-2.compute.internal static pod not found
2023-01-09T04:44:14.622218745Z I0109 04:44:14.622187       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:15.806825870Z I0109 04:44:15.806783       1 request.go:601] Waited for 1.395936326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:44:16.609524733Z E0109 04:44:16.609479       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:16.609524733Z E0109 04:44:16.609508       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:16.609744535Z E0109 04:44:16.609724       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:17.006818455Z I0109 04:44:17.006778       1 request.go:601] Waited for 1.396100851s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:17.210524801Z I0109 04:44:17.210463       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:17.210711570Z E0109 04:44:17.210687       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:17.614062254Z I0109 04:44:17.614011       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:44:18.006860321Z I0109 04:44:18.006814       1 request.go:601] Waited for 1.393509194s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:18.810500226Z I0109 04:44:18.810457       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:44:19.206897377Z I0109 04:44:19.206860       1 request.go:601] Waited for 1.371820447s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:44:19.410346025Z E0109 04:44:19.410299       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:19.410346025Z E0109 04:44:19.410332       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:19.410553443Z E0109 04:44:19.410538       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:20.010442141Z I0109 04:44:20.010388       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:20.010621619Z E0109 04:44:20.010587       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:20.406229442Z I0109 04:44:20.406188       1 request.go:601] Waited for 1.395346883s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:44:21.406247816Z I0109 04:44:21.406209       1 request.go:601] Waited for 1.195827339s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:21.410093137Z I0109 04:44:21.410062       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:22.010403994Z E0109 04:44:22.010365       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:22.010403994Z E0109 04:44:22.010391       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:22.010663008Z E0109 04:44:22.010641       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:22.014903371Z E0109 04:44:22.014878       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:22.014903371Z E0109 04:44:22.014899       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:22.407062387Z I0109 04:44:22.407025       1 request.go:601] Waited for 1.196322895s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:23.610339919Z I0109 04:44:23.610297       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:24.011962373Z E0109 04:44:24.011930       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.619118431Z E0109 04:44:38.619064       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.619118431Z E0109 04:44:38.619100       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.619422980Z E0109 04:44:38.619395       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.635442091Z E0109 04:44:38.635403       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.635505521Z E0109 04:44:38.635493       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.657546551Z E0109 04:44:38.657510       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.680309780Z E0109 04:44:38.680276       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.680386830Z E0109 04:44:38.680351       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.680571039Z E0109 04:44:38.680556       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.845420139Z E0109 04:44:38.845381       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.845420139Z E0109 04:44:38.845408       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.845641461Z E0109 04:44:38.845622       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:40.418551049Z E0109 04:44:40.418513       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:40.418551049Z E0109 04:44:40.418536       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:40.418747429Z E0109 04:44:40.418734       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:42.952431325Z E0109 04:44:42.952396       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:42.952431325Z E0109 04:44:42.952421       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:42.952634134Z E0109 04:44:42.952619       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:43.608314644Z E0109 04:44:43.608280       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:43.608314644Z E0109 04:44:43.608306       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:43.608510934Z E0109 04:44:43.608497       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:45.844832424Z E0109 04:44:45.844793       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:45.852355865Z E0109 04:44:45.852325       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:45.866828701Z E0109 04:44:45.866786       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:44:45.868471984Z I0109 04:44:45.868443       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:45.874118817Z I0109 04:44:45.872410       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:45.882553218Z I0109 04:44:45.881485       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]"
2023-01-09T04:44:45.882553218Z I0109 04:44:45.881734       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:45.883177685Z I0109 04:44:45.883142       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:45.883236966Z I0109 04:44:45.883204       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:45.883353088Z E0109 04:44:45.883322       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:45.888807141Z E0109 04:44:45.888782       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:44:49.249109472Z E0109 04:44:49.249068       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:49.249109472Z E0109 04:44:49.249094       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:49.259156903Z E0109 04:44:49.259125       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:49.259980158Z I0109 04:44:49.259949       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:49.260825657Z I0109 04:44:49.260795       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:49.270244481Z I0109 04:44:49.268832       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243063       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:21 +0000 UTC to 2033-01-06 04:28:21 +0000 UTC (now=2023-01-09 04:44:50.24302134 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243121       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.243098322 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243151       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.243132699 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243179       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.243161141 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243209       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2033-01-06 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.243189915 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243241       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1673239265\" [] issuer=\"kubelet-signer\" (2023-01-09 04:41:04 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.243220531 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243268       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1673239266\" [] issuer=\"<self>\" (2023-01-09 04:41:06 +0000 UTC to 2024-01-09 04:41:07 +0000 UTC (now=2023-01-09 04:44:50.243250361 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243295       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.243279832 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243482       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-controller-manager-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-controller-manager-operator.svc,metrics.openshift-kube-controller-manager-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:18 +0000 UTC to 2025-01-08 04:41:19 +0000 UTC (now=2023-01-09 04:44:50.243456022 +0000 UTC))"
2023-01-09T04:44:50.244094535Z I0109 04:44:50.243615       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239333\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239333\" (2023-01-09 03:42:13 +0000 UTC to 2024-01-09 03:42:13 +0000 UTC (now=2023-01-09 04:44:50.243594485 +0000 UTC))"
2023-01-09T04:44:50.446070545Z I0109 04:44:50.446027       1 request.go:601] Waited for 1.185278593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:51.144750016Z E0109 04:44:51.144712       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:51.249311704Z I0109 04:44:51.249261       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:51.249412402Z E0109 04:44:51.249392       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:51.452318853Z I0109 04:44:51.452280       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:52.245745171Z I0109 04:44:52.245701       1 request.go:601] Waited for 1.104511932s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:44:53.445965980Z I0109 04:44:53.445921       1 request.go:601] Waited for 1.195257437s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:44:53.649540141Z E0109 04:44:53.649498       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:53.659820643Z E0109 04:44:53.659767       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:44:53.660297932Z I0109 04:44:53.660267       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:53.661596165Z I0109 04:44:53.661561       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:53.669420852Z I0109 04:44:53.669367       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal]"
2023-01-09T04:44:54.446302796Z I0109 04:44:54.446261       1 request.go:601] Waited for 1.194985645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller
2023-01-09T04:44:55.449771195Z I0109 04:44:55.449731       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:55.646063938Z I0109 04:44:55.646024       1 request.go:601] Waited for 1.597231776s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T04:44:56.248550302Z I0109 04:44:56.248489       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:56.248658961Z E0109 04:44:56.248626       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:56.449318952Z E0109 04:44:56.449282       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:56.449318952Z E0109 04:44:56.449310       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:56.459249621Z E0109 04:44:56.459209       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:56.459977373Z I0109 04:44:56.459948       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:56.460557856Z I0109 04:44:56.460532       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:56.468631781Z I0109 04:44:56.468587       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:56.646169937Z I0109 04:44:56.646122       1 request.go:601] Waited for 1.195409652s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:57.845322496Z I0109 04:44:57.845284       1 request.go:601] Waited for 1.385464393s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:44:57.846563774Z E0109 04:44:57.846537       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:44:58.845699469Z I0109 04:44:58.845665       1 request.go:601] Waited for 1.390144974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T04:44:59.048888614Z I0109 04:44:59.048839       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:44:59.049085555Z E0109 04:44:59.049061       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:44:59.449520452Z I0109 04:44:59.449477       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:00.045697354Z I0109 04:45:00.045654       1 request.go:601] Waited for 1.151225483s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:45:02.251728702Z E0109 04:45:02.251681       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:02.252102286Z I0109 04:45:02.252064       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:45:02.263012654Z E0109 04:45:02.262953       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:02.263680623Z I0109 04:45:02.263644       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:02.264516284Z I0109 04:45:02.264476       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:02.277242686Z I0109 04:45:02.277174       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: [Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:45:02.452552252Z I0109 04:45:02.452512       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:02.646075742Z I0109 04:45:02.645851       1 request.go:601] Waited for 1.183191324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:03.646259674Z I0109 04:45:03.646215       1 request.go:601] Waited for 1.382350887s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:45:04.845614563Z I0109 04:45:04.845572       1 request.go:601] Waited for 1.593100185s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:45:05.049592721Z I0109 04:45:05.049539       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:05.049709872Z E0109 04:45:05.049688       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:06.045923254Z I0109 04:45:06.045885       1 request.go:601] Waited for 1.395972079s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:06.649707699Z I0109 04:45:06.649664       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:07.046017426Z I0109 04:45:07.045975       1 request.go:601] Waited for 1.197473629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-controller-ca
2023-01-09T04:45:07.649485532Z E0109 04:45:07.649444       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:07.649835914Z E0109 04:45:07.649818       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:09.848910205Z I0109 04:45:09.848869       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:11.249188984Z E0109 04:45:11.249147       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:11.249380532Z E0109 04:45:11.249366       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:12.646146859Z I0109 04:45:12.646098       1 request.go:601] Waited for 1.182848302s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:14.249270957Z I0109 04:45:14.249230       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:45:14.249270957Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:45:14.249270957Z  CurrentRevision: (int32) 5,
2023-01-09T04:45:14.249270957Z  TargetRevision: (int32) 0,
2023-01-09T04:45:14.249270957Z  LastFailedRevision: (int32) 0,
2023-01-09T04:45:14.249270957Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:45:14.249270957Z  LastFailedReason: (string) "",
2023-01-09T04:45:14.249270957Z  LastFailedCount: (int) 0,
2023-01-09T04:45:14.249270957Z  LastFallbackCount: (int) 0,
2023-01-09T04:45:14.249270957Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:45:14.249270957Z }
2023-01-09T04:45:14.249270957Z  because static pod is ready
2023-01-09T04:45:14.258680844Z I0109 04:45:14.258629       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 0 to 5 because static pod is ready
2023-01-09T04:45:14.259188476Z I0109 04:45:14.259158       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:14.259821811Z I0109 04:45:14.259787       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:14.267700180Z I0109 04:45:14.267664       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5" to "NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 5",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 5"
2023-01-09T04:45:14.649626056Z E0109 04:45:14.649588       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:14.649792637Z E0109 04:45:14.649777       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:15.045747468Z I0109 04:45:15.045710       1 request.go:601] Waited for 1.001373559s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:16.246011252Z I0109 04:45:16.245953       1 request.go:601] Waited for 1.592210542s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:16.448773992Z I0109 04:45:16.448720       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:16.448967166Z E0109 04:45:16.448940       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:17.246111598Z I0109 04:45:17.246065       1 request.go:601] Waited for 1.376170925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:45:18.249030784Z I0109 04:45:18.248966       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal static pod not found and needs new revision 5
2023-01-09T04:45:18.249066404Z I0109 04:45:18.249036       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:45:18.249066404Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:45:18.249066404Z  CurrentRevision: (int32) 0,
2023-01-09T04:45:18.249066404Z  TargetRevision: (int32) 5,
2023-01-09T04:45:18.249066404Z  LastFailedRevision: (int32) 0,
2023-01-09T04:45:18.249066404Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:45:18.249066404Z  LastFailedReason: (string) "",
2023-01-09T04:45:18.249066404Z  LastFailedCount: (int) 0,
2023-01-09T04:45:18.249066404Z  LastFallbackCount: (int) 0,
2023-01-09T04:45:18.249066404Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:45:18.249066404Z }
2023-01-09T04:45:18.258605662Z I0109 04:45:18.258562       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 5 because node ip-10-0-145-4.us-east-2.compute.internal static pod not found
2023-01-09T04:45:18.259515785Z I0109 04:45:18.259480       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:18.446293365Z I0109 04:45:18.446252       1 request.go:601] Waited for 1.196930306s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2023-01-09T04:45:19.049864299Z I0109 04:45:19.049814       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:19.049950383Z E0109 04:45:19.049931       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:19.646056998Z I0109 04:45:19.646020       1 request.go:601] Waited for 1.386243867s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:20.249027939Z E0109 04:45:20.248961       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:20.249214375Z E0109 04:45:20.249198       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:20.845447127Z I0109 04:45:20.845411       1 request.go:601] Waited for 1.196671654s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods
2023-01-09T04:45:20.868587317Z I0109 04:45:20.868545       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:45:21.648443404Z I0109 04:45:21.648394       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:21.648556287Z E0109 04:45:21.648538       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:21.845734517Z I0109 04:45:21.845696       1 request.go:601] Waited for 1.19703301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:45:22.049097929Z I0109 04:45:22.049058       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:45:22.849345247Z E0109 04:45:22.849294       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:23.045790143Z I0109 04:45:23.045742       1 request.go:601] Waited for 1.196307294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:45:24.045926463Z I0109 04:45:24.045885       1 request.go:601] Waited for 1.196231028s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:24.448520301Z I0109 04:45:24.448479       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:45:25.049031397Z E0109 04:45:25.048975       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:26.250119393Z I0109 04:45:26.247421       1 request.go:601] Waited for 1.19501787s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:27.249213512Z I0109 04:45:27.249163       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:27.446169397Z I0109 04:45:27.446129       1 request.go:601] Waited for 1.315071307s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-apiserver/pods?labelSelector=app%3Dopenshift-kube-apiserver
2023-01-09T04:45:28.645468419Z I0109 04:45:28.645426       1 request.go:601] Waited for 1.194652105s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/default/endpoints/kubernetes
2023-01-09T04:45:28.648687447Z I0109 04:45:28.648645       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:28.648801373Z E0109 04:45:28.648781       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:29.646288626Z I0109 04:45:29.646238       1 request.go:601] Waited for 1.192403211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:45:30.049363525Z E0109 04:45:30.049307       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:30.049530597Z E0109 04:45:30.049508       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:30.849255465Z I0109 04:45:30.849210       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:30.849461505Z E0109 04:45:30.849436       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:32.649091046Z I0109 04:45:32.649039       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:32.649298878Z E0109 04:45:32.649278       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:33.648684998Z E0109 04:45:33.648648       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:33.648866360Z E0109 04:45:33.648851       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:34.248789056Z I0109 04:45:34.248739       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:34.248905071Z E0109 04:45:34.248884       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:36.249292517Z E0109 04:45:36.249251       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:36.249497473Z E0109 04:45:36.249482       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:37.649594027Z E0109 04:45:37.649554       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:37.849328971Z I0109 04:45:37.849267       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:37.849435731Z E0109 04:45:37.849417       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:39.250557357Z E0109 04:45:39.250520       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:39.448259301Z I0109 04:45:39.448206       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerStuck' unexpected addresses: 10.0.18.164
2023-01-09T04:45:39.448425069Z E0109 04:45:39.448401       1 base_controller.go:272] SATokenSignerController reconciliation failed: unexpected addresses: 10.0.18.164
2023-01-09T04:45:41.848640663Z E0109 04:45:41.848600       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:41.848857251Z E0109 04:45:41.848828       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:41.851738964Z E0109 04:45:41.851712       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:43.448877012Z E0109 04:45:43.448839       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:49.049360851Z E0109 04:45:49.049324       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:49.049545707Z E0109 04:45:49.049531       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:50.848862133Z E0109 04:45:50.848824       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:50.849069498Z E0109 04:45:50.849051       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:56.914914464Z I0109 04:45:56.914875       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:56.923313092Z E0109 04:45:56.923285       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:56.932680675Z E0109 04:45:56.932651       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:56.933649268Z I0109 04:45:56.933620       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:56.933916448Z I0109 04:45:56.933886       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:17Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 2 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:56.940978386Z I0109 04:45:56.940942       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:45:58.108650357Z I0109 04:45:58.108607       1 request.go:601] Waited for 1.170720521s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T04:45:58.511639664Z I0109 04:45:58.511581       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SATokenSignerControllerOK' found expected kube-apiserver endpoints
2023-01-09T04:45:58.711222885Z I0109 04:45:58.711178       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:59.308322306Z I0109 04:45:59.308282       1 request.go:601] Waited for 1.183259015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:46:00.308958465Z I0109 04:46:00.308921       1 request.go:601] Waited for 1.19625129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/cluster-policy-controller-config
2023-01-09T04:46:01.309142650Z I0109 04:46:01.309105       1 request.go:601] Waited for 1.169955778s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:46:02.112525736Z I0109 04:46:02.112485       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:46:02.316086621Z I0109 04:46:02.316034       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/next-service-account-private-key -n openshift-kube-controller-manager-operator because it was missing
2023-01-09T04:46:02.508804331Z I0109 04:46:02.508759       1 request.go:601] Waited for 1.397001076s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:46:03.111463818Z E0109 04:46:03.111416       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:03.111642158Z E0109 04:46:03.111627       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:03.709169720Z I0109 04:46:03.709130       1 request.go:601] Waited for 1.393023694s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/configmaps/sa-token-signing-certs
2023-01-09T04:46:03.717181448Z I0109 04:46:03.717115       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/sa-token-signing-certs -n openshift-config-managed:
2023-01-09T04:46:03.717181448Z cause by changes in data.service-account-002.pub
2023-01-09T04:46:05.085191721Z I0109 04:46:05.085137       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "secret/localhost-recovery-client-token has changed"
2023-01-09T04:46:05.318017787Z I0109 04:46:05.317960       1 request.go:601] Waited for 1.006586586s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:05.713048755Z I0109 04:46:05.712972       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:46:06.113527059Z I0109 04:46:06.113468       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:06.509138965Z I0109 04:46:06.509105       1 request.go:601] Waited for 1.185913233s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:46:07.115450428Z I0109 04:46:07.115397       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-manager-pod-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:07.913571689Z I0109 04:46:07.913520       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:08.309082504Z I0109 04:46:08.309045       1 request.go:601] Waited for 1.081363536s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:46:09.314043886Z I0109 04:46:09.313978       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/cluster-policy-controller-config-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:09.508753236Z I0109 04:46:09.508709       1 request.go:601] Waited for 1.397450713s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-5-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:10.709180461Z I0109 04:46:10.709131       1 request.go:601] Waited for 1.395292308s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps
2023-01-09T04:46:10.713746030Z I0109 04:46:10.713691       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/controller-manager-kubeconfig-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:10.911441891Z I0109 04:46:10.911403       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:46:10.911441891Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:46:10.911441891Z  CurrentRevision: (int32) 5,
2023-01-09T04:46:10.911441891Z  TargetRevision: (int32) 0,
2023-01-09T04:46:10.911441891Z  LastFailedRevision: (int32) 0,
2023-01-09T04:46:10.911441891Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:46:10.911441891Z  LastFailedReason: (string) "",
2023-01-09T04:46:10.911441891Z  LastFailedCount: (int) 0,
2023-01-09T04:46:10.911441891Z  LastFallbackCount: (int) 0,
2023-01-09T04:46:10.911441891Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:46:10.911441891Z }
2023-01-09T04:46:10.911441891Z  because static pod is ready
2023-01-09T04:46:10.921354016Z I0109 04:46:10.921303       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 5 because static pod is ready
2023-01-09T04:46:10.922151529Z I0109 04:46:10.922117       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:10.923007409Z I0109 04:46:10.922963       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:10Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:10.930497375Z I0109 04:46:10.930463       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 5"),Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 2 nodes are at revision 5" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5"
2023-01-09T04:46:11.715571529Z I0109 04:46:11.715524       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:11.737629776Z I0109 04:46:11.737586       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"GarbageCollector_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:10Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:11.738049434Z I0109 04:46:11.737968       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:11.751498949Z I0109 04:46:11.751447       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host"
2023-01-09T04:46:11.908835643Z I0109 04:46:11.908797       1 request.go:601] Waited for 1.39449098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:46:12.114165720Z I0109 04:46:12.114114       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-cert-syncer-kubeconfig-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:12.909086648Z I0109 04:46:12.909045       1 request.go:601] Waited for 1.596921892s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/controller-manager-kubeconfig
2023-01-09T04:46:13.714458667Z I0109 04:46:13.714400       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:14.109151176Z I0109 04:46:14.109118       1 request.go:601] Waited for 1.593710693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:46:15.308940957Z I0109 04:46:15.308901       1 request.go:601] Waited for 1.594488685s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps
2023-01-09T04:46:15.313414559Z I0109 04:46:15.313359       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/service-ca-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:15.512228320Z I0109 04:46:15.512190       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:46:15.512228320Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:46:15.512228320Z  CurrentRevision: (int32) 5,
2023-01-09T04:46:15.512228320Z  TargetRevision: (int32) 0,
2023-01-09T04:46:15.512228320Z  LastFailedRevision: (int32) 0,
2023-01-09T04:46:15.512228320Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:46:15.512228320Z  LastFailedReason: (string) "",
2023-01-09T04:46:15.512228320Z  LastFailedCount: (int) 0,
2023-01-09T04:46:15.512228320Z  LastFallbackCount: (int) 0,
2023-01-09T04:46:15.512228320Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:46:15.512228320Z }
2023-01-09T04:46:15.512228320Z  because static pod is ready
2023-01-09T04:46:16.508691235Z I0109 04:46:16.508654       1 request.go:601] Waited for 1.347363678s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:46:16.913398495Z I0109 04:46:16.913349       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/recycler-config-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:17.509795505Z I0109 04:46:17.509762       1 request.go:601] Waited for 1.596520089s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/kube-controller-manager-pod
2023-01-09T04:46:18.113807902Z I0109 04:46:18.113756       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/service-account-private-key-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:18.708724506Z I0109 04:46:18.708683       1 request.go:601] Waited for 1.195477525s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/trusted-ca-bundle
2023-01-09T04:46:19.314165726Z I0109 04:46:19.314119       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:19.908818226Z I0109 04:46:19.908783       1 request.go:601] Waited for 1.179121991s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T04:46:20.515055490Z I0109 04:46:20.514971       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-6 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:20.524772122Z I0109 04:46:20.524718       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 5 created because secret/localhost-recovery-client-token has changed
2023-01-09T04:46:20.525284423Z I0109 04:46:20.525243       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:21.308640267Z I0109 04:46:21.308606       1 request.go:601] Waited for 1.02490459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:46:22.309102232Z I0109 04:46:22.309055       1 request.go:601] Waited for 1.197795974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:46:23.121516034Z I0109 04:46:23.121474       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:23.122146223Z I0109 04:46:23.122121       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host","reason":"GarbageCollector_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:23.129338497Z I0109 04:46:23.129260       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6"
2023-01-09T04:46:23.509062566Z I0109 04:46:23.509018       1 request.go:601] Waited for 1.195539966s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:24.709153852Z I0109 04:46:24.709113       1 request.go:601] Waited for 1.197196422s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:28.515181575Z I0109 04:46:28.515138       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal with revision 5 is the oldest and needs new revision 6
2023-01-09T04:46:28.515211485Z I0109 04:46:28.515192       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:46:28.515211485Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:46:28.515211485Z  CurrentRevision: (int32) 5,
2023-01-09T04:46:28.515211485Z  TargetRevision: (int32) 6,
2023-01-09T04:46:28.515211485Z  LastFailedRevision: (int32) 0,
2023-01-09T04:46:28.515211485Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:46:28.515211485Z  LastFailedReason: (string) "",
2023-01-09T04:46:28.515211485Z  LastFailedCount: (int) 0,
2023-01-09T04:46:28.515211485Z  LastFallbackCount: (int) 0,
2023-01-09T04:46:28.515211485Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:46:28.515211485Z }
2023-01-09T04:46:28.525604456Z I0109 04:46:28.525558       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 5 to 6 because node ip-10-0-199-219.us-east-2.compute.internal with revision 5 is the oldest
2023-01-09T04:46:28.526802498Z I0109 04:46:28.526772       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:33.511945557Z I0109 04:46:33.511908       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal with revision 5 is the oldest and needs new revision 6
2023-01-09T04:46:33.511980480Z I0109 04:46:33.511946       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:46:33.511980480Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:46:33.511980480Z  CurrentRevision: (int32) 5,
2023-01-09T04:46:33.511980480Z  TargetRevision: (int32) 6,
2023-01-09T04:46:33.511980480Z  LastFailedRevision: (int32) 0,
2023-01-09T04:46:33.511980480Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:46:33.511980480Z  LastFailedReason: (string) "",
2023-01-09T04:46:33.511980480Z  LastFailedCount: (int) 0,
2023-01-09T04:46:33.511980480Z  LastFallbackCount: (int) 0,
2023-01-09T04:46:33.511980480Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:46:33.511980480Z }
2023-01-09T04:46:35.119051543Z I0109 04:46:35.118971       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:46:35.711561188Z I0109 04:46:35.711518       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:46:37.712165119Z I0109 04:46:37.712128       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:38.708560392Z I0109 04:46:38.708520       1 request.go:601] Waited for 1.010316321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T04:46:39.912811958Z I0109 04:46:39.912775       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:47:11.042491486Z I0109 04:47:11.042451       1 request.go:601] Waited for 1.150608616s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:12.042800928Z I0109 04:47:12.042764       1 request.go:601] Waited for 1.195629069s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:13.242757400Z I0109 04:47:13.242707       1 request.go:601] Waited for 1.379037171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T04:47:13.648003266Z I0109 04:47:13.647951       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:47:13.962495518Z E0109 04:47:13.962459       1 base_controller.go:272] GarbageCollectorWatcherController reconciliation failed: error fetching rules: Get "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules": dial tcp 172.30.74.101:9091: connect: connection refused
2023-01-09T04:47:13.963398016Z I0109 04:47:13.963361       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:13.963428248Z I0109 04:47:13.963416       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused","reason":"GarbageCollector_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:13.975083739Z I0109 04:47:13.972320       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused"
2023-01-09T04:47:14.242859004Z I0109 04:47:14.242820       1 request.go:601] Waited for 1.388459563s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:47:15.442165550Z I0109 04:47:15.442130       1 request.go:601] Waited for 1.390846996s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:47:16.442587845Z I0109 04:47:16.442538       1 request.go:601] Waited for 1.393989412s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:17.442684444Z I0109 04:47:17.442640       1 request.go:601] Waited for 1.193842112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:17.447068206Z I0109 04:47:17.447035       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:47:20.448142621Z I0109 04:47:20.448104       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:47:20.642164152Z I0109 04:47:20.642117       1 request.go:601] Waited for 1.092545127s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:23.042193168Z I0109 04:47:23.042155       1 request.go:601] Waited for 1.192204832s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:23.846790595Z I0109 04:47:23.846754       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:24.050775646Z E0109 04:47:24.050741       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:24.241971014Z I0109 04:47:24.241928       1 request.go:601] Waited for 1.092723657s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:25.242478523Z I0109 04:47:25.242440       1 request.go:601] Waited for 1.191319933s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:26.246970558Z I0109 04:47:26.246914       1 request.go:601] Waited for 1.200568716s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:27.442698074Z I0109 04:47:27.442658       1 request.go:601] Waited for 1.190969446s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:27.446447577Z I0109 04:47:27.446424       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:28.657681405Z E0109 04:47:28.657642       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:28.658935372Z I0109 04:47:28.658897       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:28.659172760Z I0109 04:47:28.659148       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GarbageCollector_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:28.668096205Z I0109 04:47:28.668041       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:47:29.842815109Z I0109 04:47:29.842778       1 request.go:601] Waited for 1.180680015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:30.847150281Z I0109 04:47:30.847114       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:31.042461497Z I0109 04:47:31.042427       1 request.go:601] Waited for 1.195905735s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:32.042841535Z I0109 04:47:32.042805       1 request.go:601] Waited for 1.194742282s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:34.048734541Z I0109 04:47:34.048698       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:34.242944229Z I0109 04:47:34.242877       1 request.go:601] Waited for 1.184529138s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:47:35.442728216Z I0109 04:47:35.442685       1 request.go:601] Waited for 1.393077294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:35.660823863Z I0109 04:47:35.660780       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:35.662111636Z I0109 04:47:35.662074       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused","reason":"GarbageCollector_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:35.671012894Z I0109 04:47:35.670955       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal" to "GarbageCollectorDegraded: error fetching rules: Get \"https://thanos-querier.openshift-monitoring.svc:9091/api/v1/rules\": dial tcp 172.30.74.101:9091: connect: connection refused"
2023-01-09T04:47:36.442759238Z I0109 04:47:36.442712       1 request.go:601] Waited for 1.194443739s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:37.442778395Z I0109 04:47:37.442730       1 request.go:601] Waited for 1.194993118s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:47:37.846984271Z I0109 04:47:37.846947       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:40.849603215Z I0109 04:47:40.849527       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:41.707271328Z I0109 04:47:41.707183       1 gcwatcher_controller.go:216] Synced alerting rules cache
2023-01-09T04:47:41.707271328Z W0109 04:47:41.707205       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T04:47:41.708961750Z W0109 04:47:41.708931       1 gcwatcher_controller.go:251] received warnings when querying alerts: No StoreAPIs matched for this query
2023-01-09T04:47:41.721190744Z I0109 04:47:41.721153       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:41.722073997Z I0109 04:47:41.722037       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:41.731324940Z I0109 04:47:41.731278       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready")
2023-01-09T04:47:42.842188827Z I0109 04:47:42.842157       1 request.go:601] Waited for 1.120473942s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:44.242768555Z I0109 04:47:44.242730       1 request.go:601] Waited for 1.02668235s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:45.046487625Z I0109 04:47:45.046444       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:45.046487625Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:47:45.046487625Z  CurrentRevision: (int32) 6,
2023-01-09T04:47:45.046487625Z  TargetRevision: (int32) 0,
2023-01-09T04:47:45.046487625Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:45.046487625Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:45.046487625Z  LastFailedReason: (string) "",
2023-01-09T04:47:45.046487625Z  LastFailedCount: (int) 0,
2023-01-09T04:47:45.046487625Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:45.046487625Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:45.046487625Z }
2023-01-09T04:47:45.046487625Z  because static pod is ready
2023-01-09T04:47:45.056685023Z I0109 04:47:45.056634       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 5 to 6 because static pod is ready
2023-01-09T04:47:45.057173581Z I0109 04:47:45.057138       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:45.057958530Z I0109 04:47:45.057923       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 2 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:45.065926281Z I0109 04:47:45.065686       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 5; 0 nodes have achieved new revision 6" to "NodeInstallerProgressing: 2 nodes are at revision 5; 1 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 5; 0 nodes have achieved new revision 6" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 5; 1 nodes are at revision 6"
2023-01-09T04:47:46.242628689Z I0109 04:47:46.242585       1 request.go:601] Waited for 1.184935659s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:48.246088737Z I0109 04:47:48.246048       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:48.246088737Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:47:48.246088737Z  CurrentRevision: (int32) 6,
2023-01-09T04:47:48.246088737Z  TargetRevision: (int32) 0,
2023-01-09T04:47:48.246088737Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:48.246088737Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:48.246088737Z  LastFailedReason: (string) "",
2023-01-09T04:47:48.246088737Z  LastFailedCount: (int) 0,
2023-01-09T04:47:48.246088737Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:48.246088737Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:48.246088737Z }
2023-01-09T04:47:48.246088737Z  because static pod is ready
2023-01-09T04:47:53.646541319Z I0109 04:47:53.646499       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 5 is the oldest and needs new revision 6
2023-01-09T04:47:53.646577331Z I0109 04:47:53.646553       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:53.646577331Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:47:53.646577331Z  CurrentRevision: (int32) 5,
2023-01-09T04:47:53.646577331Z  TargetRevision: (int32) 6,
2023-01-09T04:47:53.646577331Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:53.646577331Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:53.646577331Z  LastFailedReason: (string) "",
2023-01-09T04:47:53.646577331Z  LastFailedCount: (int) 0,
2023-01-09T04:47:53.646577331Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:53.646577331Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:53.646577331Z }
2023-01-09T04:47:53.659557802Z I0109 04:47:53.659502       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 5 to 6 because node ip-10-0-160-211.us-east-2.compute.internal with revision 5 is the oldest
2023-01-09T04:47:53.660173224Z I0109 04:47:53.660138       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:54.842178356Z I0109 04:47:54.842139       1 request.go:601] Waited for 1.181413811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:55.859508554Z I0109 04:47:55.859455       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:47:56.849688129Z I0109 04:47:56.849646       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:47:57.042445186Z I0109 04:47:57.042406       1 request.go:601] Waited for 1.183104189s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:58.242909631Z I0109 04:47:58.242874       1 request.go:601] Waited for 1.195051445s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:59.253411087Z I0109 04:47:59.253357       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:47:59.445027007Z I0109 04:47:59.444458       1 request.go:601] Waited for 1.196560417s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:48:01.446926941Z I0109 04:48:01.446892       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:48:31.646534750Z I0109 04:48:31.646496       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:48:34.246143881Z I0109 04:48:34.246085       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:48:36.043951371Z E0109 04:48:36.043904       1 guard_controller.go:327] Unable to apply pod kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.244475620Z W0109 04:48:36.244430       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.244475620Z E0109 04:48:36.244467       1 base_controller.go:272] GuardController reconciliation failed: [Unable to apply pod kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused, Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:36.250540570Z E0109 04:48:36.250504       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.251513718Z W0109 04:48:36.251476       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.251513718Z E0109 04:48:36.251502       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.262535932Z E0109 04:48:36.262508       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.263526336Z W0109 04:48:36.263501       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.263547161Z E0109 04:48:36.263526       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.284466741Z E0109 04:48:36.284438       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.285390226Z W0109 04:48:36.285364       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.285411414Z E0109 04:48:36.285389       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.326614559Z E0109 04:48:36.326571       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.327860152Z W0109 04:48:36.327826       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.327880314Z E0109 04:48:36.327862       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.408975086Z E0109 04:48:36.408932       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.410025115Z W0109 04:48:36.409976       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.410042050Z E0109 04:48:36.410027       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.571167335Z E0109 04:48:36.571118       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.572157020Z W0109 04:48:36.572125       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.572168579Z E0109 04:48:36.572156       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.893671647Z E0109 04:48:36.893624       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.894641044Z W0109 04:48:36.894602       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.894641044Z E0109 04:48:36.894629       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.536420447Z E0109 04:48:37.536374       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.537552725Z W0109 04:48:37.537510       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.537576711Z E0109 04:48:37.537548       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.818428457Z E0109 04:48:38.818381       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.819338035Z W0109 04:48:38.819299       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.819338035Z E0109 04:48:38.819328       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.380754523Z E0109 04:48:41.380707       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.381708678Z W0109 04:48:41.381680       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.381722582Z E0109 04:48:41.381711       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.122715922Z E0109 04:48:44.122664       1 leaderelection.go:330] error retrieving resource lock openshift-kube-controller-manager-operator/kube-controller-manager-operator-lock: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/kube-controller-manager-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.502691525Z E0109 04:48:46.502646       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.503602677Z W0109 04:48:46.503576       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.503618621Z E0109 04:48:46.503603       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:57.807509798Z I0109 04:48:57.807472       1 request.go:601] Waited for 1.1224089s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:48:58.223607840Z I0109 04:48:58.223566       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:58.807567368Z I0109 04:48:58.807528       1 request.go:601] Waited for 1.564114624s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:58.813830463Z E0109 04:48:58.813795       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:59.225297403Z I0109 04:48:59.225257       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:49:00.007099467Z I0109 04:49:00.007051       1 request.go:601] Waited for 1.747766823s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T04:49:00.236595560Z W0109 04:49:00.236558       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T04:49:01.007123213Z I0109 04:49:01.007085       1 request.go:601] Waited for 1.922590329s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T04:49:02.007679169Z I0109 04:49:02.007634       1 request.go:601] Waited for 1.38894732s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:02.028834068Z E0109 04:49:02.028791       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:49:02.029937539Z I0109 04:49:02.029882       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:02.030480626Z I0109 04:49:02.030439       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 2 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:02.039924896Z I0109 04:49:02.039876       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal"
2023-01-09T04:49:03.007770153Z I0109 04:49:03.007731       1 request.go:601] Waited for 1.395299858s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:04.011148854Z I0109 04:49:04.011100       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:04.011148854Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:49:04.011148854Z  CurrentRevision: (int32) 6,
2023-01-09T04:49:04.011148854Z  TargetRevision: (int32) 0,
2023-01-09T04:49:04.011148854Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:04.011148854Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:04.011148854Z  LastFailedReason: (string) "",
2023-01-09T04:49:04.011148854Z  LastFailedCount: (int) 0,
2023-01-09T04:49:04.011148854Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:04.011148854Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:04.011148854Z }
2023-01-09T04:49:04.011148854Z  because static pod is ready
2023-01-09T04:49:04.026423694Z I0109 04:49:04.026354       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 5 to 6 because static pod is ready
2023-01-09T04:49:04.027552658Z I0109 04:49:04.027522       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:04.028056783Z I0109 04:49:04.027964       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:04.042648709Z I0109 04:49:04.042599       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 5; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 5; 1 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6"
2023-01-09T04:49:04.207086398Z I0109 04:49:04.207048       1 request.go:601] Waited for 1.396456391s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:05.207648742Z I0109 04:49:05.207606       1 request.go:601] Waited for 1.180853689s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:06.207748885Z I0109 04:49:06.207706       1 request.go:601] Waited for 1.197407818s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:49:07.409936944Z I0109 04:49:07.409900       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal with revision 5 is the oldest not ready and needs new revision 6
2023-01-09T04:49:07.409966995Z I0109 04:49:07.409944       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:07.409966995Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:49:07.409966995Z  CurrentRevision: (int32) 5,
2023-01-09T04:49:07.409966995Z  TargetRevision: (int32) 6,
2023-01-09T04:49:07.409966995Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:07.409966995Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:07.409966995Z  LastFailedReason: (string) "",
2023-01-09T04:49:07.409966995Z  LastFailedCount: (int) 0,
2023-01-09T04:49:07.409966995Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:07.409966995Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:07.409966995Z }
2023-01-09T04:49:07.420745622Z I0109 04:49:07.420684       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 5 to 6 because node ip-10-0-145-4.us-east-2.compute.internal with revision 5 is the oldest not ready
2023-01-09T04:49:07.421853480Z I0109 04:49:07.421805       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:08.607422556Z I0109 04:49:08.607384       1 request.go:601] Waited for 1.185650519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:09.222091010Z I0109 04:49:09.222048       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:09.224477864Z I0109 04:49:09.224408       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:09.234733354Z I0109 04:49:09.234691       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:46:23Z","message":"NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:09.235162313Z I0109 04:49:09.235118       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal" to "NodeControllerDegraded: All master nodes are ready"
2023-01-09T04:49:09.240575945Z E0109 04:49:09.240539       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:49:09.613192385Z I0109 04:49:09.613136       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:49:10.609980205Z I0109 04:49:10.609938       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:49:10.807090512Z I0109 04:49:10.807047       1 request.go:601] Waited for 1.193878122s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:11.807167976Z I0109 04:49:11.807128       1 request.go:601] Waited for 1.196284812s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:13.007772694Z I0109 04:49:13.007726       1 request.go:601] Waited for 1.197884043s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:13.010429518Z I0109 04:49:13.010391       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:14.207186909Z I0109 04:49:14.207147       1 request.go:601] Waited for 1.195778091s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:15.207830940Z I0109 04:49:15.207791       1 request.go:601] Waited for 1.162031472s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:15.410147228Z I0109 04:49:15.410108       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:16.407410268Z I0109 04:49:16.407368       1 request.go:601] Waited for 1.161508671s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:49:43.795856061Z I0109 04:49:43.795817       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:45.980745301Z I0109 04:49:45.980701       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:50:03.168830816Z I0109 04:50:03.168794       1 request.go:601] Waited for 1.11782701s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:04.772260623Z I0109 04:50:04.772222       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:50:40.746099247Z I0109 04:50:40.746053       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:50:42.946808455Z I0109 04:50:42.946761       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:50:48.223649034Z I0109 04:50:48.223592       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:50:50.616924686Z I0109 04:50:50.616881       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:50:50.616924686Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:50:50.616924686Z  CurrentRevision: (int32) 6,
2023-01-09T04:50:50.616924686Z  TargetRevision: (int32) 0,
2023-01-09T04:50:50.616924686Z  LastFailedRevision: (int32) 0,
2023-01-09T04:50:50.616924686Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:50:50.616924686Z  LastFailedReason: (string) "",
2023-01-09T04:50:50.616924686Z  LastFailedCount: (int) 0,
2023-01-09T04:50:50.616924686Z  LastFallbackCount: (int) 0,
2023-01-09T04:50:50.616924686Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:50:50.616924686Z }
2023-01-09T04:50:50.616924686Z  because static pod is ready
2023-01-09T04:50:50.626077028Z I0109 04:50:50.626025       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 5 to 6 because static pod is ready
2023-01-09T04:50:50.632240530Z I0109 04:50:50.632197       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:50:50Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:50:50.639618179Z I0109 04:50:50.639574       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 6"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6"
2023-01-09T04:50:51.814308090Z I0109 04:50:51.814266       1 request.go:601] Waited for 1.181858049s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:50:52.820029521Z I0109 04:50:52.819950       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:50:53.014063140Z I0109 04:50:53.014025       1 request.go:601] Waited for 1.377153995s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:50:53.818093633Z I0109 04:50:53.818051       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:50:53.818093633Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:50:53.818093633Z  CurrentRevision: (int32) 6,
2023-01-09T04:50:53.818093633Z  TargetRevision: (int32) 0,
2023-01-09T04:50:53.818093633Z  LastFailedRevision: (int32) 0,
2023-01-09T04:50:53.818093633Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:50:53.818093633Z  LastFailedReason: (string) "",
2023-01-09T04:50:53.818093633Z  LastFailedCount: (int) 0,
2023-01-09T04:50:53.818093633Z  LastFallbackCount: (int) 0,
2023-01-09T04:50:53.818093633Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:50:53.818093633Z }
2023-01-09T04:50:53.818093633Z  because static pod is ready
2023-01-09T04:50:54.014601922Z I0109 04:50:54.014568       1 request.go:601] Waited for 1.194373058s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:50:55.214499623Z I0109 04:50:55.214459       1 request.go:601] Waited for 1.39583442s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:55.620459558Z I0109 04:50:55.620403       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:50:56.214654219Z I0109 04:50:56.214620       1 request.go:601] Waited for 1.396986519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:50:57.414326258Z I0109 04:50:57.414292       1 request.go:601] Waited for 1.397517255s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:50:58.421767836Z I0109 04:50:58.421707       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:50:58.614506556Z I0109 04:50:58.614468       1 request.go:601] Waited for 1.394175944s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:50:59.814151953Z I0109 04:50:59.814111       1 request.go:601] Waited for 1.392586323s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/revision-status-1
2023-01-09T04:51:00.814549741Z I0109 04:51:00.814508       1 request.go:601] Waited for 1.389035827s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:01.814617492Z I0109 04:51:01.814577       1 request.go:601] Waited for 1.19813024s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:03.014188588Z I0109 04:51:03.014143       1 request.go:601] Waited for 1.19629386s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:51:04.014495502Z I0109 04:51:04.014452       1 request.go:601] Waited for 1.197721348s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:13.020483688Z I0109 04:51:13.020428       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretUpdated' Updated Secret/service-account-private-key -n openshift-kube-controller-manager because it changed
2023-01-09T04:51:13.028535534Z I0109 04:51:13.028464       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "secret/service-account-private-key has changed"
2023-01-09T04:51:13.621604959Z I0109 04:51:13.621549       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:14.220400931Z I0109 04:51:14.220323       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-manager-pod-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:15.218676655Z I0109 04:51:15.218623       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:16.027100163Z I0109 04:51:16.026944       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/cluster-policy-controller-config-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:17.023870634Z I0109 04:51:17.023814       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/controller-manager-kubeconfig-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:17.619351237Z I0109 04:51:17.619285       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-controller-cert-syncer-kubeconfig-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:18.220152071Z I0109 04:51:18.220101       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:18.818827141Z I0109 04:51:18.818777       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/service-ca-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:19.418431508Z I0109 04:51:19.418370       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/recycler-config-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:20.019299810Z I0109 04:51:20.019230       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/service-account-private-key-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:20.620109598Z I0109 04:51:20.620057       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:21.221105312Z I0109 04:51:21.221051       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-7 -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:21.229895079Z I0109 04:51:21.229842       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 6 created because secret/service-account-private-key has changed
2023-01-09T04:51:21.234921801Z I0109 04:51:21.234878       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "secret/service-account-private-key has changed"
2023-01-09T04:51:21.243166544Z W0109 04:51:21.243127       1 staticpod.go:38] revision 7 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:51:21.252516054Z E0109 04:51:21.252466       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 7
2023-01-09T04:51:21.253679670Z I0109 04:51:21.253631       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision 7","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:50:50Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:21.262018506Z I0109 04:51:21.261242       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision 7"
2023-01-09T04:51:21.268786063Z I0109 04:51:21.268751       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:50:50Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:21.276431250Z I0109 04:51:21.275594       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nRevisionControllerDegraded: conflicting latestAvailableRevision 7" to "NodeControllerDegraded: All master nodes are ready"
2023-01-09T04:51:22.414307130Z I0109 04:51:22.414265       1 request.go:601] Waited for 1.181937224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:51:23.423361294Z I0109 04:51:23.423280       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:23.614099899Z I0109 04:51:23.614049       1 request.go:601] Waited for 1.193806696s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:51:24.814531721Z I0109 04:51:24.814491       1 request.go:601] Waited for 1.390817018s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:26.014095442Z I0109 04:51:26.014051       1 request.go:601] Waited for 1.39583595s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:51:26.219570006Z I0109 04:51:26.219498       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:27.014417793Z I0109 04:51:27.014377       1 request.go:601] Waited for 1.397505004s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:28.014649787Z I0109 04:51:28.014612       1 request.go:601] Waited for 1.396934945s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/recycler-config
2023-01-09T04:51:28.616519109Z I0109 04:51:28.616472       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:51:28.616570366Z I0109 04:51:28.616522       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:51:28.616570366Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:51:28.616570366Z  CurrentRevision: (int32) 6,
2023-01-09T04:51:28.616570366Z  TargetRevision: (int32) 7,
2023-01-09T04:51:28.616570366Z  LastFailedRevision: (int32) 0,
2023-01-09T04:51:28.616570366Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:51:28.616570366Z  LastFailedReason: (string) "",
2023-01-09T04:51:28.616570366Z  LastFailedCount: (int) 0,
2023-01-09T04:51:28.616570366Z  LastFallbackCount: (int) 0,
2023-01-09T04:51:28.616570366Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:51:28.616570366Z }
2023-01-09T04:51:28.627080416Z I0109 04:51:28.627030       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 6 to 7 because node ip-10-0-199-219.us-east-2.compute.internal with revision 6 is the oldest
2023-01-09T04:51:28.628464170Z I0109 04:51:28.628426       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:28.637350133Z I0109 04:51:28.637293       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision 6; 0 nodes have achieved new revision 7"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6; 0 nodes have achieved new revision 7"
2023-01-09T04:51:29.023397379Z I0109 04:51:29.023347       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:29.214741921Z I0109 04:51:29.214692       1 request.go:601] Waited for 1.398051897s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:30.414678463Z I0109 04:51:30.414639       1 request.go:601] Waited for 1.595928979s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:51:31.614465259Z I0109 04:51:31.614422       1 request.go:601] Waited for 1.398251123s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods
2023-01-09T04:51:31.620705274Z I0109 04:51:31.620654       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:51:32.814528410Z I0109 04:51:32.814487       1 request.go:601] Waited for 1.397299013s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:33.017369907Z I0109 04:51:33.017325       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:51:34.013775000Z I0109 04:51:34.013728       1 request.go:601] Waited for 1.391378133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T04:51:35.014559886Z I0109 04:51:35.014515       1 request.go:601] Waited for 1.19787838s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/csr-signer
2023-01-09T04:51:35.617248867Z I0109 04:51:35.617203       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:51:37.617832578Z I0109 04:51:37.617791       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:07.027239187Z I0109 04:52:07.027196       1 request.go:601] Waited for 1.086595754s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:07.832003076Z I0109 04:52:07.831947       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:09.026908034Z I0109 04:52:09.026867       1 request.go:601] Waited for 1.066431224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:11.031925246Z I0109 04:52:11.031883       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:13.944706147Z W0109 04:52:13.944664       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T04:52:14.626822751Z I0109 04:52:14.626784       1 request.go:601] Waited for 1.188986203s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:52:15.827177549Z I0109 04:52:15.827134       1 request.go:601] Waited for 1.170003844s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/kube-system/serviceaccounts/vsphere-legacy-cloud-provider
2023-01-09T04:52:16.827345513Z I0109 04:52:16.827291       1 request.go:601] Waited for 1.395830814s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:17.033379551Z I0109 04:52:17.033337       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:18.027048916Z I0109 04:52:18.026976       1 request.go:601] Waited for 1.194833654s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:19.226957270Z I0109 04:52:19.226916       1 request.go:601] Waited for 1.178268018s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:20.431323856Z I0109 04:52:20.431278       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:22.031555000Z E0109 04:52:22.031517       1 guard_controller.go:274] Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:22.042654787Z E0109 04:52:22.042622       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:22.045205210Z I0109 04:52:22.045171       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:22.054689938Z I0109 04:52:22.054647       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:52:23.227305973Z I0109 04:52:23.227262       1 request.go:601] Waited for 1.182947416s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:23.831549761Z I0109 04:52:23.831493       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:24.426896313Z I0109 04:52:24.426846       1 request.go:601] Waited for 1.194770357s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:25.427358492Z I0109 04:52:25.427317       1 request.go:601] Waited for 1.196146897s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/localhost-recovery-client-token
2023-01-09T04:52:26.626600352Z I0109 04:52:26.626556       1 request.go:601] Waited for 1.367743427s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T04:52:27.631087470Z I0109 04:52:27.631048       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:52:27.631087470Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:52:27.631087470Z  CurrentRevision: (int32) 7,
2023-01-09T04:52:27.631087470Z  TargetRevision: (int32) 0,
2023-01-09T04:52:27.631087470Z  LastFailedRevision: (int32) 0,
2023-01-09T04:52:27.631087470Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:52:27.631087470Z  LastFailedReason: (string) "",
2023-01-09T04:52:27.631087470Z  LastFailedCount: (int) 0,
2023-01-09T04:52:27.631087470Z  LastFallbackCount: (int) 0,
2023-01-09T04:52:27.631087470Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:52:27.631087470Z }
2023-01-09T04:52:27.631087470Z  because static pod is ready
2023-01-09T04:52:27.641962646Z I0109 04:52:27.641924       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 6 to 7 because static pod is ready
2023-01-09T04:52:27.643916861Z I0109 04:52:27.643879       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:27.653492189Z I0109 04:52:27.653451       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 6; 0 nodes have achieved new revision 7" to "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7",Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6; 0 nodes have achieved new revision 7" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7"
2023-01-09T04:52:27.827047791Z I0109 04:52:27.826979       1 request.go:601] Waited for 1.195393637s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2023-01-09T04:52:28.827319446Z I0109 04:52:28.827274       1 request.go:601] Waited for 1.184082039s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:30.026836633Z I0109 04:52:30.026796       1 request.go:601] Waited for 1.59599396s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:30.043429929Z I0109 04:52:30.043389       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:30.056518272Z I0109 04:52:30.056450       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: Missing PodIP in operand kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal" to "NodeControllerDegraded: All master nodes are ready"
2023-01-09T04:52:31.227085706Z I0109 04:52:31.227042       1 request.go:601] Waited for 1.183994693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T04:52:31.831816507Z I0109 04:52:31.831775       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:52:31.831816507Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:52:31.831816507Z  CurrentRevision: (int32) 7,
2023-01-09T04:52:31.831816507Z  TargetRevision: (int32) 0,
2023-01-09T04:52:31.831816507Z  LastFailedRevision: (int32) 0,
2023-01-09T04:52:31.831816507Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:52:31.831816507Z  LastFailedReason: (string) "",
2023-01-09T04:52:31.831816507Z  LastFailedCount: (int) 0,
2023-01-09T04:52:31.831816507Z  LastFallbackCount: (int) 0,
2023-01-09T04:52:31.831816507Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:52:31.831816507Z }
2023-01-09T04:52:31.831816507Z  because static pod is ready
2023-01-09T04:52:32.227505657Z I0109 04:52:32.227464       1 request.go:601] Waited for 1.392582579s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:52:33.427488508Z I0109 04:52:33.427449       1 request.go:601] Waited for 1.169564435s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/kube-system/serviceaccounts/vsphere-legacy-cloud-provider
2023-01-09T04:52:34.626544031Z I0109 04:52:34.626502       1 request.go:601] Waited for 1.186089888s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T04:52:35.626602206Z I0109 04:52:35.626556       1 request.go:601] Waited for 1.194840154s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:37.831343031Z I0109 04:52:37.831294       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:52:37.831384310Z I0109 04:52:37.831348       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:52:37.831384310Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:52:37.831384310Z  CurrentRevision: (int32) 6,
2023-01-09T04:52:37.831384310Z  TargetRevision: (int32) 7,
2023-01-09T04:52:37.831384310Z  LastFailedRevision: (int32) 0,
2023-01-09T04:52:37.831384310Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:52:37.831384310Z  LastFailedReason: (string) "",
2023-01-09T04:52:37.831384310Z  LastFailedCount: (int) 0,
2023-01-09T04:52:37.831384310Z  LastFallbackCount: (int) 0,
2023-01-09T04:52:37.831384310Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:52:37.831384310Z }
2023-01-09T04:52:37.842116701Z I0109 04:52:37.842067       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 6 to 7 because node ip-10-0-160-211.us-east-2.compute.internal with revision 6 is the oldest
2023-01-09T04:52:39.026929482Z I0109 04:52:39.026888       1 request.go:601] Waited for 1.182513056s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:39.836212983Z I0109 04:52:39.836153       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:52:41.026806203Z I0109 04:52:41.026758       1 request.go:601] Waited for 1.190818104s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:41.031010470Z I0109 04:52:41.030976       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:52:42.027013721Z I0109 04:52:42.026952       1 request.go:601] Waited for 1.395213211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:43.027244967Z I0109 04:52:43.027203       1 request.go:601] Waited for 1.195729159s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/serviceaccount-ca
2023-01-09T04:52:43.631506972Z I0109 04:52:43.631463       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:44.227308712Z I0109 04:52:44.227271       1 request.go:601] Waited for 1.195665166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T04:52:45.631095499Z I0109 04:52:45.631051       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:45.829600637Z W0109 04:52:45.829554       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.829600637Z E0109 04:52:45.829593       1 base_controller.go:272] GuardController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.830575059Z E0109 04:52:45.830533       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.831587693Z W0109 04:52:45.831550       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.831587693Z E0109 04:52:45.831582       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.835557365Z E0109 04:52:45.835529       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.836524728Z W0109 04:52:45.836497       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.836537055Z E0109 04:52:45.836522       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.858007902Z E0109 04:52:45.857954       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.859078392Z W0109 04:52:45.859046       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.859078392Z E0109 04:52:45.859072       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.900310171Z E0109 04:52:45.900269       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.901298480Z W0109 04:52:45.901261       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.901298480Z E0109 04:52:45.901286       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.982368510Z E0109 04:52:45.982326       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.983292384Z W0109 04:52:45.983264       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.983304005Z E0109 04:52:45.983296       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.144910969Z E0109 04:52:46.144852       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.145945616Z W0109 04:52:46.145921       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.145957890Z E0109 04:52:46.145948       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.467165578Z E0109 04:52:46.467109       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.468406214Z W0109 04:52:46.468348       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.468406214Z E0109 04:52:46.468391       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.028476488Z E0109 04:52:47.028432       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.109929012Z E0109 04:52:47.109880       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.111156335Z W0109 04:52:47.111129       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.111171891Z E0109 04:52:47.111157       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.230893777Z E0109 04:52:47.230853       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-controller-manager/recycler-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/csr_approver_clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:cluster-csr-approver-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/csr_approver_clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:cluster-csr-approver-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/gce/cloud-provider-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:kube-controller-manager:gce-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/gce/cloud-provider-binding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:kube-controller-manager:gce-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-kube-controller-manager-role-kube-system.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:openshift:leader-election-lock-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-kube-controller-manager-rolebinding-kube-system.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-election-lock-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-sa.yaml" (string): Delete "https://172.30.0.1:443/api/v1/namespaces/kube-system/serviceaccounts/vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-role.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:kube-controller-manager:vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-binding.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:kube-controller-manager:vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:52:47.428798257Z E0109 04:52:47.428760       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.429337837Z E0109 04:52:47.429315       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.435146619Z E0109 04:52:47.435120       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.455951891Z E0109 04:52:47.455921       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.496821300Z E0109 04:52:47.496782       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.577744651Z E0109 04:52:47.577704       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.738620731Z E0109 04:52:47.738570       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.060125008Z E0109 04:52:48.060081       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.392529905Z E0109 04:52:48.392483       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.393535636Z W0109 04:52:48.393495       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.393535636Z E0109 04:52:48.393526       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.428250979Z E0109 04:52:48.428213       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.700900572Z E0109 04:52:48.700855       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.629032353Z E0109 04:52:49.628959       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.982009936Z E0109 04:52:49.981956       1 base_controller.go:272] TargetConfigController reconciliation failed: Get "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.230178648Z E0109 04:52:50.230123       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-controller-manager/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-openshift-infra.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-infra": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/recycler-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/csr_approver_clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:cluster-csr-approver-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/csr_approver_clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:cluster-csr-approver-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/gce/cloud-provider-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:kube-controller-manager:gce-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/gce/cloud-provider-binding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:kube-controller-manager:gce-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-kube-controller-manager-role-kube-system.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/roles/system:openshift:leader-election-lock-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-kube-controller-manager-rolebinding-kube-system.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-election-lock-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-sa.yaml" (string): Delete "https://172.30.0.1:443/api/v1/namespaces/kube-system/serviceaccounts/vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-role.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:kube-controller-manager:vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/vsphere/legacy-cloud-provider-binding.yaml" (string): Delete "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:kube-controller-manager:vsphere-legacy-cloud-provider": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:52:50.828281739Z E0109 04:52:50.828243       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.955157287Z E0109 04:52:50.955112       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.956034527Z W0109 04:52:50.955986       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.956065599Z E0109 04:52:50.956035       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-controller-manager/poddisruptionbudgets/kube-controller-manager-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:52.028792946Z E0109 04:52:52.028755       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubecontrollermanagers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:57.373855614Z I0109 04:52:57.373815       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:57.487932747Z I0109 04:52:57.487892       1 request.go:601] Waited for 1.025717693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config/configmaps?resourceVersion=26444
2023-01-09T04:52:58.492137229Z I0109 04:52:58.492078       1 request.go:601] Waited for 1.512284898s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services?resourceVersion=26427
2023-01-09T04:52:59.687964178Z I0109 04:52:59.687925       1 request.go:601] Waited for 1.748945377s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/nodes?resourceVersion=26425
2023-01-09T04:52:59.910959262Z W0109 04:52:59.910925       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T04:53:00.834778234Z E0109 04:53:00.834734       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-controller-manager/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/namespace-openshift-infra.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-infra": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/recycler-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-controller-manager/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:53:00.835573969Z I0109 04:53:00.835533       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:00.846934161Z I0109 04:53:00.846884       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready" to "NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: "
2023-01-09T04:53:00.887758375Z I0109 04:53:00.887721       1 request.go:601] Waited for 2.107313047s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps?resourceVersion=26849
2023-01-09T04:53:01.499839924Z I0109 04:53:01.499795       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:02.087497985Z I0109 04:53:02.087455       1 request.go:601] Waited for 1.602090504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:03.088173336Z I0109 04:53:03.088138       1 request.go:601] Waited for 1.395398454s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:03.150318491Z I0109 04:53:03.150259       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.165876259Z E0109 04:53:03.165831       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:03.166605773Z I0109 04:53:03.166563       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.219352244Z E0109 04:53:03.219303       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:03.220380912Z I0109 04:53:03.220330       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.229929789Z E0109 04:53:03.229892       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:03.230511600Z I0109 04:53:03.230483       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.259738978Z E0109 04:53:03.259700       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:03.260357974Z I0109 04:53:03.260318       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.291185001Z E0109 04:53:03.291151       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:03.300962866Z I0109 04:53:03.300911       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: ","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.339882157Z E0109 04:53:03.339843       1 base_controller.go:272] StatusSyncer_kube-controller-manager reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-controller-manager": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:53:04.088215020Z I0109 04:53:04.088164       1 request.go:601] Waited for 1.186254308s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:04.108081166Z I0109 04:53:04.108038       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:08.512451332Z I0109 04:53:08.512405       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:08.522351405Z I0109 04:53:08.520784       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/ns.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-role.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/roles/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/leader-election-cluster-policy-controller-rolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-controller-manager/rolebindings/system:openshift:leader-election-lock-cluster-policy-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-security-allocation-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:namespace-security-allocation-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrole.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterroles/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/podsecurity-admission-label-syncer-controller-clusterrolebinding.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:controller:podsecurity-admission-label-syncer-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/namespace-openshift-infra.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/svc.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/recycler-sa.yaml\" (string): Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: \"assets/kube-controller-manager/localhost-recovery-client-crb.yaml\" (string): Get \"https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-controller-manager-recovery\": dial tcp 172.30.0.1:443: connect: connection refused\nKubeControllerManagerStaticResourcesDegraded: " to "NodeControllerDegraded: All master nodes are ready"
2023-01-09T04:53:09.687660068Z I0109 04:53:09.687618       1 request.go:601] Waited for 1.175250932s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:10.688027743Z I0109 04:53:10.687975       1 request.go:601] Waited for 1.196681974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:10.691714112Z I0109 04:53:10.691685       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:14.687734773Z I0109 04:53:14.687696       1 request.go:601] Waited for 1.031513817s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:15.688078041Z I0109 04:53:15.688040       1 request.go:601] Waited for 1.194371631s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:53:16.887648314Z I0109 04:53:16.887610       1 request.go:601] Waited for 1.395764934s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:17.291820654Z I0109 04:53:17.291778       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:53:17.887668540Z I0109 04:53:17.887627       1 request.go:601] Waited for 1.196775112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/trusted-ca-bundle
2023-01-09T04:53:20.291670950Z I0109 04:53:20.291630       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:53:29.091837858Z I0109 04:53:29.091796       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:53:31.291480966Z I0109 04:53:31.291438       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:53:37.487783798Z I0109 04:53:37.487736       1 request.go:601] Waited for 1.092409082s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:38.488166418Z I0109 04:53:38.488125       1 request.go:601] Waited for 1.196824275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:39.491382667Z I0109 04:53:39.491340       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:39.491382667Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:53:39.491382667Z  CurrentRevision: (int32) 7,
2023-01-09T04:53:39.491382667Z  TargetRevision: (int32) 0,
2023-01-09T04:53:39.491382667Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:39.491382667Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:39.491382667Z  LastFailedReason: (string) "",
2023-01-09T04:53:39.491382667Z  LastFailedCount: (int) 0,
2023-01-09T04:53:39.491382667Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:39.491382667Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:39.491382667Z }
2023-01-09T04:53:39.491382667Z  because static pod is ready
2023-01-09T04:53:39.501913904Z I0109 04:53:39.501847       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 6 to 7 because static pod is ready
2023-01-09T04:53:39.506690460Z I0109 04:53:39.506653       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:51:28Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:39.514846847Z I0109 04:53:39.514800       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7" to "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7"
2023-01-09T04:53:40.687621156Z I0109 04:53:40.687576       1 request.go:601] Waited for 1.180175102s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:41.887648740Z I0109 04:53:41.887603       1 request.go:601] Waited for 1.395909243s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:43.087595070Z I0109 04:53:43.087553       1 request.go:601] Waited for 1.194803969s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:43.292111421Z I0109 04:53:43.292070       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:43.292111421Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:53:43.292111421Z  CurrentRevision: (int32) 7,
2023-01-09T04:53:43.292111421Z  TargetRevision: (int32) 0,
2023-01-09T04:53:43.292111421Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:43.292111421Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:43.292111421Z  LastFailedReason: (string) "",
2023-01-09T04:53:43.292111421Z  LastFailedCount: (int) 0,
2023-01-09T04:53:43.292111421Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:43.292111421Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:43.292111421Z }
2023-01-09T04:53:43.292111421Z  because static pod is ready
2023-01-09T04:53:44.088044569Z I0109 04:53:44.088003       1 request.go:601] Waited for 1.196290736s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:48.693909645Z I0109 04:53:48.693868       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:53:48.693944223Z I0109 04:53:48.693910       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:48.693944223Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:53:48.693944223Z  CurrentRevision: (int32) 6,
2023-01-09T04:53:48.693944223Z  TargetRevision: (int32) 7,
2023-01-09T04:53:48.693944223Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:48.693944223Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:48.693944223Z  LastFailedReason: (string) "",
2023-01-09T04:53:48.693944223Z  LastFailedCount: (int) 0,
2023-01-09T04:53:48.693944223Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:48.693944223Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:48.693944223Z }
2023-01-09T04:53:48.707551723Z I0109 04:53:48.707499       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 6 to 7 because node ip-10-0-145-4.us-east-2.compute.internal with revision 6 is the oldest
2023-01-09T04:53:49.888039067Z I0109 04:53:49.887982       1 request.go:601] Waited for 1.179002824s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:50.893597787Z I0109 04:53:50.893547       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-controller-manager because it was missing
2023-01-09T04:53:51.087810945Z I0109 04:53:51.087768       1 request.go:601] Waited for 1.195307365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:51.891842883Z I0109 04:53:51.891803       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:53:52.088155412Z I0109 04:53:52.088112       1 request.go:601] Waited for 1.178727509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T04:53:53.287194559Z I0109 04:53:53.287150       1 request.go:601] Waited for 1.394381301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/installer-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:54.287972004Z I0109 04:53:54.287928       1 request.go:601] Waited for 1.195958657s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:54.491473950Z I0109 04:53:54.491430       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:55.487704315Z I0109 04:53:55.487666       1 request.go:601] Waited for 1.19568501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:56.691934227Z I0109 04:53:56.691888       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:54:25.029281047Z I0109 04:54:25.029231       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:54:27.835580973Z I0109 04:54:27.835533       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:37.051226525Z I0109 04:54:37.051183       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:39.245608823Z I0109 04:54:39.245571       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:41.246834340Z I0109 04:54:41.246793       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:47.219109894Z I0109 04:54:47.219067       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:49.812745614Z I0109 04:54:49.812700       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:55:12.058175855Z I0109 04:55:12.058132       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:55:37.855399547Z I0109 04:55:37.855349       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:55:48.284346733Z I0109 04:55:48.284299       1 request.go:601] Waited for 1.070791026s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:49.484366881Z I0109 04:55:49.484323       1 request.go:601] Waited for 1.195364368s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:50.286897378Z I0109 04:55:50.286856       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:55:50.286897378Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:55:50.286897378Z  CurrentRevision: (int32) 7,
2023-01-09T04:55:50.286897378Z  TargetRevision: (int32) 0,
2023-01-09T04:55:50.286897378Z  LastFailedRevision: (int32) 0,
2023-01-09T04:55:50.286897378Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:55:50.286897378Z  LastFailedReason: (string) "",
2023-01-09T04:55:50.286897378Z  LastFailedCount: (int) 0,
2023-01-09T04:55:50.286897378Z  LastFallbackCount: (int) 0,
2023-01-09T04:55:50.286897378Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:55:50.286897378Z }
2023-01-09T04:55:50.286897378Z  because static pod is ready
2023-01-09T04:55:50.298818692Z I0109 04:55:50.298775       1 status_controller.go:211] clusteroperator/kube-controller-manager diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:47:41Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:50Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:11Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:50.299087040Z I0109 04:55:50.299055       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 6 to 7 because static pod is ready
2023-01-09T04:55:50.309735299Z I0109 04:55:50.309673       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-controller-manager-operator", Name:"kube-controller-manager-operator", UID:"890950c6-ddc3-4ddb-9fcd-0d958a5e9fc7", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-controller-manager changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 7"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7"
2023-01-09T04:55:51.483711593Z I0109 04:55:51.483673       1 request.go:601] Waited for 1.185239433s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:52.484325541Z I0109 04:55:52.484286       1 request.go:601] Waited for 1.395130883s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:53.484332153Z I0109 04:55:53.484293       1 request.go:601] Waited for 1.197776917s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-signer-ca
2023-01-09T04:55:54.684328499Z I0109 04:55:54.684293       1 request.go:601] Waited for 1.195211892s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-controller-ca
2023-01-09T04:55:55.883643765Z I0109 04:55:55.883607       1 request.go:601] Waited for 1.197049709s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/csr-signer
2023-01-09T04:57:13.945741439Z W0109 04:57:13.945704       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:02:13.946377590Z W0109 05:02:13.946339       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:02:58.689228585Z I0109 05:02:58.689185       1 request.go:601] Waited for 1.101971035s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T05:02:59.889285206Z I0109 05:02:59.889239       1 request.go:601] Waited for 1.196864113s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T05:07:13.947342039Z W0109 05:07:13.947302       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:12:13.947389814Z W0109 05:12:13.947348       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:12:58.689768856Z I0109 05:12:58.689734       1 request.go:601] Waited for 1.100638334s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T05:17:13.947706367Z W0109 05:17:13.947664       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:22:13.948385212Z W0109 05:22:13.948346       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:22:58.690833515Z I0109 05:22:58.690789       1 request.go:601] Waited for 1.102621915s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T05:27:13.948794289Z W0109 05:27:13.948754       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:32:13.949674827Z W0109 05:32:13.949635       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:37:13.950209354Z W0109 05:37:13.950169       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:42:13.950461489Z W0109 05:42:13.950426       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:42:58.491488415Z I0109 05:42:58.491449       1 request.go:601] Waited for 1.191015405s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T05:47:13.951047480Z W0109 05:47:13.951009       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:52:13.951729892Z W0109 05:52:13.951690       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T05:52:57.452415995Z I0109 05:52:57.452370       1 request.go:601] Waited for 1.020301923s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T05:52:58.652360308Z I0109 05:52:58.652301       1 request.go:601] Waited for 1.351642372s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T05:52:59.851639173Z I0109 05:52:59.851603       1 request.go:601] Waited for 1.190022998s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T05:53:01.651940469Z I0109 05:53:01.651900       1 request.go:601] Waited for 1.16048113s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T05:53:02.852073763Z I0109 05:53:02.852038       1 request.go:601] Waited for 1.192098459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T05:57:13.952067176Z W0109 05:57:13.952024       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:02:13.953109012Z W0109 06:02:13.953071       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:02:57.314684156Z I0109 06:02:57.314645       1 request.go:601] Waited for 1.01873576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:02:58.513736123Z I0109 06:02:58.513695       1 request.go:601] Waited for 1.212836925s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:02:59.714253492Z I0109 06:02:59.714207       1 request.go:601] Waited for 1.19469187s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:03:00.714618901Z I0109 06:03:00.714565       1 request.go:601] Waited for 1.196625358s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:03:01.914152482Z I0109 06:03:01.914113       1 request.go:601] Waited for 1.19717462s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:03:03.114089776Z I0109 06:03:03.114047       1 request.go:601] Waited for 1.197437284s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:07:13.953261949Z W0109 06:07:13.953219       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:12:13.954228731Z W0109 06:12:13.954187       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:12:57.329085385Z I0109 06:12:57.329047       1 request.go:601] Waited for 1.035518492s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:12:58.329653992Z I0109 06:12:58.329611       1 request.go:601] Waited for 1.186293051s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:12:59.529110151Z I0109 06:12:59.529061       1 request.go:601] Waited for 1.196432599s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:13:00.529407464Z I0109 06:13:00.529361       1 request.go:601] Waited for 1.197222679s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/localhost-recovery-client-token
2023-01-09T06:13:01.730071861Z I0109 06:13:01.730029       1 request.go:601] Waited for 1.197081862s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/controller-manager-kubeconfig
2023-01-09T06:13:02.929984565Z I0109 06:13:02.929942       1 request.go:601] Waited for 1.196495508s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T06:17:13.955082560Z W0109 06:17:13.955041       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:22:13.956068777Z W0109 06:22:13.956022       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:22:57.487257001Z I0109 06:22:57.487211       1 request.go:601] Waited for 1.167928016s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T06:22:58.687019202Z I0109 06:22:58.686953       1 request.go:601] Waited for 1.384825511s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:22:59.687798851Z I0109 06:22:59.687762       1 request.go:601] Waited for 1.195531828s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:23:00.887650855Z I0109 06:23:00.887605       1 request.go:601] Waited for 1.197512519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:23:01.887752685Z I0109 06:23:01.887714       1 request.go:601] Waited for 1.195854019s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:23:03.087623376Z I0109 06:23:03.087587       1 request.go:601] Waited for 1.197433501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:27:13.956274041Z W0109 06:27:13.956219       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:32:13.956877872Z W0109 06:32:13.956831       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:32:57.355476146Z I0109 06:32:57.355435       1 request.go:601] Waited for 1.056326926s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:32:58.355749434Z I0109 06:32:58.355706       1 request.go:601] Waited for 1.198220386s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:32:59.554884313Z I0109 06:32:59.554846       1 request.go:601] Waited for 1.196750573s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:33:00.555089675Z I0109 06:33:00.555044       1 request.go:601] Waited for 1.196743519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:33:01.555616090Z I0109 06:33:01.555573       1 request.go:601] Waited for 1.061465419s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:33:02.755373547Z I0109 06:33:02.755332       1 request.go:601] Waited for 1.19434369s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:33:03.755677034Z I0109 06:33:03.755635       1 request.go:601] Waited for 1.197423607s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-controller-ca
2023-01-09T06:37:13.956973223Z W0109 06:37:13.956936       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:42:13.957766813Z W0109 06:42:13.957727       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:42:58.128466180Z I0109 06:42:58.128415       1 request.go:601] Waited for 1.147112411s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T06:42:59.128708692Z I0109 06:42:59.128664       1 request.go:601] Waited for 1.197065117s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:43:00.129367598Z I0109 06:43:00.129327       1 request.go:601] Waited for 1.196563622s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:43:01.528814275Z I0109 06:43:01.528772       1 request.go:601] Waited for 1.034279681s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:43:02.728797678Z I0109 06:43:02.728752       1 request.go:601] Waited for 1.194257868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:47:13.957940030Z W0109 06:47:13.957902       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:52:13.958050067Z W0109 06:52:13.958008       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T06:52:58.127384226Z I0109 06:52:58.127346       1 request.go:601] Waited for 1.145497644s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T06:52:59.131389445Z I0109 06:52:59.131338       1 request.go:601] Waited for 1.199228636s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:53:00.327190212Z I0109 06:53:00.327144       1 request.go:601] Waited for 1.192352369s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:53:01.527564580Z I0109 06:53:01.527513       1 request.go:601] Waited for 1.032342714s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T06:53:02.527676545Z I0109 06:53:02.527626       1 request.go:601] Waited for 1.197170742s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:57:13.958825904Z W0109 06:57:13.958776       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:02:13.959046960Z W0109 07:02:13.959003       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:02:58.692797723Z I0109 07:02:58.692760       1 request.go:601] Waited for 1.097324713s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:02:59.892572465Z I0109 07:02:59.892534       1 request.go:601] Waited for 1.195434601s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T07:07:13.960096266Z W0109 07:07:13.960051       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:12:13.960786369Z W0109 07:12:13.960747       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:12:58.698233312Z I0109 07:12:58.698191       1 request.go:601] Waited for 1.102664047s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:17:13.960828000Z W0109 07:17:13.960788       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:22:13.961678109Z W0109 07:22:13.961641       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:22:58.698303584Z I0109 07:22:58.698266       1 request.go:601] Waited for 1.098949779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:27:13.962578829Z W0109 07:27:13.962537       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:32:13.963278316Z W0109 07:32:13.963236       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:32:58.699654624Z I0109 07:32:58.699609       1 request.go:601] Waited for 1.098977293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:37:13.963682452Z W0109 07:37:13.963627       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:42:13.964611694Z W0109 07:42:13.964575       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:42:58.700096187Z I0109 07:42:58.700051       1 request.go:601] Waited for 1.10017771s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:47:13.965627293Z W0109 07:47:13.965588       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:52:13.966270514Z W0109 07:52:13.966229       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T07:52:58.700081787Z I0109 07:52:58.700042       1 request.go:601] Waited for 1.102327567s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T07:57:13.966801724Z W0109 07:57:13.966766       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:02:13.967166040Z W0109 08:02:13.967123       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:02:58.701156535Z I0109 08:02:58.701117       1 request.go:601] Waited for 1.099858863s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T08:07:13.967884564Z W0109 08:07:13.967842       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:12:13.968574000Z W0109 08:12:13.968532       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:12:58.701581374Z I0109 08:12:58.701542       1 request.go:601] Waited for 1.101232591s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T08:17:13.969290312Z W0109 08:17:13.969225       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:22:13.969611075Z W0109 08:22:13.969554       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:22:58.702549723Z I0109 08:22:58.702511       1 request.go:601] Waited for 1.101767709s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T08:22:59.905206271Z I0109 08:22:59.905158       1 request.go:601] Waited for 1.000376553s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2023-01-09T08:27:13.970679238Z W0109 08:27:13.970632       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:32:13.971057901Z W0109 08:32:13.971016       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:32:58.503386189Z I0109 08:32:58.503345       1 request.go:601] Waited for 1.192215101s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:32:59.703648104Z I0109 08:32:59.703602       1 request.go:601] Waited for 1.192638567s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:37:13.971624007Z W0109 08:37:13.971582       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:42:13.971683855Z W0109 08:42:13.971643       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:42:57.550644118Z I0109 08:42:57.550604       1 request.go:601] Waited for 1.180301507s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T08:42:58.551147735Z I0109 08:42:58.551107       1 request.go:601] Waited for 1.239043744s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:42:59.751400239Z I0109 08:42:59.751356       1 request.go:601] Waited for 1.191461644s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:43:01.550631146Z I0109 08:43:01.550588       1 request.go:601] Waited for 1.048230518s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:43:02.551203051Z I0109 08:43:02.551159       1 request.go:601] Waited for 1.197181826s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:47:13.972097859Z W0109 08:47:13.972055       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:52:13.972152326Z W0109 08:52:13.972108       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T08:52:57.424665311Z I0109 08:52:57.424623       1 request.go:601] Waited for 1.113987139s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:52:58.624461052Z I0109 08:52:58.624406       1 request.go:601] Waited for 1.311299398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T08:52:59.629700499Z I0109 08:52:59.627876       1 request.go:601] Waited for 1.201267133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:53:00.824897904Z I0109 08:53:00.824857       1 request.go:601] Waited for 1.19170454s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:53:02.024030221Z I0109 08:53:02.023963       1 request.go:601] Waited for 1.194434327s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:53:03.024947250Z I0109 08:53:03.024904       1 request.go:601] Waited for 1.195941445s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:57:13.972466552Z W0109 08:57:13.972423       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:02:13.973200370Z W0109 09:02:13.973155       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:02:57.883783998Z I0109 09:02:57.883736       1 request.go:601] Waited for 1.0287863s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T09:02:59.083654770Z I0109 09:02:59.083614       1 request.go:601] Waited for 1.174212098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T09:03:00.283184365Z I0109 09:03:00.283138       1 request.go:601] Waited for 1.197131686s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2023-01-09T09:03:01.683588350Z I0109 09:03:01.683539       1 request.go:601] Waited for 1.180306818s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:03:02.882763723Z I0109 09:03:02.882715       1 request.go:601] Waited for 1.191237877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:07:13.974017541Z W0109 09:07:13.973957       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:12:13.975079453Z W0109 09:12:13.975041       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:12:57.341742786Z I0109 09:12:57.341708       1 request.go:601] Waited for 1.035888627s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods?labelSelector=app%3Dinstaller
2023-01-09T09:12:58.342006110Z I0109 09:12:58.341954       1 request.go:601] Waited for 1.196891481s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:12:59.342452069Z I0109 09:12:59.342414       1 request.go:601] Waited for 1.397755551s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:13:00.342465471Z I0109 09:13:00.342427       1 request.go:601] Waited for 1.198366062s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T09:13:01.542612312Z I0109 09:13:01.542568       1 request.go:601] Waited for 1.038324555s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:13:02.741738197Z I0109 09:13:02.741688       1 request.go:601] Waited for 1.193963519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:13:03.742381286Z I0109 09:13:03.742339       1 request.go:601] Waited for 1.185599278s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T09:17:13.975872412Z W0109 09:17:13.975832       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:22:13.976017515Z W0109 09:22:13.975947       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:22:57.382158197Z I0109 09:22:57.382115       1 request.go:601] Waited for 1.073053487s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:22:58.581642813Z I0109 09:22:58.581603       1 request.go:601] Waited for 1.265332394s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:22:59.582227272Z I0109 09:22:59.582174       1 request.go:601] Waited for 1.198191704s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:23:00.782158469Z I0109 09:23:00.782124       1 request.go:601] Waited for 1.197579368s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:23:01.782174683Z I0109 09:23:01.782133       1 request.go:601] Waited for 1.19683465s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2023-01-09T09:23:02.782214461Z I0109 09:23:02.782177       1 request.go:601] Waited for 1.194877841s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:27:13.976573791Z W0109 09:27:13.976524       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:32:13.977025863Z W0109 09:32:13.976960       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:32:58.027818765Z I0109 09:32:58.027773       1 request.go:601] Waited for 1.035878445s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T09:32:59.027875958Z I0109 09:32:59.027828       1 request.go:601] Waited for 1.397570008s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:33:00.227305156Z I0109 09:33:00.227259       1 request.go:601] Waited for 1.196928561s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:33:01.627620420Z I0109 09:33:01.627580       1 request.go:601] Waited for 1.121753508s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:33:02.627819136Z I0109 09:33:02.627774       1 request.go:601] Waited for 1.197354812s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:37:13.977763165Z W0109 09:37:13.977705       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:42:13.978395351Z W0109 09:42:13.978348       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:42:58.081967989Z I0109 09:42:58.081927       1 request.go:601] Waited for 1.089521358s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T09:42:59.082327566Z I0109 09:42:59.082283       1 request.go:601] Waited for 1.195726786s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:43:00.281962257Z I0109 09:43:00.281917       1 request.go:601] Waited for 1.194667877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:43:01.682429510Z I0109 09:43:01.682369       1 request.go:601] Waited for 1.17532452s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:43:02.882342451Z I0109 09:43:02.882302       1 request.go:601] Waited for 1.193968182s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T09:47:13.978462077Z W0109 09:47:13.978422       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:52:13.979064443Z W0109 09:52:13.979024       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T09:52:58.739509608Z I0109 09:52:58.739465       1 request.go:601] Waited for 1.132548693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T09:52:59.939367099Z I0109 09:52:59.939326       1 request.go:601] Waited for 1.196827289s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T09:57:13.979852911Z W0109 09:57:13.979816       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:02:13.980134599Z W0109 10:02:13.980092       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:02:58.710168432Z I0109 10:02:58.710127       1 request.go:601] Waited for 1.103014676s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:07:13.981093237Z W0109 10:07:13.981049       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:12:13.981295612Z W0109 10:12:13.981257       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:12:58.721641288Z I0109 10:12:58.721594       1 request.go:601] Waited for 1.112984608s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:12:59.721675535Z I0109 10:12:59.721634       1 request.go:601] Waited for 1.197470156s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:17:13.981770471Z W0109 10:17:13.981728       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:22:13.982133362Z W0109 10:22:13.982094       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:22:58.713042052Z I0109 10:22:58.712981       1 request.go:601] Waited for 1.103645659s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:22:59.913760876Z I0109 10:22:59.913720       1 request.go:601] Waited for 1.19749367s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T10:27:13.982708682Z W0109 10:27:13.982662       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:32:13.983710320Z W0109 10:32:13.983666       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:32:58.713617251Z I0109 10:32:58.713580       1 request.go:601] Waited for 1.102730572s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:37:13.984065162Z W0109 10:37:13.984020       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:42:13.984424959Z W0109 10:42:13.984381       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:42:58.713883948Z I0109 10:42:58.713846       1 request.go:601] Waited for 1.104473974s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:42:59.714109723Z I0109 10:42:59.714067       1 request.go:601] Waited for 1.195668075s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:47:13.985107292Z W0109 10:47:13.985066       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:52:13.985533032Z W0109 10:52:13.985494       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T10:52:58.715640434Z I0109 10:52:58.715603       1 request.go:601] Waited for 1.105628418s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T10:52:59.915153496Z I0109 10:52:59.915114       1 request.go:601] Waited for 1.19505212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/secrets/serving-cert
2023-01-09T10:57:13.985906183Z W0109 10:57:13.985860       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:02:13.986840621Z W0109 11:02:13.986793       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:02:58.716183380Z I0109 11:02:58.716141       1 request.go:601] Waited for 1.105139666s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T11:07:13.987070503Z W0109 11:07:13.987029       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:12:13.987443350Z W0109 11:12:13.987397       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:12:58.334783869Z I0109 11:12:58.334723       1 request.go:601] Waited for 1.009963797s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:12:59.517045431Z I0109 11:12:59.516959       1 request.go:601] Waited for 1.176356663s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:17:13.987735215Z W0109 11:17:13.987683       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:22:13.988626029Z W0109 11:22:13.988583       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:22:58.516712321Z I0109 11:22:58.516672       1 request.go:601] Waited for 1.191670457s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:22:59.716824208Z I0109 11:22:59.716778       1 request.go:601] Waited for 1.18919182s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:27:13.988942346Z W0109 11:27:13.988901       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:32:13.989691609Z W0109 11:32:13.989651       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:32:57.339311515Z I0109 11:32:57.339271       1 request.go:601] Waited for 1.017684836s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:32:58.538961283Z I0109 11:32:58.538913       1 request.go:601] Waited for 1.213367629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:32:59.539376383Z I0109 11:32:59.539330       1 request.go:601] Waited for 1.19696168s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:33:00.739105502Z I0109 11:33:00.739058       1 request.go:601] Waited for 1.197186993s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:33:01.739680614Z I0109 11:33:01.739635       1 request.go:601] Waited for 1.198416152s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:33:02.939653669Z I0109 11:33:02.939611       1 request.go:601] Waited for 1.196476096s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:37:13.990221127Z W0109 11:37:13.990172       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:42:13.990542580Z W0109 11:42:13.990500       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:42:57.423067632Z I0109 11:42:57.423027       1 request.go:601] Waited for 1.102262849s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:42:58.622830260Z I0109 11:42:58.622786       1 request.go:601] Waited for 1.296365267s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:42:59.822785211Z I0109 11:42:59.822738       1 request.go:601] Waited for 1.192740531s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:43:00.823099757Z I0109 11:43:00.823056       1 request.go:601] Waited for 1.197916498s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:43:02.022867942Z I0109 11:43:02.022816       1 request.go:601] Waited for 1.19711893s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:43:03.222497875Z I0109 11:43:03.222445       1 request.go:601] Waited for 1.196962423s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:47:13.991186749Z W0109 11:47:13.991143       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:52:13.992113391Z W0109 11:52:13.992070       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T11:52:57.504623508Z I0109 11:52:57.504581       1 request.go:601] Waited for 1.176164918s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T11:52:58.504909242Z I0109 11:52:58.504862       1 request.go:601] Waited for 1.198048625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:52:59.504972316Z I0109 11:52:59.504925       1 request.go:601] Waited for 1.197427598s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:53:00.704572490Z I0109 11:53:00.704529       1 request.go:601] Waited for 1.196634447s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:53:01.904685075Z I0109 11:53:01.904642       1 request.go:601] Waited for 1.197699941s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:53:02.904855838Z I0109 11:53:02.904802       1 request.go:601] Waited for 1.193476487s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T11:57:13.992837791Z W0109 11:57:13.992795       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:02:13.992909849Z W0109 12:02:13.992867       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:02:57.385488697Z I0109 12:02:57.385442       1 request.go:601] Waited for 1.063482398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:02:58.585441070Z I0109 12:02:58.585396       1 request.go:601] Waited for 1.25671075s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:02:59.785183188Z I0109 12:02:59.785140       1 request.go:601] Waited for 1.190075082s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:03:00.785823213Z I0109 12:03:00.785781       1 request.go:601] Waited for 1.197996458s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:03:01.985430168Z I0109 12:03:01.985388       1 request.go:601] Waited for 1.196995123s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:03:02.985699724Z I0109 12:03:02.985654       1 request.go:601] Waited for 1.197495922s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:07:13.993280834Z W0109 12:07:13.993228       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:12:13.993740704Z W0109 12:12:13.993697       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:12:58.053630732Z I0109 12:12:58.053590       1 request.go:601] Waited for 1.048994052s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T12:12:59.253550074Z I0109 12:12:59.253504       1 request.go:601] Waited for 1.174678503s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T12:13:00.453398201Z I0109 12:13:00.453352       1 request.go:601] Waited for 1.19738665s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/services/kube-controller-manager
2023-01-09T12:13:01.653316018Z I0109 12:13:01.653270       1 request.go:601] Waited for 1.136183331s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:13:02.853191538Z I0109 12:13:02.853138       1 request.go:601] Waited for 1.192707589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:17:13.994301810Z W0109 12:17:13.994246       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:22:13.994869295Z W0109 12:22:13.994824       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:22:58.134023745Z I0109 12:22:58.133962       1 request.go:601] Waited for 1.128581108s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T12:22:59.134177744Z I0109 12:22:59.134130       1 request.go:601] Waited for 1.195524106s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:23:00.334092425Z I0109 12:23:00.334048       1 request.go:601] Waited for 1.196605385s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:23:01.533853377Z I0109 12:23:01.533816       1 request.go:601] Waited for 1.01598355s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:23:02.534300108Z I0109 12:23:02.534257       1 request.go:601] Waited for 1.197374094s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:27:13.995834649Z W0109 12:27:13.995786       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:32:13.996751183Z W0109 12:32:13.996704       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:32:58.184623184Z I0109 12:32:58.184572       1 request.go:601] Waited for 1.17904054s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T12:32:59.383890904Z I0109 12:32:59.383848       1 request.go:601] Waited for 1.173995628s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T12:33:00.384174655Z I0109 12:33:00.384129       1 request.go:601] Waited for 1.195214316s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:33:01.584888370Z I0109 12:33:01.584838       1 request.go:601] Waited for 1.066705458s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:33:02.784501295Z I0109 12:33:02.784457       1 request.go:601] Waited for 1.193814917s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T12:37:13.997292970Z W0109 12:37:13.997253       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:42:13.997820679Z W0109 12:42:13.997778       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:42:58.669491107Z I0109 12:42:58.669454       1 request.go:601] Waited for 1.050239246s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T12:42:59.669511726Z I0109 12:42:59.669470       1 request.go:601] Waited for 1.197798687s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:43:01.482474997Z I0109 12:43:01.482092       1 request.go:601] Waited for 1.010163254s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:47:13.998119860Z W0109 12:47:13.998077       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:52:13.999087531Z W0109 12:52:13.999040       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T12:52:58.722581580Z I0109 12:52:58.722540       1 request.go:601] Waited for 1.104971672s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T12:57:13.999663620Z W0109 12:57:13.999625       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:02:14.000198777Z W0109 13:02:14.000155       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:02:58.722684829Z I0109 13:02:58.722639       1 request.go:601] Waited for 1.104506227s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T13:07:14.000766867Z W0109 13:07:14.000723       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:12:14.001666790Z W0109 13:12:14.001626       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:12:58.723096076Z I0109 13:12:58.723057       1 request.go:601] Waited for 1.09949641s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T13:17:14.001904089Z W0109 13:17:14.001864       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:22:14.002476364Z W0109 13:22:14.002429       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:22:58.723491509Z I0109 13:22:58.723450       1 request.go:601] Waited for 1.10025293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T13:27:14.003388066Z W0109 13:27:14.003343       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:32:14.004077028Z W0109 13:32:14.004036       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:32:58.724775769Z I0109 13:32:58.724736       1 request.go:601] Waited for 1.102469241s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T13:37:14.004680807Z W0109 13:37:14.004637       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:42:14.005756956Z W0109 13:42:14.005718       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:42:58.724972067Z I0109 13:42:58.724922       1 request.go:601] Waited for 1.099326015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T13:47:14.006062111Z W0109 13:47:14.006017       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:52:14.007012509Z W0109 13:52:14.006947       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T13:52:58.530458029Z I0109 13:52:58.530413       1 request.go:601] Waited for 1.00105956s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T13:52:59.725358971Z I0109 13:52:59.725316       1 request.go:601] Waited for 1.191241774s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:57:14.007804479Z W0109 13:57:14.007758       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:02:14.008341335Z W0109 14:02:14.008295       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:02:58.726337893Z I0109 14:02:58.726298       1 request.go:601] Waited for 1.10311978s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T14:07:14.008882272Z W0109 14:07:14.008844       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:12:14.009171148Z W0109 14:12:14.009124       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:12:58.532641457Z I0109 14:12:58.532603       1 request.go:601] Waited for 1.194070033s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T14:12:59.732333096Z I0109 14:12:59.732292       1 request.go:601] Waited for 1.194502398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T14:17:14.009792751Z W0109 14:17:14.009750       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:22:14.010241198Z W0109 14:22:14.010196       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:22:57.403439634Z I0109 14:22:57.403387       1 request.go:601] Waited for 1.071208326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:22:58.403881094Z I0109 14:22:58.403818       1 request.go:601] Waited for 1.197391045s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:22:59.404138566Z I0109 14:22:59.404095       1 request.go:601] Waited for 1.393523395s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/kube-controller-manager-sa
2023-01-09T14:23:00.604207028Z I0109 14:23:00.604159       1 request.go:601] Waited for 1.196872748s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra/serviceaccounts/pv-recycler-controller
2023-01-09T14:23:01.803666049Z I0109 14:23:01.803628       1 request.go:601] Waited for 1.194325099s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/localhost-recovery-client
2023-01-09T14:23:03.003866178Z I0109 14:23:03.003822       1 request.go:601] Waited for 1.178829755s due to client-side throttling, not priority and fairness, request: DELETE:https://172.30.0.1:443/api/v1/namespaces/kube-system/serviceaccounts/vsphere-legacy-cloud-provider
2023-01-09T14:27:14.010929326Z W0109 14:27:14.010888       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:32:14.011655976Z W0109 14:32:14.011613       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:32:57.481790695Z I0109 14:32:57.481752       1 request.go:601] Waited for 1.14913252s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:32:58.681792974Z I0109 14:32:58.681749       1 request.go:601] Waited for 1.342097795s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T14:32:59.881659910Z I0109 14:32:59.881623       1 request.go:601] Waited for 1.192568972s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T14:33:00.881878865Z I0109 14:33:00.881831       1 request.go:601] Waited for 1.198105828s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:33:02.082186758Z I0109 14:33:02.082140       1 request.go:601] Waited for 1.197717869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:33:03.281673705Z I0109 14:33:03.281631       1 request.go:601] Waited for 1.196937274s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:37:14.012753928Z W0109 14:37:14.012707       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:42:14.013763966Z W0109 14:42:14.013725       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:42:57.536369296Z I0109 14:42:57.536331       1 request.go:601] Waited for 1.195335613s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager-operator/configmaps/csr-controller-ca
2023-01-09T14:42:58.736712710Z I0109 14:42:58.736672       1 request.go:601] Waited for 1.396916144s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T14:42:59.737176268Z I0109 14:42:59.737133       1 request.go:601] Waited for 1.197510014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:43:00.936814737Z I0109 14:43:00.936777       1 request.go:601] Waited for 1.195292064s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:43:01.937178271Z I0109 14:43:01.937141       1 request.go:601] Waited for 1.196563052s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:43:02.937211889Z I0109 14:43:02.937170       1 request.go:601] Waited for 1.195539237s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:47:14.014740084Z W0109 14:47:14.014695       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:52:14.014974606Z W0109 14:52:14.014932       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T14:52:57.429299512Z I0109 14:52:57.429255       1 request.go:601] Waited for 1.091259512s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:52:58.429681334Z I0109 14:52:58.429637       1 request.go:601] Waited for 1.197513779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:52:59.630028202Z I0109 14:52:59.629979       1 request.go:601] Waited for 1.197138288s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:53:00.829746153Z I0109 14:53:00.829694       1 request.go:601] Waited for 1.19720009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:53:02.029714074Z I0109 14:53:02.029672       1 request.go:601] Waited for 1.197462654s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:53:03.029769215Z I0109 14:53:03.029732       1 request.go:601] Waited for 1.198010166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:57:14.015615556Z W0109 14:57:14.015571       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:02:14.016030846Z W0109 15:02:14.015956       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:02:58.074448352Z I0109 15:02:58.074406       1 request.go:601] Waited for 1.058789179s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T15:02:59.274476808Z I0109 15:02:59.274431       1 request.go:601] Waited for 1.395498505s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:03:00.474092503Z I0109 15:03:00.474052       1 request.go:601] Waited for 1.196449078s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T15:03:01.674082922Z I0109 15:03:01.674040       1 request.go:601] Waited for 1.146133772s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T15:03:02.674357700Z I0109 15:03:02.674310       1 request.go:601] Waited for 1.196389258s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:07:14.016780542Z W0109 15:07:14.016733       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:12:14.016883559Z W0109 15:12:14.016841       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:12:58.139425500Z I0109 15:12:58.139386       1 request.go:601] Waited for 1.123730791s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T15:12:59.338969732Z I0109 15:12:59.338917       1 request.go:601] Waited for 1.17794014s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T15:13:00.339078517Z I0109 15:13:00.339039       1 request.go:601] Waited for 1.197175285s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T15:13:01.539862526Z I0109 15:13:01.539825       1 request.go:601] Waited for 1.0114641s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T15:13:02.738920753Z I0109 15:13:02.738877       1 request.go:601] Waited for 1.192241621s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T15:17:14.017394001Z W0109 15:17:14.017348       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:22:14.018356006Z W0109 15:22:14.018322       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:22:58.186400684Z I0109 15:22:58.186355       1 request.go:601] Waited for 1.170222877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager
2023-01-09T15:22:59.385685149Z I0109 15:22:59.385650       1 request.go:601] Waited for 1.172276043s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-infra
2023-01-09T15:23:00.386195461Z I0109 15:23:00.386153       1 request.go:601] Waited for 1.196778s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T15:23:01.586174658Z I0109 15:23:01.586135       1 request.go:601] Waited for 1.056590148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T15:23:02.785651936Z I0109 15:23:02.785606       1 request.go:601] Waited for 1.194043057s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/serviceaccounts/installer-sa
2023-01-09T15:27:14.018705439Z W0109 15:27:14.018664       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:32:14.019015961Z W0109 15:32:14.018951       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
2023-01-09T15:32:58.691873640Z I0109 15:32:58.691828       1 request.go:601] Waited for 1.062705153s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/configmaps/config
2023-01-09T15:32:59.692097999Z I0109 15:32:59.692055       1 request.go:601] Waited for 1.19615063s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-controller-manager/pods/kube-controller-manager-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:37:14.020036887Z W0109 15:37:14.019965       1 gcwatcher_controller.go:189] missing required alerts: GarbageCollectorSyncFailed
