2023-01-09T04:42:23.538158955Z I0109 04:42:23.538068       1 cmd.go:209] Using service-serving-cert provided certificates
2023-01-09T04:42:23.538448583Z I0109 04:42:23.538430       1 observer_polling.go:159] Starting file observer
2023-01-09T04:42:23.552647629Z I0109 04:42:23.552612       1 builder.go:262] openshift-cluster-kube-scheduler-operator version 4.12.0-202301042354.p0.ge0b6bf9.assembly.stream-e0b6bf9-e0b6bf9c4ddb0da9268d504d23ca2ca11880d970
2023-01-09T04:42:23.553227915Z I0109 04:42:23.553206       1 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:24.170522832Z I0109 04:42:24.170488       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2023-01-09T04:42:24.172612489Z I0109 04:42:24.172582       1 maxinflight.go:140] "Initialized nonMutatingChan" len=400
2023-01-09T04:42:24.172612489Z I0109 04:42:24.172599       1 maxinflight.go:146] "Initialized mutatingChan" len=200
2023-01-09T04:42:24.172639047Z I0109 04:42:24.172621       1 maxinflight.go:117] "Set denominator for readonly requests" limit=400
2023-01-09T04:42:24.172639047Z I0109 04:42:24.172628       1 maxinflight.go:121] "Set denominator for mutating requests" limit=200
2023-01-09T04:42:24.175036683Z W0109 04:42:24.175013       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:24.175036683Z W0109 04:42:24.175031       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:24.175144460Z I0109 04:42:24.175018       1 genericapiserver.go:480] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2023-01-09T04:42:24.177452125Z I0109 04:42:24.177422       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2023-01-09T04:42:24.177452125Z I0109 04:42:24.177430       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-01-09T04:42:24.177452125Z I0109 04:42:24.177442       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2023-01-09T04:42:24.177481507Z I0109 04:42:24.177467       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:24.177481507Z I0109 04:42:24.177469       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:24.177493474Z I0109 04:42:24.177469       1 shared_informer.go:255] Waiting for caches to sync for RequestHeaderAuthRequestController
2023-01-09T04:42:24.177695458Z I0109 04:42:24.177673       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:24.177753171Z I0109 04:42:24.177735       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:19 +0000 UTC to 2025-01-08 04:41:20 +0000 UTC (now=2023-01-09 04:42:24.17766867 +0000 UTC))"
2023-01-09T04:42:24.177806040Z I0109 04:42:24.177788       1 leaderelection.go:248] attempting to acquire leader lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock...
2023-01-09T04:42:24.177885038Z I0109 04:42:24.177870       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239344\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239343\" (2023-01-09 03:42:23 +0000 UTC to 2024-01-09 03:42:23 +0000 UTC (now=2023-01-09 04:42:24.177853698 +0000 UTC))"
2023-01-09T04:42:24.177906314Z I0109 04:42:24.177894       1 secure_serving.go:210] Serving securely on [::]:8443
2023-01-09T04:42:24.177920869Z I0109 04:42:24.177912       1 genericapiserver.go:585] [graceful-termination] waiting for shutdown to be initiated
2023-01-09T04:42:24.177936372Z I0109 04:42:24.177926       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2023-01-09T04:42:24.190534283Z I0109 04:42:24.190507       1 leaderelection.go:258] successfully acquired lease openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock
2023-01-09T04:42:24.190639740Z I0109 04:42:24.190585       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"bec00702-468e-4305-abac-afabc5555437", APIVersion:"v1", ResourceVersion:"9731", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-5646746647-rqljw_fe2af043-a2ae-422c-a769-a384f355bc97 became leader
2023-01-09T04:42:24.190652111Z I0109 04:42:24.190636       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-cluster-kube-scheduler-operator-lock", UID:"e3909e67-6ad2-458c-8eb2-01d1d9e8a3e8", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"9732", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' openshift-kube-scheduler-operator-5646746647-rqljw_fe2af043-a2ae-422c-a769-a384f355bc97 became leader
2023-01-09T04:42:24.196027713Z I0109 04:42:24.195966       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.196536726Z I0109 04:42:24.196510       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.196634974Z I0109 04:42:24.196593       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.196977600Z I0109 04:42:24.196950       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.197033628Z I0109 04:42:24.197013       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.197170620Z I0109 04:42:24.197147       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.197480933Z I0109 04:42:24.197452       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2023-01-09T04:42:24.198479483Z I0109 04:42:24.198426       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2023-01-09T04:42:24.198550152Z I0109 04:42:24.198473       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2023-01-09T04:42:24.198586758Z I0109 04:42:24.198488       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2023-01-09T04:42:24.198621463Z I0109 04:42:24.198558       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2023-01-09T04:42:24.198650614Z I0109 04:42:24.198598       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_kube-scheduler
2023-01-09T04:42:24.198681170Z I0109 04:42:24.198622       1 base_controller.go:67] Waiting for caches to sync for NodeController
2023-01-09T04:42:24.198710437Z I0109 04:42:24.198625       1 base_controller.go:67] Waiting for caches to sync for GuardController
2023-01-09T04:42:24.198757933Z I0109 04:42:24.198731       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-01-09T04:42:24.198807770Z I0109 04:42:24.198792       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2023-01-09T04:42:24.198859893Z I0109 04:42:24.198641       1 base_controller.go:67] Waiting for caches to sync for KubeControllerManagerStaticResources
2023-01-09T04:42:24.198893844Z I0109 04:42:24.198804       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2023-01-09T04:42:24.198924089Z I0109 04:42:24.198909       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2023-01-09T04:42:24.198967769Z I0109 04:42:24.198758       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2023-01-09T04:42:24.199017566Z I0109 04:42:24.198759       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2023-01-09T04:42:24.199063700Z I0109 04:42:24.198773       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2023-01-09T04:42:24.199063700Z I0109 04:42:24.198821       1 base_controller.go:67] Waiting for caches to sync for PruneController
2023-01-09T04:42:24.199090141Z I0109 04:42:24.198984       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:24.277620341Z I0109 04:42:24.277575       1 shared_informer.go:262] Caches are synced for RequestHeaderAuthRequestController
2023-01-09T04:42:24.277620341Z I0109 04:42:24.277586       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:24.277620341Z I0109 04:42:24.277597       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:24.277824335Z I0109 04:42:24.277804       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:42:24.277770377 +0000 UTC))"
2023-01-09T04:42:24.278033875Z I0109 04:42:24.278017       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:19 +0000 UTC to 2025-01-08 04:41:20 +0000 UTC (now=2023-01-09 04:42:24.277976913 +0000 UTC))"
2023-01-09T04:42:24.278146014Z I0109 04:42:24.278129       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239344\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239343\" (2023-01-09 03:42:23 +0000 UTC to 2024-01-09 03:42:23 +0000 UTC (now=2023-01-09 04:42:24.278115985 +0000 UTC))"
2023-01-09T04:42:24.278265110Z I0109 04:42:24.278250       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:21 +0000 UTC to 2033-01-06 04:28:21 +0000 UTC (now=2023-01-09 04:42:24.278236615 +0000 UTC))"
2023-01-09T04:42:24.278282810Z I0109 04:42:24.278273       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:42:24.278263134 +0000 UTC))"
2023-01-09T04:42:24.278304709Z I0109 04:42:24.278292       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:42:24.278281005 +0000 UTC))"
2023-01-09T04:42:24.278322364Z I0109 04:42:24.278312       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:42:24.278301926 +0000 UTC))"
2023-01-09T04:42:24.278339440Z I0109 04:42:24.278329       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2033-01-06 04:28:22 +0000 UTC (now=2023-01-09 04:42:24.278319695 +0000 UTC))"
2023-01-09T04:42:24.278353059Z I0109 04:42:24.278347       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:42:24.278337651 +0000 UTC))"
2023-01-09T04:42:24.278457553Z I0109 04:42:24.278444       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:19 +0000 UTC to 2025-01-08 04:41:20 +0000 UTC (now=2023-01-09 04:42:24.278431329 +0000 UTC))"
2023-01-09T04:42:24.278551703Z I0109 04:42:24.278538       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239344\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239343\" (2023-01-09 03:42:23 +0000 UTC to 2024-01-09 03:42:23 +0000 UTC (now=2023-01-09 04:42:24.278522997 +0000 UTC))"
2023-01-09T04:42:24.297862927Z I0109 04:42:24.297828       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2023-01-09T04:42:24.297862927Z I0109 04:42:24.297848       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2023-01-09T04:42:24.298928737Z I0109 04:42:24.298897       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-01-09T04:42:24.298928737Z I0109 04:42:24.298911       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-01-09T04:42:24.298958868Z I0109 04:42:24.298923       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2023-01-09T04:42:24.298958868Z I0109 04:42:24.298935       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2023-01-09T04:42:24.298958868Z I0109 04:42:24.298943       1 base_controller.go:73] Caches are synced for StatusSyncer_kube-scheduler 
2023-01-09T04:42:24.298958868Z I0109 04:42:24.298948       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2023-01-09T04:42:24.298972601Z I0109 04:42:24.298955       1 base_controller.go:110] Starting #1 worker of StatusSyncer_kube-scheduler controller ...
2023-01-09T04:42:24.298972601Z I0109 04:42:24.298962       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2023-01-09T04:42:24.299074952Z I0109 04:42:24.299046       1 base_controller.go:73] Caches are synced for InstallerController 
2023-01-09T04:42:24.299140313Z I0109 04:42:24.299113       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2023-01-09T04:42:24.299173197Z I0109 04:42:24.299133       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2023-01-09T04:42:24.299173197Z I0109 04:42:24.299117       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2023-01-09T04:42:24.299200832Z I0109 04:42:24.299084       1 base_controller.go:73] Caches are synced for BackingResourceController 
2023-01-09T04:42:24.299443445Z I0109 04:42:24.299221       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2023-01-09T04:42:24.299477259Z I0109 04:42:24.299102       1 base_controller.go:73] Caches are synced for PruneController 
2023-01-09T04:42:24.299477259Z I0109 04:42:24.299251       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2023-01-09T04:42:24.299477259Z I0109 04:42:24.299083       1 base_controller.go:73] Caches are synced for InstallerStateController 
2023-01-09T04:42:24.299477259Z I0109 04:42:24.299303       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2023-01-09T04:42:24.299776291Z I0109 04:42:24.299759       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:24.300046382Z I0109 04:42:24.300022       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.307824527Z E0109 04:42:24.307801       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.308837526Z I0109 04:42:24.308810       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:24.308896991Z I0109 04:42:24.308869       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.308912153Z E0109 04:42:24.308899       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.309326977Z I0109 04:42:24.309304       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:24.313561918Z I0109 04:42:24.313530       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.313586264Z E0109 04:42:24.313573       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.315612136Z I0109 04:42:24.315583       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]"
2023-01-09T04:42:24.334348662Z I0109 04:42:24.334300       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.334400497Z E0109 04:42:24.334377       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.374754621Z I0109 04:42:24.374698       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.374780775Z E0109 04:42:24.374767       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.455347700Z I0109 04:42:24.455303       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.455382372Z E0109 04:42:24.455355       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.616424306Z I0109 04:42:24.616371       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4
2023-01-09T04:42:24.616459229Z E0109 04:42:24.616440       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]
2023-01-09T04:42:24.699060806Z I0109 04:42:24.699020       1 base_controller.go:73] Caches are synced for ConfigObserver 
2023-01-09T04:42:24.699060806Z I0109 04:42:24.699043       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2023-01-09T04:42:24.899459165Z I0109 04:42:24.899416       1 base_controller.go:73] Caches are synced for KubeControllerManagerStaticResources 
2023-01-09T04:42:24.899459165Z I0109 04:42:24.899436       1 base_controller.go:73] Caches are synced for RevisionController 
2023-01-09T04:42:24.899459165Z I0109 04:42:24.899446       1 base_controller.go:110] Starting #1 worker of KubeControllerManagerStaticResources controller ...
2023-01-09T04:42:24.899498331Z I0109 04:42:24.899454       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2023-01-09T04:42:24.899498331Z I0109 04:42:24.899437       1 base_controller.go:73] Caches are synced for TargetConfigController 
2023-01-09T04:42:24.899498331Z I0109 04:42:24.899471       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2023-01-09T04:42:25.099065922Z I0109 04:42:25.099028       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2023-01-09T04:42:25.099065922Z I0109 04:42:25.099049       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2023-01-09T04:42:25.299310303Z I0109 04:42:25.299272       1 base_controller.go:73] Caches are synced for GuardController 
2023-01-09T04:42:25.299310303Z I0109 04:42:25.299304       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2023-01-09T04:42:25.299367079Z I0109 04:42:25.299283       1 base_controller.go:73] Caches are synced for NodeController 
2023-01-09T04:42:25.299378123Z I0109 04:42:25.299369       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2023-01-09T04:42:25.301552315Z E0109 04:42:25.301535       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.301565560Z E0109 04:42:25.301554       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.301573012Z E0109 04:42:25.301564       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.301744932Z E0109 04:42:25.301732       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.309019931Z E0109 04:42:25.308976       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.309019931Z E0109 04:42:25.309012       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.309048971Z E0109 04:42:25.309023       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.309186668Z E0109 04:42:25.309172       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.321391926Z E0109 04:42:25.321358       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.321391926Z E0109 04:42:25.321380       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.321448172Z E0109 04:42:25.321390       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.321537274Z E0109 04:42:25.321522       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.343770306Z E0109 04:42:25.343741       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.343770306Z E0109 04:42:25.343766       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.343794930Z E0109 04:42:25.343777       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.343934730Z E0109 04:42:25.343919       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.386096586Z E0109 04:42:25.386054       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.386096586Z E0109 04:42:25.386081       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.386136016Z E0109 04:42:25.386096       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.386273957Z E0109 04:42:25.386256       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.398200334Z I0109 04:42:25.398175       1 request.go:601] Waited for 1.098763422s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:42:25.469397566Z E0109 04:42:25.469362       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.469397566Z E0109 04:42:25.469383       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.469433278Z E0109 04:42:25.469394       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.476466325Z E0109 04:42:25.476433       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:42:25.476985411Z I0109 04:42:25.476913       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:25.477670856Z I0109 04:42:25.477644       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:25.484215742Z I0109 04:42:25.483776       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]"
2023-01-09T04:42:25.508824993Z E0109 04:42:25.508789       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.508911924Z E0109 04:42:25.508884       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.508944757Z E0109 04:42:25.508909       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.518183576Z E0109 04:42:25.518147       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:25.518503488Z I0109 04:42:25.518472       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:25.519449179Z I0109 04:42:25.519422       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:25.526578914Z I0109 04:42:25.526542       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]"
2023-01-09T04:42:25.639505306Z E0109 04:42:25.639470       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:25.639505306Z E0109 04:42:25.639495       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:25.639535293Z E0109 04:42:25.639506       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:25.639682500Z E0109 04:42:25.639668       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:26.282335947Z E0109 04:42:26.282294       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:26.282335947Z E0109 04:42:26.282327       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:26.282374712Z E0109 04:42:26.282341       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:26.282560494Z E0109 04:42:26.282543       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:27.287182241Z E0109 04:42:27.287136       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:27.287213265Z E0109 04:42:27.287176       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:27.287213265Z E0109 04:42:27.287194       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:27.287461022Z E0109 04:42:27.287438       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:27.293103294Z E0109 04:42:27.293060       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:27.293144617Z E0109 04:42:27.293110       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:27.293144617Z E0109 04:42:27.293126       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:27.293427183Z E0109 04:42:27.293389       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:27.295597961Z E0109 04:42:27.295548       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:27.295597961Z E0109 04:42:27.295592       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:27.295626437Z E0109 04:42:27.295607       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:27.295836091Z E0109 04:42:27.295808       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:27.400042035Z I0109 04:42:27.400004       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:27.407496565Z I0109 04:42:27.407460       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:27.407900861Z I0109 04:42:27.407871       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:27.413948214Z I0109 04:42:27.413904       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nInstallerControllerDegraded: missing required resources: [secrets: kube-scheduler-client-cert-key, secrets: localhost-recovery-client-token-4]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:42:27.565729678Z E0109 04:42:27.565694       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:27.565729678Z E0109 04:42:27.565719       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:27.565760496Z E0109 04:42:27.565730       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:27.565908729Z E0109 04:42:27.565894       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:28.364954108Z E0109 04:42:28.364906       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:28.364954108Z E0109 04:42:28.364940       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:28.364983808Z E0109 04:42:28.364954       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:28.365223221Z E0109 04:42:28.365183       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:28.597962405Z I0109 04:42:28.597923       1 request.go:601] Waited for 1.189943842s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:29.601062993Z I0109 04:42:29.601026       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:31.200091714Z I0109 04:42:31.200048       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:40.907282455Z E0109 04:42:40.907243       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:40.907313034Z E0109 04:42:40.907278       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:40.907313034Z E0109 04:42:40.907295       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:40.916404568Z E0109 04:42:40.916369       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:42:40.917216463Z I0109 04:42:40.917182       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:40.917823953Z I0109 04:42:40.917789       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:40.922452987Z I0109 04:42:40.922422       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:40.925534724Z I0109 04:42:40.925495       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:42:42.882455834Z E0109 04:42:42.882410       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:42.882512955Z E0109 04:42:42.882445       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:42.882512955Z E0109 04:42:42.882474       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:42.891097683Z E0109 04:42:42.891053       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:42.891753308Z I0109 04:42:42.891715       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:42.892489261Z I0109 04:42:42.892460       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:42.898038671Z I0109 04:42:42.897951       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:42:44.319754297Z I0109 04:42:44.319710       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:44.623788516Z E0109 04:42:44.623752       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:44.623788516Z E0109 04:42:44.623776       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:44.623836906Z E0109 04:42:44.623791       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:44.624022013Z E0109 04:42:44.623954       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:48.048270413Z E0109 04:42:48.048231       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:48.048270413Z E0109 04:42:48.048255       1 guard_controller.go:268] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:48.048331599Z E0109 04:42:48.048266       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:48.048432165Z E0109 04:42:48.048418       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:58.446637970Z E0109 04:42:58.446597       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:58.446718252Z E0109 04:42:58.446684       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:58.446718252Z E0109 04:42:58.446705       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:58.449808448Z I0109 04:42:58.449775       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:58.457158369Z E0109 04:42:58.457131       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:58.457676832Z I0109 04:42:58.457650       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:58.458356587Z I0109 04:42:58.458327       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:58.464253816Z I0109 04:42:58.464214       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "NodeControllerDegraded: All master nodes are ready\nGuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:42:58.579098964Z I0109 04:42:58.579048       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "kube-scheduler" changed from "" to "1.25.4"
2023-01-09T04:42:58.579098964Z I0109 04:42:58.579076       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:58.579302022Z I0109 04:42:58.579281       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"kube-scheduler","version":"1.25.4"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:42:58.585946505Z I0109 04:42:58.585911       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"kube-scheduler","version":"1.25.4"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:42:58.586271241Z I0109 04:42:58.586238       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: status.versions changed from [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"}] to [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"} {"kube-scheduler" "1.25.4"} {"operator" "4.12.0-0.nightly-2023-01-08-142418"}]
2023-01-09T04:42:58.586336902Z I0109 04:42:58.586316       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "kube-scheduler" changed from "" to "1.25.4"
2023-01-09T04:42:58.586375949Z I0109 04:42:58.586358       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/kube-scheduler version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:58.589938087Z E0109 04:42:58.589910       1 base_controller.go:272] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:59.368565063Z E0109 04:42:59.368525       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:59.368565063Z E0109 04:42:59.368548       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:59.368565063Z E0109 04:42:59.368560       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:59.368699391Z E0109 04:42:59.368687       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:00.178389544Z I0109 04:43:00.178346       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:00.376248197Z I0109 04:43:00.376199       1 request.go:601] Waited for 1.009581373s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:01.384092540Z E0109 04:43:01.384045       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:01.384092540Z E0109 04:43:01.384068       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:01.384092540Z E0109 04:43:01.384080       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:01.384219276Z E0109 04:43:01.384206       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:02.778532595Z I0109 04:43:02.778494       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:43:04.578613923Z I0109 04:43:04.578571       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:43:05.001330204Z I0109 04:43:05.001288       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:05.011252028Z I0109 04:43:05.011203       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from False to True ("GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]")
2023-01-09T04:43:05.388413654Z I0109 04:43:05.388369       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/serviceaccount-ca -n openshift-kube-scheduler:
2023-01-09T04:43:05.388413654Z cause by changes in data.ca-bundle.crt
2023-01-09T04:43:05.389416800Z I0109 04:43:05.389370       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "configmap/serviceaccount-ca has changed"
2023-01-09T04:43:05.980843168Z I0109 04:43:05.980789       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:06.331114210Z E0109 04:43:06.331062       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:06.580247730Z I0109 04:43:06.580196       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-pod-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:07.375842256Z I0109 04:43:07.375804       1 request.go:601] Waited for 1.046226841s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:07.980575703Z I0109 04:43:07.980524       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:08.376347070Z I0109 04:43:08.376306       1 request.go:601] Waited for 1.184209417s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:09.381265862Z I0109 04:43:09.381212       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:09.576429920Z I0109 04:43:09.576390       1 request.go:601] Waited for 1.397579503s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2023-01-09T04:43:09.978606395Z I0109 04:43:09.978567       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:43:10.381222060Z E0109 04:43:10.381182       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:10.381291873Z I0109 04:43:10.381262       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:10.392479433Z E0109 04:43:10.392439       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:10.395033917Z I0109 04:43:10.393158       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:10.395033917Z I0109 04:43:10.393725       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:10.395033917Z E0109 04:43:10.394598       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:10.398981962Z I0109 04:43:10.398916       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:43:10.776191960Z I0109 04:43:10.776147       1 request.go:601] Waited for 1.394922761s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps
2023-01-09T04:43:10.780142596Z I0109 04:43:10.780090       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/scheduler-kubeconfig-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:11.776327917Z I0109 04:43:11.776283       1 request.go:601] Waited for 1.382689227s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:43:12.382070091Z I0109 04:43:12.382016       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:12.975846751Z I0109 04:43:12.975805       1 request.go:601] Waited for 1.597252335s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:13.378845716Z E0109 04:43:13.378809       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:13.379005966Z E0109 04:43:13.378973       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:13.381102682Z E0109 04:43:13.381077       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:13.780359945Z I0109 04:43:13.780311       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:13.976679596Z I0109 04:43:13.976645       1 request.go:601] Waited for 1.396531219s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:43:14.378372612Z I0109 04:43:14.378332       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:43:15.176128664Z I0109 04:43:15.176095       1 request.go:601] Waited for 1.395771368s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets
2023-01-09T04:43:15.180232550Z I0109 04:43:15.180192       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-5 -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:15.188089143Z I0109 04:43:15.188041       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 4 created because configmap/serviceaccount-ca has changed
2023-01-09T04:43:15.188209423Z I0109 04:43:15.188185       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:15.977940394Z E0109 04:43:15.977899       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.978118177Z E0109 04:43:15.978099       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:15.980081092Z E0109 04:43:15.980054       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:16.176663194Z I0109 04:43:16.176620       1 request.go:601] Waited for 1.198568742s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2023-01-09T04:43:17.376471835Z I0109 04:43:17.376434       1 request.go:601] Waited for 1.197666362s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2023-01-09T04:43:17.980535172Z I0109 04:43:17.980493       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:43:17.989669725Z I0109 04:43:17.989632       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:17.990334412Z I0109 04:43:17.990304       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:18.001580543Z I0109 04:43:18.001541       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 4" to "StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5"
2023-01-09T04:43:18.177803429Z E0109 04:43:18.177765       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:18.177940490Z E0109 04:43:18.177927       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:18.978697658Z I0109 04:43:18.978642       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:18.978697658Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:18.978697658Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:18.978697658Z  TargetRevision: (int32) 5,
2023-01-09T04:43:18.978697658Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:18.978697658Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:18.978697658Z  LastFailedReason: (string) "",
2023-01-09T04:43:18.978697658Z  LastFailedCount: (int) 0,
2023-01-09T04:43:18.978697658Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:18.978697658Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:18.978697658Z }
2023-01-09T04:43:18.978697658Z  because new revision pending
2023-01-09T04:43:18.987402915Z I0109 04:43:18.987363       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:19.175744550Z I0109 04:43:19.175707       1 request.go:601] Waited for 1.185398258s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:43:20.980712544Z I0109 04:43:20.980643       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:43:20.983053265Z E0109 04:43:20.983017       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:21.778493104Z I0109 04:43:21.778453       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:43:22.176603956Z I0109 04:43:22.176563       1 request.go:601] Waited for 1.193088517s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:23.376062130Z I0109 04:43:23.376026       1 request.go:601] Waited for 1.197200539s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:23.378140056Z E0109 04:43:23.378106       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:23.378338625Z E0109 04:43:23.378321       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:23.380355830Z E0109 04:43:23.380325       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:24.177978088Z I0109 04:43:24.177935       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:24.575953437Z I0109 04:43:24.575917       1 request.go:601] Waited for 1.195229426s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:25.776062438Z I0109 04:43:25.776026       1 request.go:601] Waited for 1.397445773s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:25.978584810Z E0109 04:43:25.978548       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:25.978737856Z E0109 04:43:25.978722       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:26.776079485Z I0109 04:43:26.776042       1 request.go:601] Waited for 1.397537501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:26.778105226Z I0109 04:43:26.778084       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:32.008092304Z E0109 04:43:32.008043       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:32.017897308Z E0109 04:43:32.017859       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:32.018163268Z E0109 04:43:32.018140       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:44.189406805Z E0109 04:43:44.189369       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:44.197429873Z E0109 04:43:44.197394       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:44.197582799Z E0109 04:43:44.197567       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:45.435214362Z E0109 04:43:45.435177       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:45.442191087Z E0109 04:43:45.442158       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:45.442378156Z E0109 04:43:45.442361       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:53.555874179Z E0109 04:43:53.555828       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:53.559356311Z I0109 04:43:53.559323       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:53.565722401Z E0109 04:43:53.565693       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:53.565832798Z E0109 04:43:53.565820       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:55.568086117Z E0109 04:43:55.568050       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:55.576234214Z E0109 04:43:55.576198       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:55.576379770Z E0109 04:43:55.576358       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:55.577263996Z I0109 04:43:55.577239       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because waiting for static pod of revision 5, found 4
2023-01-09T04:44:02.532607686Z E0109 04:44:02.532564       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:02.532607686Z E0109 04:44:02.532597       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:02.556206973Z I0109 04:44:02.556165       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:02.557415909Z I0109 04:44:02.557369       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:02.559592070Z E0109 04:44:02.559557       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:02.568085288Z I0109 04:44:02.568048       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:02.572060879Z I0109 04:44:02.568559       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:44:02.575175687Z E0109 04:44:02.575133       1 base_controller.go:272] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:44:03.146206501Z I0109 04:44:03.146119       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because waiting for static pod of revision 5, found 4
2023-01-09T04:44:03.340543054Z E0109 04:44:03.340500       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:03.340543054Z E0109 04:44:03.340525       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:04.533603790Z I0109 04:44:04.533564       1 request.go:601] Waited for 1.194697321s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2023-01-09T04:44:05.533630760Z I0109 04:44:05.533579       1 request.go:601] Waited for 1.396361957s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:44:06.136647141Z E0109 04:44:06.136603       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:06.533685273Z I0109 04:44:06.533637       1 request.go:601] Waited for 1.196482327s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:06.537142932Z I0109 04:44:06.537110       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:07.734071619Z I0109 04:44:07.734023       1 request.go:601] Waited for 1.196080502s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:08.537445052Z E0109 04:44:08.537401       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:08.537445052Z E0109 04:44:08.537432       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:08.537606851Z E0109 04:44:08.537591       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:08.933641114Z I0109 04:44:08.933594       1 request.go:601] Waited for 1.196240414s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:09.736959339Z I0109 04:44:09.736919       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:14.844566094Z E0109 04:44:14.844524       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:14.855183165Z E0109 04:44:14.855150       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:14.866374865Z E0109 04:44:14.866337       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:14.868211147Z I0109 04:44:14.868179       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:14.868623271Z I0109 04:44:14.868592       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:14.875856186Z I0109 04:44:14.875810       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:14.876214834Z I0109 04:44:14.876185       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:14.882070839Z E0109 04:44:14.882039       1 base_controller.go:272] StatusSyncer_kube-scheduler reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "kube-scheduler": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:44:15.249389691Z I0109 04:44:15.249345       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:15.905276388Z E0109 04:44:15.905235       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:17.057792134Z E0109 04:44:17.057753       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:17.057932591Z E0109 04:44:17.057917       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.600149659Z E0109 04:44:38.600102       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:38.640550770Z E0109 04:44:38.640506       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.640958780Z E0109 04:44:38.640927       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.656509590Z E0109 04:44:38.655247       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:38.669310924Z E0109 04:44:38.669270       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.669898390Z E0109 04:44:38.669502       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.798224750Z E0109 04:44:38.798191       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.798224750Z E0109 04:44:38.798215       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:38.885766913Z E0109 04:44:38.885729       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:38.886168454Z I0109 04:44:38.886134       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:38.886836842Z I0109 04:44:38.886805       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:38.902540237Z I0109 04:44:38.902499       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:44:40.205425698Z I0109 04:44:40.205383       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:40.409695735Z E0109 04:44:40.409642       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:40.409695735Z E0109 04:44:40.409686       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:41.404916338Z E0109 04:44:41.404876       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:42.939753619Z E0109 04:44:42.939717       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:43.607163106Z E0109 04:44:43.607119       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:43.616219974Z E0109 04:44:43.616186       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:43.616886111Z I0109 04:44:43.616855       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:43.617142270Z I0109 04:44:43.617110       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:04Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 5","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:43.619401333Z E0109 04:44:43.619378       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:43.624743095Z I0109 04:44:43.624708       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:44.801765781Z I0109 04:44:44.801727       1 request.go:601] Waited for 1.181983432s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:45.805392055Z E0109 04:44:45.805355       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:45.805535974Z E0109 04:44:45.805521       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:45.843611064Z E0109 04:44:45.843575       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:46.605284742Z I0109 04:44:46.605244       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:44:47.405697114Z E0109 04:44:47.405659       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:47.405839513Z E0109 04:44:47.405826       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:48.382124611Z E0109 04:44:48.382080       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:49.005692475Z E0109 04:44:49.005649       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:49.005834333Z E0109 04:44:49.005818       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:50.255354360Z I0109 04:44:50.255312       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:21 +0000 UTC to 2033-01-06 04:28:21 +0000 UTC (now=2023-01-09 04:44:50.255282773 +0000 UTC))"
2023-01-09T04:44:50.255387007Z I0109 04:44:50.255360       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.255338039 +0000 UTC))"
2023-01-09T04:44:50.255398305Z I0109 04:44:50.255387       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.255369904 +0000 UTC))"
2023-01-09T04:44:50.255446742Z I0109 04:44:50.255418       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.255397044 +0000 UTC))"
2023-01-09T04:44:50.255482347Z I0109 04:44:50.255448       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2033-01-06 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.255435627 +0000 UTC))"
2023-01-09T04:44:50.255482347Z I0109 04:44:50.255466       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1673239265\" [] issuer=\"kubelet-signer\" (2023-01-09 04:41:04 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.25545534 +0000 UTC))"
2023-01-09T04:44:50.255494679Z I0109 04:44:50.255488       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1673239266\" [] issuer=\"<self>\" (2023-01-09 04:41:06 +0000 UTC to 2024-01-09 04:41:07 +0000 UTC (now=2023-01-09 04:44:50.255475437 +0000 UTC))"
2023-01-09T04:44:50.255527282Z I0109 04:44:50.255512       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.255494739 +0000 UTC))"
2023-01-09T04:44:50.255714876Z I0109 04:44:50.255700       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-kube-scheduler-operator.svc\" [serving] validServingFor=[metrics.openshift-kube-scheduler-operator.svc,metrics.openshift-kube-scheduler-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:19 +0000 UTC to 2025-01-08 04:41:20 +0000 UTC (now=2023-01-09 04:44:50.255679051 +0000 UTC))"
2023-01-09T04:44:50.256957350Z I0109 04:44:50.256928       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239344\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239343\" (2023-01-09 03:42:23 +0000 UTC to 2024-01-09 03:42:23 +0000 UTC (now=2023-01-09 04:44:50.256895844 +0000 UTC))"
2023-01-09T04:44:53.393281675Z E0109 04:44:53.393237       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:53.399507328Z I0109 04:44:53.399398       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:44:53.399507328Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:44:53.399507328Z  CurrentRevision: (int32) 5,
2023-01-09T04:44:53.399507328Z  TargetRevision: (int32) 0,
2023-01-09T04:44:53.399507328Z  LastFailedRevision: (int32) 0,
2023-01-09T04:44:53.399507328Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:44:53.399507328Z  LastFailedReason: (string) "",
2023-01-09T04:44:53.399507328Z  LastFailedCount: (int) 0,
2023-01-09T04:44:53.399507328Z  LastFallbackCount: (int) 0,
2023-01-09T04:44:53.399507328Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:44:53.399507328Z }
2023-01-09T04:44:53.399507328Z  because static pod is ready
2023-01-09T04:44:53.401820594Z E0109 04:44:53.401788       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:53.402021351Z E0109 04:44:53.401976       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:53.408494502Z I0109 04:44:53.408413       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 0 to 5 because static pod is ready
2023-01-09T04:44:53.409264602Z I0109 04:44:53.409222       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:53.410022586Z I0109 04:44:53.409969       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:53.417907147Z I0109 04:44:53.417872       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5",Available changed from False to True ("StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5")
2023-01-09T04:44:54.591633785Z I0109 04:44:54.591591       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal static pod not found and needs new revision 5
2023-01-09T04:44:54.591681098Z I0109 04:44:54.591640       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:44:54.591681098Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:44:54.591681098Z  CurrentRevision: (int32) 0,
2023-01-09T04:44:54.591681098Z  TargetRevision: (int32) 5,
2023-01-09T04:44:54.591681098Z  LastFailedRevision: (int32) 0,
2023-01-09T04:44:54.591681098Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:44:54.591681098Z  LastFailedReason: (string) "",
2023-01-09T04:44:54.591681098Z  LastFailedCount: (int) 0,
2023-01-09T04:44:54.591681098Z  LastFallbackCount: (int) 0,
2023-01-09T04:44:54.591681098Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:44:54.591681098Z }
2023-01-09T04:44:54.601282455Z I0109 04:44:54.601232       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 5 because node ip-10-0-145-4.us-east-2.compute.internal static pod not found
2023-01-09T04:44:54.601888028Z I0109 04:44:54.601856       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:56.398348419Z I0109 04:44:56.398279       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:44:57.391155223Z I0109 04:44:57.391116       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:44:57.587281884Z I0109 04:44:57.587242       1 request.go:601] Waited for 1.184682794s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:58.587882880Z I0109 04:44:58.587839       1 request.go:601] Waited for 1.195964979s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:58.799952803Z E0109 04:44:58.799914       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:58.799952803Z E0109 04:44:58.799939       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:58.813194603Z E0109 04:44:58.813153       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:44:58.813879107Z I0109 04:44:58.813841       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:58.813948545Z I0109 04:44:58.813927       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:58.821573555Z E0109 04:44:58.821539       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:58.825251237Z I0109 04:44:58.825166       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:44:59.591385945Z I0109 04:44:59.591342       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:44:59.987808696Z I0109 04:44:59.987768       1 request.go:601] Waited for 1.165839758s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:00.988086209Z I0109 04:45:00.988047       1 request.go:601] Waited for 1.010227217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:01.191384501Z E0109 04:45:01.191337       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:01.201187834Z E0109 04:45:01.201148       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:01.201538493Z I0109 04:45:01.201505       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:01.202115425Z I0109 04:45:01.202085       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:01.204531315Z E0109 04:45:01.204512       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:01.209445441Z I0109 04:45:01.209409       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:45:01.991127861Z I0109 04:45:01.991087       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:02.387699551Z I0109 04:45:02.387662       1 request.go:601] Waited for 1.182818779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:03.391099265Z E0109 04:45:03.391054       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:03.391244822Z E0109 04:45:03.391230       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:03.394436789Z E0109 04:45:03.394416       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:03.394452299Z E0109 04:45:03.394443       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:04.191389755Z I0109 04:45:04.191347       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:05.206583331Z E0109 04:45:05.206543       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:45:05.207049533Z I0109 04:45:05.206983       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:05.207604907Z I0109 04:45:05.207573       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:05.212415885Z E0109 04:45:05.212389       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:05.212415885Z E0109 04:45:05.212411       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:05.224542021Z I0109 04:45:05.224496       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:45:06.388200199Z I0109 04:45:06.388158       1 request.go:601] Waited for 1.175382617s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:07.191326623Z I0109 04:45:07.191289       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:07.413158514Z E0109 04:45:07.413118       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:45:07.508121536Z E0109 04:45:07.508080       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:08.792269377Z E0109 04:45:08.792224       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:08.801785941Z E0109 04:45:08.801725       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:08.803672612Z I0109 04:45:08.803633       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:08.804155419Z I0109 04:45:08.804126       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:08.806829008Z E0109 04:45:08.806156       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:08.813872100Z I0109 04:45:08.813817       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:45:09.987430128Z I0109 04:45:09.987388       1 request.go:601] Waited for 1.180678139s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:10.591214148Z I0109 04:45:10.591169       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:10.992285073Z E0109 04:45:10.992248       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:10.992487331Z E0109 04:45:10.992470       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:10.995882812Z E0109 04:45:10.995831       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:12.392428733Z E0109 04:45:12.392387       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:12.392568881Z E0109 04:45:12.392553       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:16.517723473Z E0109 04:45:16.517684       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:16.524577405Z E0109 04:45:16.524531       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:16.524726477Z E0109 04:45:16.524710       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:17.526012474Z E0109 04:45:17.525956       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:17.533228328Z E0109 04:45:17.533192       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:17.533351061Z E0109 04:45:17.533337       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:23.408849940Z E0109 04:45:23.408807       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:23.957158648Z E0109 04:45:23.957119       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:23.957297498Z E0109 04:45:23.957281       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:23.961713032Z E0109 04:45:23.961671       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:23.961713032Z E0109 04:45:23.961698       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:24.365223597Z E0109 04:45:24.365177       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:45:24.365771247Z I0109 04:45:24.365739       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:24.366535719Z I0109 04:45:24.366506       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:24.368287643Z E0109 04:45:24.368236       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:24.373831367Z I0109 04:45:24.373792       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]"
2023-01-09T04:45:25.554665196Z I0109 04:45:25.554623       1 request.go:601] Waited for 1.188283125s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-5-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:26.555101280Z I0109 04:45:26.555051       1 request.go:601] Waited for 1.197723594s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/serviceaccount-ca
2023-01-09T04:45:26.761902994Z I0109 04:45:26.761866       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:26.960863566Z E0109 04:45:26.960822       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:26.975210162Z E0109 04:45:26.975174       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:26.975890068Z I0109 04:45:26.975826       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:26.976046610Z I0109 04:45:26.975982       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:26.980055647Z E0109 04:45:26.980030       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:26.989007117Z I0109 04:45:26.988270       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-199-219.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:45:27.754043540Z I0109 04:45:27.753985       1 request.go:601] Waited for 1.191699322s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:45:28.755133330Z I0109 04:45:28.755087       1 request.go:601] Waited for 1.194597095s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:45:29.159436896Z I0109 04:45:29.159398       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:29.358765904Z E0109 04:45:29.358710       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:29.358886001Z E0109 04:45:29.358872       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:29.361916142Z E0109 04:45:29.361888       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:29.954772603Z I0109 04:45:29.954734       1 request.go:601] Waited for 1.194907151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2023-01-09T04:45:31.362371427Z E0109 04:45:31.362331       1 guard_controller.go:268] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:31.362637584Z E0109 04:45:31.362620       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:31.366583598Z E0109 04:45:31.366549       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:31.754664877Z I0109 04:45:31.754623       1 request.go:601] Waited for 1.162294971s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:32.954609159Z I0109 04:45:32.954566       1 request.go:601] Waited for 1.395555319s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:34.154408876Z I0109 04:45:34.154368       1 request.go:601] Waited for 1.396977704s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:34.157477709Z E0109 04:45:34.157451       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:34.165603812Z E0109 04:45:34.165562       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:34.166021175Z I0109 04:45:34.165954       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:34.166651660Z I0109 04:45:34.166621       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:34.168527494Z E0109 04:45:34.168503       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:34.174889488Z I0109 04:45:34.174834       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:45:34.757450484Z I0109 04:45:34.757410       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:35.354257959Z I0109 04:45:35.354219       1 request.go:601] Waited for 1.187852878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:45:36.354542247Z I0109 04:45:36.354504       1 request.go:601] Waited for 1.197361625s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:45:36.558726263Z E0109 04:45:36.558658       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:36.559057148Z E0109 04:45:36.559034       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:37.554905523Z I0109 04:45:37.554864       1 request.go:601] Waited for 1.307292312s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:45:38.357115708Z I0109 04:45:38.357075       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:38.754526645Z I0109 04:45:38.754486       1 request.go:601] Waited for 1.192283985s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:45:40.154632690Z I0109 04:45:40.154595       1 request.go:601] Waited for 1.049698787s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:41.354257608Z I0109 04:45:41.354218       1 request.go:601] Waited for 1.19458645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:41.957217405Z I0109 04:45:41.957177       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:42.354566827Z I0109 04:45:42.354528       1 request.go:601] Waited for 1.197216865s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2023-01-09T04:45:42.562198694Z E0109 04:45:42.562156       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:42.562237426Z I0109 04:45:42.562198       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:45:42.572756607Z I0109 04:45:42.572711       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:42.573356964Z I0109 04:45:42.573325       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:42.573975004Z E0109 04:45:42.573938       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:42.578218601Z E0109 04:45:42.578192       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:42.584926032Z I0109 04:45:42.581838       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-199-219.us-east-2.compute.internal, Missing PodIP in operand openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:45:43.754023607Z I0109 04:45:43.753972       1 request.go:601] Waited for 1.180819783s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:45:44.754088827Z I0109 04:45:44.754052       1 request.go:601] Waited for 1.396578377s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:45:45.557514323Z I0109 04:45:45.557471       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:45.754660299Z I0109 04:45:45.754620       1 request.go:601] Waited for 1.185705102s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2023-01-09T04:45:46.954569292Z I0109 04:45:46.954526       1 request.go:601] Waited for 1.19685845s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa
2023-01-09T04:45:47.358584406Z E0109 04:45:47.358553       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:47.363012368Z E0109 04:45:47.362967       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:48.757437679Z I0109 04:45:48.757388       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 5, but has not made progress because static pod is pending
2023-01-09T04:45:50.157203081Z E0109 04:45:50.157164       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:50.160121979Z E0109 04:45:50.160099       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:52.357508846Z E0109 04:45:52.357462       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:58.843392909Z E0109 04:45:58.843354       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:58.857258954Z E0109 04:45:58.857208       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:59.554419505Z E0109 04:45:59.554377       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:59.569007602Z E0109 04:45:59.568956       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:05.905468014Z I0109 04:46:05.905423       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "secret/localhost-recovery-client-token has changed"
2023-01-09T04:46:06.321077010Z I0109 04:46:06.321020       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:06.919677204Z I0109 04:46:06.919608       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-pod-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:07.519111979Z I0109 04:46:07.519038       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/config-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:08.120936554Z I0109 04:46:08.120869       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/serviceaccount-ca-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:08.719693097Z I0109 04:46:08.719640       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/scheduler-kubeconfig-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:09.319402470Z I0109 04:46:09.319352       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/kube-scheduler-cert-syncer-kubeconfig-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:09.919631324Z I0109 04:46:09.919576       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/serving-cert-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:10.520726518Z I0109 04:46:10.520680       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/localhost-recovery-client-token-6 -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:10.529193494Z I0109 04:46:10.529152       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 5 created because secret/localhost-recovery-client-token has changed
2023-01-09T04:46:10.529867629Z I0109 04:46:10.529836       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:10.530515312Z I0109 04:46:10.530485       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "secret/localhost-recovery-client-token has changed"
2023-01-09T04:46:10.532958513Z W0109 04:46:10.532934       1 staticpod.go:38] revision 6 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:46:10.542222456Z E0109 04:46:10.542198       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 6
2023-01-09T04:46:10.542599691Z I0109 04:46:10.542580       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:10.543067151Z I0109 04:46:10.543047       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 6","reason":"GuardController_SyncError::RevisionController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:10.555105660Z I0109 04:46:10.551785       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal" to "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 6"
2023-01-09T04:46:10.555105660Z I0109 04:46:10.554735       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:10.557183865Z I0109 04:46:10.557155       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:10.568053398Z I0109 04:46:10.564575       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 6" to "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:46:11.118593972Z I0109 04:46:11.118537       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:46:11.118593972Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:46:11.118593972Z  CurrentRevision: (int32) 0,
2023-01-09T04:46:11.118593972Z  TargetRevision: (int32) 6,
2023-01-09T04:46:11.118593972Z  LastFailedRevision: (int32) 0,
2023-01-09T04:46:11.118593972Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:46:11.118593972Z  LastFailedReason: (string) "",
2023-01-09T04:46:11.118593972Z  LastFailedCount: (int) 0,
2023-01-09T04:46:11.118593972Z  LastFallbackCount: (int) 0,
2023-01-09T04:46:11.118593972Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:46:11.118593972Z }
2023-01-09T04:46:11.118593972Z  because new revision pending
2023-01-09T04:46:11.127568840Z I0109 04:46:11.127531       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:11.128051049Z I0109 04:46:11.127984       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:11.135193480Z I0109 04:46:11.135160       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6"
2023-01-09T04:46:13.123720099Z I0109 04:46:13.123662       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:46:13.917978215Z I0109 04:46:13.917939       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:46:14.314389877Z I0109 04:46:14.314356       1 request.go:601] Waited for 1.186370343s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:15.315065255Z I0109 04:46:15.315026       1 request.go:601] Waited for 1.195811935s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:46:15.519201739Z E0109 04:46:15.519167       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:16.315090021Z I0109 04:46:16.315052       1 request.go:601] Waited for 1.184527047s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:16.318019188Z I0109 04:46:16.317960       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:17.515103229Z I0109 04:46:17.515055       1 request.go:601] Waited for 1.196273413s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:17.718184717Z E0109 04:46:17.718149       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:17.721222947Z E0109 04:46:17.721189       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:18.518054730Z I0109 04:46:18.518015       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:21.118401016Z E0109 04:46:21.118361       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:30.278900947Z E0109 04:46:30.278843       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:30.281132762Z E0109 04:46:30.281097       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:30.914312393Z E0109 04:46:30.914275       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:30.920043612Z I0109 04:46:30.920015       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:31.118366486Z E0109 04:46:31.118335       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:38.749985004Z E0109 04:46:38.749951       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:38.766475668Z E0109 04:46:38.766440       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:45.265667576Z I0109 04:46:45.265628       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:45.267005396Z E0109 04:46:45.266954       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:45.809198013Z E0109 04:46:45.809158       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:47.281131464Z E0109 04:46:47.281094       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:47.288549984Z I0109 04:46:47.288508       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:46:47.609688681Z E0109 04:46:47.609647       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:48.144552486Z E0109 04:46:48.144514       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:48.409925722Z E0109 04:46:48.409891       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:57.179275637Z E0109 04:46:57.179214       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:57.190771256Z I0109 04:46:57.190734       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:46:58.574161283Z E0109 04:46:58.574119       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:58.578204297Z E0109 04:46:58.578178       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:46:59.573417365Z I0109 04:46:59.573379       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:01.573779275Z E0109 04:47:01.573744       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:01.773501911Z I0109 04:47:01.773463       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:02.573604507Z E0109 04:47:02.573566       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:03.773807667Z E0109 04:47:03.773766       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:03.777946915Z E0109 04:47:03.777913       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:04.776769987Z I0109 04:47:04.776724       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:05.375632860Z E0109 04:47:05.375584       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:06.668316933Z E0109 04:47:06.668274       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:06.678089038Z E0109 04:47:06.678051       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:13.173940030Z E0109 04:47:13.173899       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:14.373528213Z E0109 04:47:14.373486       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:18.874715381Z E0109 04:47:18.874680       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:18.874849251Z E0109 04:47:18.874835       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:30.818080435Z E0109 04:47:30.818041       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:30.840373891Z E0109 04:47:30.840331       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:31.506660146Z E0109 04:47:31.506624       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:31.525222045Z E0109 04:47:31.525188       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:35.837052728Z E0109 04:47:35.835677       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:37.464718456Z E0109 04:47:37.464680       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:37.468540483Z E0109 04:47:37.468508       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:39.062453426Z E0109 04:47:39.062415       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:39.862267642Z E0109 04:47:39.862223       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:40.263130287Z E0109 04:47:40.263095       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:40.304153917Z E0109 04:47:40.304120       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:41.061886931Z E0109 04:47:41.061847       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:41.461574549Z E0109 04:47:41.461540       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:41.864608096Z E0109 04:47:41.864569       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:42.663376818Z E0109 04:47:42.663336       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:42.663521553Z E0109 04:47:42.663508       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:45.921058189Z E0109 04:47:45.921018       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:45.937750983Z E0109 04:47:45.937718       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:47.241792135Z I0109 04:47:47.241750       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:47.241792135Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:47:47.241792135Z  CurrentRevision: (int32) 6,
2023-01-09T04:47:47.241792135Z  TargetRevision: (int32) 0,
2023-01-09T04:47:47.241792135Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:47.241792135Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:47.241792135Z  LastFailedReason: (string) "",
2023-01-09T04:47:47.241792135Z  LastFailedCount: (int) 0,
2023-01-09T04:47:47.241792135Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:47.241792135Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:47.241792135Z }
2023-01-09T04:47:47.241792135Z  because static pod is ready
2023-01-09T04:47:47.245885198Z E0109 04:47:47.245850       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:47.252083092Z I0109 04:47:47.252042       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 6 because static pod is ready
2023-01-09T04:47:47.252484163Z I0109 04:47:47.252422       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:47.258656132Z I0109 04:47:47.253156       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:47.268983366Z I0109 04:47:47.267717       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 5; 0 nodes have achieved new revision 6" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6"
2023-01-09T04:47:47.429634425Z E0109 04:47:47.429584       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:48.426328669Z I0109 04:47:48.426281       1 request.go:601] Waited for 1.173342132s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:47:49.429794079Z I0109 04:47:49.429756       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal static pod not found and needs new revision 6
2023-01-09T04:47:49.429826492Z I0109 04:47:49.429796       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:49.429826492Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:47:49.429826492Z  CurrentRevision: (int32) 0,
2023-01-09T04:47:49.429826492Z  TargetRevision: (int32) 6,
2023-01-09T04:47:49.429826492Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:49.429826492Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:49.429826492Z  LastFailedReason: (string) "",
2023-01-09T04:47:49.429826492Z  LastFailedCount: (int) 0,
2023-01-09T04:47:49.429826492Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:49.429826492Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:49.429826492Z }
2023-01-09T04:47:49.439304886Z I0109 04:47:49.439259       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 0 to 6 because node ip-10-0-199-219.us-east-2.compute.internal static pod not found
2023-01-09T04:47:49.439554304Z I0109 04:47:49.439530       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:49.460415873Z E0109 04:47:49.460325       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:50.626108476Z I0109 04:47:50.626061       1 request.go:601] Waited for 1.165262021s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:51.438351858Z I0109 04:47:51.438283       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:47:52.431397760Z I0109 04:47:52.431344       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:47:52.625547015Z I0109 04:47:52.625503       1 request.go:601] Waited for 1.187359648s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:53.825384113Z I0109 04:47:53.825343       1 request.go:601] Waited for 1.193241963s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:54.030380813Z E0109 04:47:54.030343       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:54.035148126Z E0109 04:47:54.035117       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:54.826157506Z I0109 04:47:54.826119       1 request.go:601] Waited for 1.196775947s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:54.844876734Z I0109 04:47:54.844841       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:47:57.430620444Z E0109 04:47:57.430586       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:57.435269984Z E0109 04:47:57.435239       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:57.629511442Z I0109 04:47:57.629455       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:47:59.630353397Z E0109 04:47:59.630307       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:03.229901323Z E0109 04:48:03.229859       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:03.230108712Z E0109 04:48:03.230087       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:04.829860243Z E0109 04:48:04.829820       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:04.830063226Z E0109 04:48:04.830044       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:07.796328484Z E0109 04:48:07.796291       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:08.430293187Z E0109 04:48:08.430257       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:08.433939227Z E0109 04:48:08.433911       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:09.230393918Z E0109 04:48:09.230351       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:09.514391037Z E0109 04:48:09.514350       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:10.029921154Z E0109 04:48:10.029880       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:12.873402337Z E0109 04:48:12.873351       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:12.896040160Z E0109 04:48:12.896008       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:16.582484608Z E0109 04:48:16.582439       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:16.608198788Z E0109 04:48:16.608159       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:17.653183303Z E0109 04:48:17.653119       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:17.662839375Z E0109 04:48:17.662785       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:18.241365921Z E0109 04:48:18.241320       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:18.258363211Z E0109 04:48:18.258329       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:20.091061086Z E0109 04:48:20.091026       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:20.099170016Z E0109 04:48:20.099130       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:23.688513927Z E0109 04:48:23.688471       1 guard_controller.go:268] Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:23.707805115Z E0109 04:48:23.707768       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:26.145009590Z E0109 04:48:26.144957       1 guard_controller.go:274] Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:27.289661481Z I0109 04:48:27.289622       1 request.go:601] Waited for 1.144300126s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:27.904185109Z I0109 04:48:27.904138       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:48:28.289742202Z I0109 04:48:28.289703       1 request.go:601] Waited for 1.194510009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:29.489963800Z I0109 04:48:29.489922       1 request.go:601] Waited for 1.191253816s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:30.689230529Z I0109 04:48:30.689194       1 request.go:601] Waited for 1.19428619s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:30.905791134Z E0109 04:48:30.905740       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:30.906480729Z I0109 04:48:30.906438       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:30.906926539Z I0109 04:48:30.906896       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:30.914864375Z I0109 04:48:30.914828       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ip-10-0-199-219.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:48:31.494891068Z I0109 04:48:31.494839       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:32.089181004Z I0109 04:48:32.089142       1 request.go:601] Waited for 1.17756104s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:34.098472878Z I0109 04:48:34.098341       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/openshift-kube-scheduler-guard-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:48:34.702623177Z I0109 04:48:34.702556       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:35.289201388Z I0109 04:48:35.289162       1 request.go:601] Waited for 1.190759015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:36.091933257Z E0109 04:48:36.091901       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.289772287Z E0109 04:48:36.289741       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.489322252Z I0109 04:48:36.489285       1 request.go:601] Waited for 1.191892523s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:36.490075737Z E0109 04:48:36.490041       1 guard_controller.go:327] Unable to apply pod openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.090621771Z E0109 04:48:37.090584       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.489913670Z E0109 04:48:37.489875       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.689874950Z I0109 04:48:37.689834       1 request.go:601] Waited for 1.199364645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:37.691677095Z W0109 04:48:37.691643       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.691703975Z E0109 04:48:37.691680       1 base_controller.go:272] GuardController reconciliation failed: [Unable to apply pod openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused, Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:37.692595758Z E0109 04:48:37.692549       1 guard_controller.go:232] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-scheduler/poddisruptionbudgets/openshift-kube-scheduler-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.693670744Z W0109 04:48:37.693640       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.693694572Z E0109 04:48:37.693666       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-kube-scheduler/poddisruptionbudgets/openshift-kube-scheduler-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.090978907Z E0109 04:48:38.090939       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:38.290277872Z I0109 04:48:38.290209       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.290967648Z E0109 04:48:38.290876       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"openshift-kube-scheduler-operator.17388aa898c1c46d", GenerateName:"", Namespace:"openshift-kube-scheduler-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 6 count 0 on node \"ip-10-0-199-219.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-kube-scheduler-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:38.291304500Z E0109 04:48:38.291288       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.690181448Z E0109 04:48:38.690141       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.890839162Z E0109 04:48:38.890801       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.290082130Z I0109 04:48:39.290029       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.290880003Z E0109 04:48:39.290844       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.490932702Z E0109 04:48:39.490881       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.690123890Z E0109 04:48:39.690081       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.290265774Z I0109 04:48:40.290221       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.291155260Z E0109 04:48:40.291124       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.690290897Z E0109 04:48:40.690255       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.289736365Z I0109 04:48:41.289689       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.290531162Z E0109 04:48:41.290517       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.690269971Z E0109 04:48:41.690218       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.091302337Z E0109 04:48:42.091264       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:42.289621419Z I0109 04:48:42.289568       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.290344566Z E0109 04:48:42.290325       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.490726651Z E0109 04:48:42.490685       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.689674031Z E0109 04:48:42.689635       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.289754406Z I0109 04:48:43.289708       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.290617484Z E0109 04:48:43.290597       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.690199122Z E0109 04:48:43.690146       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.890706657Z E0109 04:48:43.890672       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.290510581Z I0109 04:48:44.290461       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.291338832Z E0109 04:48:44.291312       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.489848014Z I0109 04:48:44.489813       1 request.go:601] Waited for 1.00024968s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:44.690468140Z E0109 04:48:44.690432       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.290183646Z I0109 04:48:45.290134       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.291157454Z E0109 04:48:45.291131       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.491545503Z E0109 04:48:45.491506       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.690038516Z E0109 04:48:45.689965       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.806668683Z E0109 04:48:45.806560       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"openshift-kube-scheduler-operator.17388aa898c1c46d", GenerateName:"", Namespace:"openshift-kube-scheduler-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 6 count 0 on node \"ip-10-0-199-219.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-kube-scheduler-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:46.091194377Z E0109 04:48:46.091153       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:46.290343234Z I0109 04:48:46.290297       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.291085025Z E0109 04:48:46.291047       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.889656120Z E0109 04:48:46.889616       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.490588844Z I0109 04:48:47.490538       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-199-219.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.491663945Z E0109 04:48:47.491632       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.090880448Z E0109 04:48:48.090842       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.291704494Z E0109 04:48:48.291667       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.091049496Z E0109 04:48:49.090981       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:49.891015286Z E0109 04:48:49.890961       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:51.291103797Z E0109 04:48:51.291066       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/kubeschedulers/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:54.110410767Z I0109 04:48:54.110364       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:54.711375071Z E0109 04:48:54.711333       1 base_controller.go:272] KubeControllerManagerStaticResources reconciliation failed: ["assets/kube-scheduler/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/leader-election-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/kube-system/rolebindings/system:openshift:leader-locking-kube-scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/scheduler-clusterrolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler:public-2": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-role.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/roles/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/policyconfigmap-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/namespaces/openshift-kube-scheduler/rolebindings/system:openshift:sa-listing-configmaps": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/openshift-kube-scheduler-sa": dial tcp 172.30.0.1:443: connect: connection refused, "assets/kube-scheduler/localhost-recovery-client-crb.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:kube-scheduler-recovery": dial tcp 172.30.0.1:443: connect: connection refused, Operation cannot be fulfilled on kubeschedulers.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again]
2023-01-09T04:48:56.490138400Z I0109 04:48:56.490093       1 request.go:601] Waited for 1.195258371s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:57.689312042Z I0109 04:48:57.689265       1 request.go:601] Waited for 1.133776244s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/nodes?resourceVersion=23437
2023-01-09T04:48:58.360647760Z I0109 04:48:58.360601       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:58.361357116Z I0109 04:48:58.361291       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"GuardController_SyncError::StaticPods_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:58.411725957Z I0109 04:48:58.411656       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused"
2023-01-09T04:48:58.689361668Z I0109 04:48:58.689305       1 request.go:601] Waited for 1.977136794s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:59.542361088Z I0109 04:48:59.542322       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:59.542830920Z I0109 04:48:59.542798       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:43:05Z","message":"GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:59.560099442Z I0109 04:48:59.560041       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused" to "GuardControllerDegraded: Missing PodIP in operand openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal"
2023-01-09T04:48:59.689582043Z I0109 04:48:59.689535       1 request.go:601] Waited for 1.528048665s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:59.709916695Z I0109 04:48:59.709847       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:49:00.890934120Z I0109 04:49:00.888916       1 request.go:601] Waited for 2.192839962s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:49:01.889170443Z I0109 04:49:01.889130       1 request.go:601] Waited for 2.178234431s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:02.825150162Z E0109 04:49:02.825046       1 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"openshift-kube-scheduler-operator.17388aa898c1c46d", GenerateName:"", Namespace:"openshift-kube-scheduler-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 6 count 0 on node \"ip-10-0-199-219.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-kube-scheduler-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 290113645, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'etcdserver: request timed out' (will not retry!)
2023-01-09T04:49:02.889819608Z I0109 04:49:02.889783       1 request.go:601] Waited for 1.968439784s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps?resourceVersion=23264
2023-01-09T04:49:04.089276249Z I0109 04:49:04.089236       1 request.go:601] Waited for 1.793762426s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:49:04.892450860Z I0109 04:49:04.892408       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:49:05.301281790Z I0109 04:49:05.301228       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:05.301339219Z I0109 04:49:05.301316       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:49:05Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:05.310612315Z I0109 04:49:05.310534       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready")
2023-01-09T04:49:06.489525382Z I0109 04:49:06.489488       1 request.go:601] Waited for 1.186129441s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:08.291853461Z I0109 04:49:08.291816       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:49:16.547922675Z I0109 04:49:16.547881       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:16.547922675Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:49:16.547922675Z  CurrentRevision: (int32) 6,
2023-01-09T04:49:16.547922675Z  TargetRevision: (int32) 0,
2023-01-09T04:49:16.547922675Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:16.547922675Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:16.547922675Z  LastFailedReason: (string) "",
2023-01-09T04:49:16.547922675Z  LastFailedCount: (int) 0,
2023-01-09T04:49:16.547922675Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:16.547922675Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:16.547922675Z }
2023-01-09T04:49:16.547922675Z  because static pod is ready
2023-01-09T04:49:16.558441773Z I0109 04:49:16.558397       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 0 to 6 because static pod is ready
2023-01-09T04:49:16.563320392Z I0109 04:49:16.563286       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:49:05Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:05Z","message":"NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:16.563405644Z I0109 04:49:16.563385       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:16.575084319Z I0109 04:49:16.573496       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 5; 2 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 5; 1 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6"
2023-01-09T04:49:17.662315298Z I0109 04:49:17.662267       1 request.go:601] Waited for 1.098919784s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2023-01-09T04:49:21.666019336Z I0109 04:49:21.665965       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 5 is the oldest and needs new revision 6
2023-01-09T04:49:21.666049124Z I0109 04:49:21.666035       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:21.666049124Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:49:21.666049124Z  CurrentRevision: (int32) 5,
2023-01-09T04:49:21.666049124Z  TargetRevision: (int32) 6,
2023-01-09T04:49:21.666049124Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:21.666049124Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:21.666049124Z  LastFailedReason: (string) "",
2023-01-09T04:49:21.666049124Z  LastFailedCount: (int) 0,
2023-01-09T04:49:21.666049124Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:21.666049124Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:21.666049124Z }
2023-01-09T04:49:21.682319523Z I0109 04:49:21.682260       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 5 to 6 because node ip-10-0-160-211.us-east-2.compute.internal with revision 5 is the oldest
2023-01-09T04:49:21.682500893Z I0109 04:49:21.682476       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:22.863081246Z I0109 04:49:22.863041       1 request.go:601] Waited for 1.179648217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:49:23.872220941Z I0109 04:49:23.872137       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:49:24.865658806Z I0109 04:49:24.865617       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:49:25.063130083Z I0109 04:49:25.063086       1 request.go:601] Waited for 1.190979134s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:26.262240365Z I0109 04:49:26.262201       1 request.go:601] Waited for 1.395556275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/installer-6-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:49:27.262961259Z I0109 04:49:27.262912       1 request.go:601] Waited for 1.19768673s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:27.465067136Z I0109 04:49:27.465021       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:28.462317819Z I0109 04:49:28.462270       1 request.go:601] Waited for 1.194047859s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:29.664794226Z I0109 04:49:29.664736       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:57.974612566Z I0109 04:49:57.974573       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:50:00.776090999Z I0109 04:50:00.776046       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:50:09.345624297Z I0109 04:50:09.345154       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 5
2023-01-09T04:50:11.535745184Z I0109 04:50:11.535707       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:50:13.935288071Z I0109 04:50:13.935249       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:50:59.440349995Z I0109 04:50:59.440303       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:50:59.440349995Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:50:59.440349995Z  CurrentRevision: (int32) 6,
2023-01-09T04:50:59.440349995Z  TargetRevision: (int32) 0,
2023-01-09T04:50:59.440349995Z  LastFailedRevision: (int32) 0,
2023-01-09T04:50:59.440349995Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:50:59.440349995Z  LastFailedReason: (string) "",
2023-01-09T04:50:59.440349995Z  LastFailedCount: (int) 0,
2023-01-09T04:50:59.440349995Z  LastFallbackCount: (int) 0,
2023-01-09T04:50:59.440349995Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:50:59.440349995Z }
2023-01-09T04:50:59.440349995Z  because static pod is ready
2023-01-09T04:50:59.459306266Z I0109 04:50:59.459244       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 5 to 6 because static pod is ready
2023-01-09T04:50:59.462803337Z I0109 04:50:59.462762       1 status_controller.go:211] clusteroperator/kube-scheduler diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:49:05Z","message":"NodeControllerDegraded: All master nodes are ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:50:59Z","message":"NodeInstallerProgressing: 3 nodes are at revision 6","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:44:53Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:50:59.479679299Z I0109 04:50:59.479612       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/kube-scheduler changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 6"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 5; 2 nodes are at revision 6" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 6"
2023-01-09T04:51:00.614846221Z I0109 04:51:00.614791       1 request.go:601] Waited for 1.145947611s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/config
2023-01-09T04:51:01.614850597Z I0109 04:51:01.614813       1 request.go:601] Waited for 1.197500971s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:02.023926182Z I0109 04:51:02.023847       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-160-211.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:51:02.814923246Z I0109 04:51:02.814880       1 request.go:601] Waited for 1.195573598s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:51:04.014863654Z I0109 04:51:04.014824       1 request.go:601] Waited for 1.395261877s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:51:04.620040947Z I0109 04:51:04.619969       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-145-4.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:51:05.214596602Z I0109 04:51:05.214546       1 request.go:601] Waited for 1.39690757s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:06.214802867Z I0109 04:51:06.214762       1 request.go:601] Waited for 1.396235167s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:51:07.215069119Z I0109 04:51:07.215028       1 request.go:601] Waited for 1.387196357s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2023-01-09T04:51:07.422542778Z I0109 04:51:07.422472       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-kube-scheduler-operator", Name:"openshift-kube-scheduler-operator", UID:"d36da3ce-b8ca-4bc5-8827-829259dcf066", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-6-ip-10-0-199-219.us-east-2.compute.internal -n openshift-kube-scheduler because it was missing
2023-01-09T04:51:08.414347025Z I0109 04:51:08.414289       1 request.go:601] Waited for 1.396555332s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/kube-scheduler-pod
2023-01-09T04:51:09.414798715Z I0109 04:51:09.414755       1 request.go:601] Waited for 1.395558129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:10.615077730Z I0109 04:51:10.615033       1 request.go:601] Waited for 1.197745113s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:51:11.815029319Z I0109 04:51:11.814966       1 request.go:601] Waited for 1.196844568s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:12.815079450Z I0109 04:51:12.815040       1 request.go:601] Waited for 1.045917644s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T04:52:48.857773105Z E0109 04:52:48.857725       1 leaderelection.go:330] error retrieving resource lock openshift-kube-scheduler-operator/openshift-cluster-kube-scheduler-operator-lock: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler-operator/configmaps/openshift-cluster-kube-scheduler-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:59.064677277Z I0109 04:52:59.064643       1 request.go:601] Waited for 1.073161618s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services/scheduler
2023-01-09T04:53:00.064873968Z I0109 04:53:00.064826       1 request.go:601] Waited for 1.152665614s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/services?resourceVersion=26427
2023-01-09T04:53:01.065194800Z I0109 04:53:01.065138       1 request.go:601] Waited for 1.394812993s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:02.264795274Z I0109 04:53:02.264752       1 request.go:601] Waited for 1.193548181s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:03.464818676Z I0109 04:53:03.464771       1 request.go:601] Waited for 1.195611165s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:05.664928883Z I0109 04:53:05.664888       1 request.go:601] Waited for 1.176635081s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/installer-sa
2023-01-09T04:53:06.664962662Z I0109 04:53:06.664906       1 request.go:601] Waited for 1.386701384s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/serviceaccounts/localhost-recovery-client
2023-01-09T04:53:07.864359086Z I0109 04:53:07.864319       1 request.go:601] Waited for 1.19285764s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler
2023-01-09T04:53:08.865034248Z I0109 04:53:08.864958       1 request.go:601] Waited for 1.179258002s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/configmaps/scheduler-kubeconfig
2023-01-09T05:03:00.895188918Z I0109 05:03:00.895116       1 request.go:601] Waited for 1.026655683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:03:01.895411458Z I0109 05:03:01.895370       1 request.go:601] Waited for 1.195376444s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:13:00.895669050Z I0109 05:13:00.895426       1 request.go:601] Waited for 1.02594598s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:13:02.095246417Z I0109 05:13:02.095204       1 request.go:601] Waited for 1.193892383s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:23:00.896435977Z I0109 05:23:00.896394       1 request.go:601] Waited for 1.026016627s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:23:02.096180412Z I0109 05:23:02.096124       1 request.go:601] Waited for 1.196588848s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:33:00.895967182Z I0109 05:33:00.895920       1 request.go:601] Waited for 1.025032067s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:33:01.896100511Z I0109 05:33:01.896061       1 request.go:601] Waited for 1.193320359s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:43:00.897240375Z I0109 05:43:00.897198       1 request.go:601] Waited for 1.026111949s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:43:01.897306242Z I0109 05:43:01.897264       1 request.go:601] Waited for 1.197068564s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:53:00.897485235Z I0109 05:53:00.897440       1 request.go:601] Waited for 1.026162479s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:53:01.897745470Z I0109 05:53:01.897696       1 request.go:601] Waited for 1.191376064s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:03:00.897965524Z I0109 06:03:00.897912       1 request.go:601] Waited for 1.026421768s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:03:02.098080670Z I0109 06:03:02.098033       1 request.go:601] Waited for 1.196659055s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:13:00.928603574Z I0109 06:13:00.928557       1 request.go:601] Waited for 1.056749943s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:13:01.929043267Z I0109 06:13:01.928986       1 request.go:601] Waited for 1.195544914s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:23:01.041655000Z I0109 06:23:01.041612       1 request.go:601] Waited for 1.169141279s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:23:02.241097125Z I0109 06:23:02.241052       1 request.go:601] Waited for 1.196515567s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:33:00.899575839Z I0109 06:33:00.899534       1 request.go:601] Waited for 1.026861241s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:33:02.099939053Z I0109 06:33:02.099896       1 request.go:601] Waited for 1.197616338s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:43:00.900635527Z I0109 06:43:00.900593       1 request.go:601] Waited for 1.027801135s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:43:01.901084981Z I0109 06:43:01.901043       1 request.go:601] Waited for 1.197692525s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:53:00.907398640Z I0109 06:53:00.907350       1 request.go:601] Waited for 1.031216056s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:53:02.106889643Z I0109 06:53:02.106848       1 request.go:601] Waited for 1.196751718s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:03:00.939098121Z I0109 07:03:00.939055       1 request.go:601] Waited for 1.065695641s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:03:02.139294881Z I0109 07:03:02.139252       1 request.go:601] Waited for 1.197066501s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:13:00.953499101Z I0109 07:13:00.953455       1 request.go:601] Waited for 1.079502007s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:13:01.953624014Z I0109 07:13:01.953581       1 request.go:601] Waited for 1.197168224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:23:00.903551027Z I0109 07:23:00.903508       1 request.go:601] Waited for 1.02905509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:23:01.903807305Z I0109 07:23:01.903762       1 request.go:601] Waited for 1.198217757s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:33:00.905013931Z I0109 07:33:00.904948       1 request.go:601] Waited for 1.029935085s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:33:02.104691867Z I0109 07:33:02.104652       1 request.go:601] Waited for 1.195056106s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:43:00.905681733Z I0109 07:43:00.905634       1 request.go:601] Waited for 1.029808748s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:43:01.905801562Z I0109 07:43:01.905752       1 request.go:601] Waited for 1.195259733s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:53:00.906585240Z I0109 07:53:00.906539       1 request.go:601] Waited for 1.030155194s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:53:02.105969767Z I0109 07:53:02.105923       1 request.go:601] Waited for 1.196859521s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:03:00.910067650Z I0109 08:03:00.910026       1 request.go:601] Waited for 1.033368791s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:03:01.910661628Z I0109 08:03:01.910619       1 request.go:601] Waited for 1.196787607s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T08:03:02.910713626Z I0109 08:03:02.910675       1 request.go:601] Waited for 1.198205256s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:13:00.907758921Z I0109 08:13:00.907708       1 request.go:601] Waited for 1.030189201s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:13:01.908109354Z I0109 08:13:01.908058       1 request.go:601] Waited for 1.198355664s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:23:00.908664405Z I0109 08:23:00.908613       1 request.go:601] Waited for 1.025847662s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:23:02.108672030Z I0109 08:23:02.108631       1 request.go:601] Waited for 1.197662146s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:33:00.909776536Z I0109 08:33:00.909729       1 request.go:601] Waited for 1.028005126s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:33:02.109331828Z I0109 08:33:02.109279       1 request.go:601] Waited for 1.194897043s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:43:00.909725407Z I0109 08:43:00.909676       1 request.go:601] Waited for 1.027158957s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:43:01.910013204Z I0109 08:43:01.909954       1 request.go:601] Waited for 1.197552465s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:53:00.910464670Z I0109 08:53:00.910417       1 request.go:601] Waited for 1.02745481s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:53:02.110020373Z I0109 08:53:02.109944       1 request.go:601] Waited for 1.196290584s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:03:00.911446473Z I0109 09:03:00.911403       1 request.go:601] Waited for 1.027635197s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:03:02.111255862Z I0109 09:03:02.111212       1 request.go:601] Waited for 1.196620392s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:13:01.020811811Z I0109 09:13:01.020760       1 request.go:601] Waited for 1.136552769s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:13:02.220564388Z I0109 09:13:02.220522       1 request.go:601] Waited for 1.197252594s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:23:00.889049723Z I0109 09:23:00.889003       1 request.go:601] Waited for 1.004707073s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:23:02.089025535Z I0109 09:23:02.088961       1 request.go:601] Waited for 1.197210185s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:33:00.923454259Z I0109 09:33:00.923416       1 request.go:601] Waited for 1.03874975s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:33:01.924318667Z I0109 09:33:01.924271       1 request.go:601] Waited for 1.197565518s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:43:00.967490737Z I0109 09:43:00.967452       1 request.go:601] Waited for 1.08200428s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:43:02.166907319Z I0109 09:43:02.166861       1 request.go:601] Waited for 1.197039256s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:53:01.012098710Z I0109 09:53:01.012059       1 request.go:601] Waited for 1.12593965s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:53:02.012590522Z I0109 09:53:02.012547       1 request.go:601] Waited for 1.196236632s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods?labelSelector=app%3Dinstaller
2023-01-09T10:03:00.896561423Z I0109 10:03:00.896509       1 request.go:601] Waited for 1.009556574s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:03:01.897098722Z I0109 10:03:01.897056       1 request.go:601] Waited for 1.196064973s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:13:00.960495883Z I0109 10:13:00.960445       1 request.go:601] Waited for 1.072483727s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:13:01.961184560Z I0109 10:13:01.961139       1 request.go:601] Waited for 1.197498148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:23:00.917170101Z I0109 10:23:00.917129       1 request.go:601] Waited for 1.028451828s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:23:02.116515937Z I0109 10:23:02.116469       1 request.go:601] Waited for 1.196225622s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:33:00.917493359Z I0109 10:33:00.917448       1 request.go:601] Waited for 1.028400273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:33:02.117338798Z I0109 10:33:02.117294       1 request.go:601] Waited for 1.195442301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:43:00.917929659Z I0109 10:43:00.917884       1 request.go:601] Waited for 1.027709727s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:43:02.118286635Z I0109 10:43:02.118246       1 request.go:601] Waited for 1.197921275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:53:00.918861925Z I0109 10:53:00.918816       1 request.go:601] Waited for 1.02767519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:53:02.118547935Z I0109 10:53:02.118508       1 request.go:601] Waited for 1.197125214s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:03:00.919613630Z I0109 11:03:00.919560       1 request.go:601] Waited for 1.028275579s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:03:02.119529919Z I0109 11:03:02.119487       1 request.go:601] Waited for 1.195602799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:13:00.920213058Z I0109 11:13:00.920146       1 request.go:601] Waited for 1.028471255s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:13:01.920593299Z I0109 11:13:01.920523       1 request.go:601] Waited for 1.192430867s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:23:00.921177613Z I0109 11:23:00.921131       1 request.go:601] Waited for 1.028816388s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:23:02.120201253Z I0109 11:23:02.120157       1 request.go:601] Waited for 1.196615359s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:33:00.920731195Z I0109 11:33:00.920689       1 request.go:601] Waited for 1.02780016s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:33:01.921695373Z I0109 11:33:01.921647       1 request.go:601] Waited for 1.195969087s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:43:00.922774266Z I0109 11:43:00.922735       1 request.go:601] Waited for 1.029390478s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:43:02.122589501Z I0109 11:43:02.122544       1 request.go:601] Waited for 1.195387646s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:53:00.922314551Z I0109 11:53:00.922268       1 request.go:601] Waited for 1.028659497s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:53:01.922544424Z I0109 11:53:01.922498       1 request.go:601] Waited for 1.19808444s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:03:00.923239430Z I0109 12:03:00.923199       1 request.go:601] Waited for 1.02948804s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:03:02.123329421Z I0109 12:03:02.123283       1 request.go:601] Waited for 1.194444341s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:13:00.923820414Z I0109 12:13:00.923779       1 request.go:601] Waited for 1.029251982s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:13:02.123948216Z I0109 12:13:02.123904       1 request.go:601] Waited for 1.195551119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:23:00.952960404Z I0109 12:23:00.952913       1 request.go:601] Waited for 1.057834294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:23:02.152347068Z I0109 12:23:02.152292       1 request.go:601] Waited for 1.196532799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:33:00.963935051Z I0109 12:33:00.963888       1 request.go:601] Waited for 1.068570527s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:33:01.964019440Z I0109 12:33:01.963940       1 request.go:601] Waited for 1.19819383s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:43:00.986084130Z I0109 12:43:00.986043       1 request.go:601] Waited for 1.090414827s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:43:01.986308881Z I0109 12:43:01.986266       1 request.go:601] Waited for 1.195451011s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:53:01.003106929Z I0109 12:53:01.003064       1 request.go:601] Waited for 1.106803784s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:53:02.203327654Z I0109 12:53:02.203283       1 request.go:601] Waited for 1.197074883s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:03:00.927367323Z I0109 13:03:00.927317       1 request.go:601] Waited for 1.030579799s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:03:02.127667964Z I0109 13:03:02.127624       1 request.go:601] Waited for 1.197949273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:13:00.928203997Z I0109 13:13:00.928151       1 request.go:601] Waited for 1.030780311s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:13:02.128483105Z I0109 13:13:02.128446       1 request.go:601] Waited for 1.197250483s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:23:00.933528706Z I0109 13:23:00.933481       1 request.go:601] Waited for 1.035630131s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:23:02.129712002Z I0109 13:23:02.129670       1 request.go:601] Waited for 1.193438283s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:23:06.532180701Z I0109 13:23:06.532133       1 request.go:601] Waited for 1.000177414s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/secrets/localhost-recovery-client-token
2023-01-09T13:33:00.929824854Z I0109 13:33:00.929778       1 request.go:601] Waited for 1.031762054s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:33:01.929971141Z I0109 13:33:01.929917       1 request.go:601] Waited for 1.195362676s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:43:00.931563116Z I0109 13:43:00.931524       1 request.go:601] Waited for 1.0334497s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:43:02.131524201Z I0109 13:43:02.131462       1 request.go:601] Waited for 1.197699719s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:53:00.931606453Z I0109 13:53:00.931565       1 request.go:601] Waited for 1.033376521s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:53:02.131603409Z I0109 13:53:02.131559       1 request.go:601] Waited for 1.196898182s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:03:00.932608137Z I0109 14:03:00.932553       1 request.go:601] Waited for 1.034077694s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:03:01.933716161Z I0109 14:03:01.933661       1 request.go:601] Waited for 1.198401946s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:13:00.932516492Z I0109 14:13:00.932474       1 request.go:601] Waited for 1.03330832s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:13:01.933056436Z I0109 14:13:01.933019       1 request.go:601] Waited for 1.193525338s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:23:00.961843760Z I0109 14:23:00.961795       1 request.go:601] Waited for 1.062074077s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:23:01.962644284Z I0109 14:23:01.962561       1 request.go:601] Waited for 1.197813151s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:33:00.933197099Z I0109 14:33:00.933153       1 request.go:601] Waited for 1.033363853s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:33:02.133542090Z I0109 14:33:02.133498       1 request.go:601] Waited for 1.195854913s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:43:00.934176596Z I0109 14:43:00.934133       1 request.go:601] Waited for 1.033264626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:43:02.133763568Z I0109 14:43:02.133719       1 request.go:601] Waited for 1.197039727s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:53:00.934692654Z I0109 14:53:00.934627       1 request.go:601] Waited for 1.033238994s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:53:01.934869130Z I0109 14:53:01.934829       1 request.go:601] Waited for 1.19702112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:03:00.936155641Z I0109 15:03:00.936116       1 request.go:601] Waited for 1.034243106s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:03:02.135392343Z I0109 15:03:02.135346       1 request.go:601] Waited for 1.196970917s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:13:01.075336304Z I0109 15:13:01.075285       1 request.go:601] Waited for 1.173290509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:13:02.275231146Z I0109 15:13:02.275188       1 request.go:601] Waited for 1.196735837s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:23:00.956162387Z I0109 15:23:00.956121       1 request.go:601] Waited for 1.05278611s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:23:02.155790257Z I0109 15:23:02.155741       1 request.go:601] Waited for 1.192769311s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:33:00.940314832Z I0109 15:33:00.940276       1 request.go:601] Waited for 1.036814278s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:33:01.940490681Z I0109 15:33:01.940452       1 request.go:601] Waited for 1.196465752s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-145-4.us-east-2.compute.internal
