2023-01-09T04:47:25.403849475Z level=info ts=2023-01-09T04:47:25.403758486Z caller=client.go:56 msg="enabling client to server TLS"
2023-01-09T04:47:25.403951735Z level=info ts=2023-01-09T04:47:25.403875636Z caller=options.go:114 msg="TLS client using provided certificate pool"
2023-01-09T04:47:25.403951735Z level=info ts=2023-01-09T04:47:25.403885446Z caller=options.go:147 msg="TLS client authentication enabled"
2023-01-09T04:47:25.407134696Z level=info ts=2023-01-09T04:47:25.407093888Z caller=options.go:26 protocol=gRPC msg="disabled TLS, key and cert must be set to enable"
2023-01-09T04:47:25.407540320Z level=info ts=2023-01-09T04:47:25.407510047Z caller=query.go:724 msg="starting query node"
2023-01-09T04:47:25.407663056Z level=info ts=2023-01-09T04:47:25.407602524Z caller=intrumentation.go:75 msg="changing probe status" status=healthy
2023-01-09T04:47:25.407681751Z level=info ts=2023-01-09T04:47:25.407663773Z caller=http.go:73 service=http/server component=query msg="listening for requests and metrics" address=127.0.0.1:9090
2023-01-09T04:47:25.407836177Z level=info ts=2023-01-09T04:47:25.407763185Z caller=intrumentation.go:56 msg="changing probe status" status=ready
2023-01-09T04:47:25.407925444Z level=info ts=2023-01-09T04:47:25.407897878Z caller=tls_config.go:195 service=http/server component=query msg="TLS is disabled." http2=false
2023-01-09T04:47:25.407958719Z level=info ts=2023-01-09T04:47:25.407927041Z caller=grpc.go:131 service=gRPC/server component=query msg="listening for serving gRPC" address=127.0.0.1:10901
2023-01-09T04:47:25.412908987Z level=error ts=2023-01-09T04:47:25.41284614Z caller=resolver.go:99 msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2023-01-09T04:47:25.415312031Z level=error ts=2023-01-09T04:47:25.41527177Z caller=resolver.go:99 msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2023-01-09T04:47:25.416981849Z level=error ts=2023-01-09T04:47:25.416950358Z caller=resolver.go:99 msg="failed to lookup SRV records" host=_grpc._tcp.prometheus-operated.openshift-monitoring.svc.cluster.local err="no such host"
2023-01-09T04:47:41.708200925Z level=warn ts=2023-01-09T04:47:41.708146587Z caller=proxy.go:362 component=proxy request="min_time:1673239361707 max_time:1673239661707 matchers:<name:\"alertstate\" value:\"firing\" > matchers:<name:\"namespace\" value:\"openshift-kube-controller-manager\" > matchers:<name:\"__name__\" value:\"ALERTS\" > aggregates:COUNT aggregates:SUM " err="No StoreAPIs matched for this query" stores=
2023-01-09T04:48:00.417867548Z level=info ts=2023-01-09T04:48:00.417809557Z caller=endpointset.go:381 component=endpointset msg="adding new sidecar with [storeAPI rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.128.2.13:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-0\"}"
2023-01-09T04:48:30.421847062Z level=info ts=2023-01-09T04:48:30.421791499Z caller=endpointset.go:381 component=endpointset msg="adding new sidecar with [storeAPI rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.131.0.14:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2023-01-09T05:30:40.408923353Z level=warn ts=2023-01-09T05:30:40.40885089Z caller=endpointset.go:416 component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.131.0.14:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.131.0.14:10901
2023-01-09T05:30:45.409349563Z level=warn ts=2023-01-09T05:30:45.409271151Z caller=endpointset.go:416 component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.131.0.14:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.131.0.14:10901
2023-01-09T05:30:50.410577458Z level=warn ts=2023-01-09T05:30:50.410508922Z caller=endpointset.go:416 component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.131.0.14:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.131.0.14:10901
2023-01-09T05:30:55.410916137Z level=warn ts=2023-01-09T05:30:55.410843807Z caller=endpointset.go:416 component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.131.0.14:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.131.0.14:10901
2023-01-09T05:31:00.411283696Z level=warn ts=2023-01-09T05:31:00.411223779Z caller=endpointset.go:416 component=endpointset msg="update of endpoint failed" err="getting metadata: fallback fetching info from 10.131.0.14:10901: rpc error: code = DeadlineExceeded desc = context deadline exceeded" address=10.131.0.14:10901
2023-01-09T05:31:00.418972933Z level=info ts=2023-01-09T05:31:00.418904131Z caller=endpointset.go:381 component=endpointset msg="adding new sidecar with [storeAPI rulesAPI exemplarsAPI targetsAPI MetricMetadataAPI]" address=10.129.2.22:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
2023-01-09T05:31:00.418972933Z level=info ts=2023-01-09T05:31:00.418937582Z caller=endpointset.go:385 component=endpointset msg="removing endpoint because it's unhealthy or does not exist" address=10.131.0.14:10901 extLset="{prometheus=\"openshift-monitoring/k8s\", prometheus_replica=\"prometheus-k8s-1\"}"
