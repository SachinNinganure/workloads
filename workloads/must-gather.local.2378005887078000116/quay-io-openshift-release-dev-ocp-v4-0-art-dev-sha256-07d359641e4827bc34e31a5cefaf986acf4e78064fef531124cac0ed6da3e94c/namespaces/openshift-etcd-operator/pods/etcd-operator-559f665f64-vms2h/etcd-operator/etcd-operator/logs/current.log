2023-01-09T04:42:12.473450646Z I0109 04:42:12.473299       1 profiler.go:21] Starting profiling endpoint at http://127.0.0.1:6060/debug/pprof/
2023-01-09T04:42:12.473735753Z I0109 04:42:12.473712       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.crt"
2023-01-09T04:42:12.473770119Z I0109 04:42:12.473757       1 observer_polling.go:52] Starting from specified content for file "/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:12.473825059Z I0109 04:42:12.473805       1 observer_polling.go:159] Starting file observer
2023-01-09T04:42:12.473928347Z I0109 04:42:12.473915       1 observer_polling.go:135] File observer successfully synced
2023-01-09T04:42:12.474087194Z I0109 04:42:12.474070       1 cmd.go:209] Using service-serving-cert provided certificates
2023-01-09T04:42:12.474143369Z I0109 04:42:12.474128       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.crt"
2023-01-09T04:42:12.474195261Z I0109 04:42:12.474181       1 observer_polling.go:74] Adding reactor for file "/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:12.474228057Z I0109 04:42:12.474216       1 observer_polling.go:52] Starting from specified content for file "/var/run/configmaps/config/config.yaml"
2023-01-09T04:42:12.474494373Z I0109 04:42:12.474477       1 observer_polling.go:159] Starting file observer
2023-01-09T04:42:12.474619034Z I0109 04:42:12.474607       1 observer_polling.go:135] File observer successfully synced
2023-01-09T04:42:12.487648260Z I0109 04:42:12.487616       1 builder.go:262] openshift-cluster-etcd-operator version 4.12.0-202301042354.p0.gf24e5ab.assembly.stream-f24e5ab-f24e5abcc646116cadf72a08b3387bb1b9540a4b
2023-01-09T04:42:12.488047419Z I0109 04:42:12.488028       1 dynamic_serving_content.go:113] "Loaded a new cert/key pair" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:12.903874587Z I0109 04:42:12.903841       1 requestheader_controller.go:244] Loaded a new request header values for RequestHeaderAuthRequestController
2023-01-09T04:42:12.905962989Z I0109 04:42:12.905937       1 maxinflight.go:140] "Initialized nonMutatingChan" len=400
2023-01-09T04:42:12.905977847Z I0109 04:42:12.905952       1 maxinflight.go:146] "Initialized mutatingChan" len=200
2023-01-09T04:42:12.905999150Z I0109 04:42:12.905981       1 timing_ratio_histogram.go:202] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=[executing readOnly]
2023-01-09T04:42:12.906028927Z I0109 04:42:12.906017       1 timing_ratio_histogram.go:202] "TimingRatioHistogramVec.NewForLabelValuesSafe hit the inefficient case" fqName="apiserver_flowcontrol_read_vs_write_current_requests" labelValues=[executing mutating]
2023-01-09T04:42:12.906045850Z I0109 04:42:12.906035       1 maxinflight.go:117] "Set denominator for readonly requests" limit=400
2023-01-09T04:42:12.906045850Z I0109 04:42:12.906043       1 maxinflight.go:121] "Set denominator for mutating requests" limit=200
2023-01-09T04:42:12.906072973Z I0109 04:42:12.906062       1 config.go:731] Not requested to run hook priority-and-fairness-config-consumer
2023-01-09T04:42:12.909177932Z W0109 04:42:12.909159       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:12.909177932Z I0109 04:42:12.909167       1 genericapiserver.go:480] MuxAndDiscoveryComplete has all endpoints registered and discovery information is complete
2023-01-09T04:42:12.909177932Z W0109 04:42:12.909173       1 secure_serving.go:69] Use of insecure cipher 'TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256' detected.
2023-01-09T04:42:12.911690752Z I0109 04:42:12.911662       1 leaderelection.go:248] attempting to acquire leader lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock...
2023-01-09T04:42:12.913070549Z I0109 04:42:12.913038       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
2023-01-09T04:42:12.913070549Z I0109 04:42:12.913059       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
2023-01-09T04:42:12.913096725Z I0109 04:42:12.913062       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
2023-01-09T04:42:12.913096725Z I0109 04:42:12.913083       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:12.913096725Z I0109 04:42:12.913092       1 shared_informer.go:255] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:12.913172661Z I0109 04:42:12.913147       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913230339Z I0109 04:42:12.913212       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913280359Z I0109 04:42:12.913082       1 shared_informer.go:255] Waiting for caches to sync for RequestHeaderAuthRequestController
2023-01-09T04:42:12.913323012Z I0109 04:42:12.913148       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913355449Z I0109 04:42:12.913319       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key"
2023-01-09T04:42:12.913355449Z I0109 04:42:12.913329       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:09 +0000 UTC to 2025-01-08 04:41:10 +0000 UTC (now=2023-01-09 04:42:12.913303993 +0000 UTC))"
2023-01-09T04:42:12.913355449Z I0109 04:42:12.913321       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913416678Z I0109 04:42:12.913148       1 reflector.go:221] Starting reflector *v1.ConfigMap (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913488158Z I0109 04:42:12.913467       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.913488158Z I0109 04:42:12.913474       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239332\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239332\" (2023-01-09 03:42:12 +0000 UTC to 2024-01-09 03:42:12 +0000 UTC (now=2023-01-09 04:42:12.913452325 +0000 UTC))"
2023-01-09T04:42:12.913521060Z I0109 04:42:12.913506       1 secure_serving.go:210] Serving securely on [::]:8443
2023-01-09T04:42:12.913546261Z I0109 04:42:12.913527       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
2023-01-09T04:42:12.913578212Z I0109 04:42:12.913531       1 genericapiserver.go:585] [graceful-termination] waiting for shutdown to be initiated
2023-01-09T04:42:12.924528337Z I0109 04:42:12.924500       1 leaderelection.go:258] successfully acquired lease openshift-etcd-operator/openshift-cluster-etcd-operator-lock
2023-01-09T04:42:12.924595964Z I0109 04:42:12.924570       1 event.go:285] Event(v1.ObjectReference{Kind:"ConfigMap", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"19de7ecb-23ae-4173-90b7-7a80417cb4b7", APIVersion:"v1", ResourceVersion:"9195", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-559f665f64-vms2h_c3ca3d76-3dc7-47d8-9270-bbbdd802733d became leader
2023-01-09T04:42:12.924609447Z I0109 04:42:12.924590       1 event.go:285] Event(v1.ObjectReference{Kind:"Lease", Namespace:"openshift-etcd-operator", Name:"openshift-cluster-etcd-operator-lock", UID:"d242f664-c1d7-4587-b30f-4acfa45dbc32", APIVersion:"coordination.k8s.io/v1", ResourceVersion:"9196", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' etcd-operator-559f665f64-vms2h_c3ca3d76-3dc7-47d8-9270-bbbdd802733d became leader
2023-01-09T04:42:12.929715839Z I0109 04:42:12.929684       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "RevisionController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.930399988Z I0109 04:42:12.930344       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "PruneController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.930604692Z I0109 04:42:12.930579       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "NodeController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.931957155Z I0109 04:42:12.931918       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.932148260Z I0109 04:42:12.932119       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "LoggingSyncer" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.932503218Z I0109 04:42:12.932384       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "GuardController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.935799357Z I0109 04:42:12.935762       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "UnsupportedConfigOverridesController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.936301123Z I0109 04:42:12.936283       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-01-09T04:42:12.936349602Z I0109 04:42:12.936326       1 base_controller.go:67] Waiting for caches to sync for QuorumGuardCleanupController
2023-01-09T04:42:12.936412469Z I0109 04:42:12.936389       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.936457244Z I0109 04:42:12.936442       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.936503464Z I0109 04:42:12.936480       1 base_controller.go:67] Waiting for caches to sync for RevisionController
2023-01-09T04:42:12.936559760Z I0109 04:42:12.936539       1 reflector.go:221] Starting reflector *v1.Infrastructure (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.936596017Z I0109 04:42:12.936575       1 base_controller.go:67] Waiting for caches to sync for EtcdEndpointsController
2023-01-09T04:42:12.936635686Z I0109 04:42:12.936614       1 base_controller.go:67] Waiting for caches to sync for InstallerController
2023-01-09T04:42:12.936720245Z I0109 04:42:12.936663       1 base_controller.go:67] Waiting for caches to sync for NodeController
2023-01-09T04:42:12.936720245Z I0109 04:42:12.936696       1 base_controller.go:67] Waiting for caches to sync for GuardController
2023-01-09T04:42:12.936720245Z I0109 04:42:12.936697       1 reflector.go:205] Reflector from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169 configured with expectedType of *unstructured.Unstructured with empty GroupVersionKind.
2023-01-09T04:42:12.936720245Z I0109 04:42:12.936325       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.936742444Z I0109 04:42:12.936718       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.936898508Z I0109 04:42:12.936876       1 base_controller.go:67] Waiting for caches to sync for EtcdStaticResources
2023-01-09T04:42:12.937125956Z I0109 04:42:12.937099       1 reflector.go:221] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937125956Z I0109 04:42:12.937115       1 reflector.go:257] Listing and watching *v1.Namespace from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937151466Z I0109 04:42:12.937130       1 reflector.go:221] Starting reflector *unstructured.Unstructured (12h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937151466Z I0109 04:42:12.937140       1 reflector.go:257] Listing and watching *unstructured.Unstructured from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937163577Z I0109 04:42:12.936402       1 base_controller.go:67] Waiting for caches to sync for DefragController
2023-01-09T04:42:12.937175756Z I0109 04:42:12.936446       1 base_controller.go:67] Waiting for caches to sync for BootstrapTeardownController
2023-01-09T04:42:12.937185778Z I0109 04:42:12.937175       1 reflector.go:221] Starting reflector *v1.Deployment (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937195858Z I0109 04:42:12.937183       1 reflector.go:257] Listing and watching *v1.Deployment from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937215253Z I0109 04:42:12.936292       1 reflector.go:221] Starting reflector *v1.Node (1h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937215253Z I0109 04:42:12.937210       1 reflector.go:257] Listing and watching *v1.Node from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937337157Z I0109 04:42:12.936301       1 base_controller.go:67] Waiting for caches to sync for RemoveStaleConditionsController
2023-01-09T04:42:12.937337157Z I0109 04:42:12.937329       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937351243Z I0109 04:42:12.937334       1 reflector.go:221] Starting reflector *v1beta1.Machine (1h0m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937351243Z I0109 04:42:12.937343       1 reflector.go:257] Listing and watching *v1beta1.Machine from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937351243Z I0109 04:42:12.937344       1 base_controller.go:67] Waiting for caches to sync for BackingResourceController
2023-01-09T04:42:12.937376484Z I0109 04:42:12.936305       1 base_controller.go:67] Waiting for caches to sync for MissingStaticPodController
2023-01-09T04:42:12.937376484Z I0109 04:42:12.937369       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937387530Z I0109 04:42:12.936485       1 reflector.go:221] Starting reflector *v1.Network (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937398165Z I0109 04:42:12.937386       1 reflector.go:257] Listing and watching *v1.Network from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937408032Z I0109 04:42:12.936289       1 base_controller.go:67] Waiting for caches to sync for ResourceSyncController
2023-01-09T04:42:12.937417980Z I0109 04:42:12.936339       1 base_controller.go:67] Waiting for caches to sync for ConfigObserver
2023-01-09T04:42:12.937438680Z I0109 04:42:12.936346       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937449660Z I0109 04:42:12.937435       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937459615Z I0109 04:42:12.937451       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937469525Z I0109 04:42:12.937105       1 base_controller.go:67] Waiting for caches to sync for UnsupportedConfigOverridesController
2023-01-09T04:42:12.937490166Z I0109 04:42:12.936357       1 reflector.go:221] Starting reflector *v1.Etcd (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937500676Z I0109 04:42:12.937479       1 reflector.go:257] Listing and watching *v1.Etcd from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937500676Z I0109 04:42:12.937483       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937500676Z I0109 04:42:12.937490       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937511980Z I0109 04:42:12.937488       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'FastControllerResync' Controller "InstallerController" resync interval is set to 0s which might lead to client request throttling
2023-01-09T04:42:12.937511980Z I0109 04:42:12.936318       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937528894Z I0109 04:42:12.937515       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937528894Z I0109 04:42:12.936349       1 base_controller.go:67] Waiting for caches to sync for MachineDeletionHooksController
2023-01-09T04:42:12.937542040Z I0109 04:42:12.936412       1 base_controller.go:67] Waiting for caches to sync for ClusterBackupController
2023-01-09T04:42:12.937554774Z I0109 04:42:12.936425       1 envvarcontroller.go:172] Starting EnvVarController
2023-01-09T04:42:12.937566716Z I0109 04:42:12.936425       1 base_controller.go:67] Waiting for caches to sync for EtcdMembersController
2023-01-09T04:42:12.937576774Z I0109 04:42:12.937565       1 shared_informer.go:285] caches populated
2023-01-09T04:42:12.937576774Z I0109 04:42:12.937571       1 base_controller.go:73] Caches are synced for EtcdMembersController 
2023-01-09T04:42:12.937633069Z I0109 04:42:12.936379       1 reflector.go:221] Starting reflector *v1.Secret (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937645974Z I0109 04:42:12.937630       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937675828Z I0109 04:42:12.936459       1 reflector.go:221] Starting reflector *v1.Namespace (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937675828Z I0109 04:42:12.937667       1 reflector.go:221] Starting reflector *v1.ConfigMap (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937675828Z I0109 04:42:12.937671       1 reflector.go:257] Listing and watching *v1.Namespace from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937688312Z I0109 04:42:12.937676       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937688312Z I0109 04:42:12.936617       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberController
2023-01-09T04:42:12.937750415Z I0109 04:42:12.936512       1 reflector.go:221] Starting reflector *v1.APIServer (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937762072Z I0109 04:42:12.937748       1 reflector.go:257] Listing and watching *v1.APIServer from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937830491Z I0109 04:42:12.937771       1 reflector.go:221] Starting reflector *v1.PodDisruptionBudget (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937830491Z I0109 04:42:12.937783       1 reflector.go:257] Listing and watching *v1.PodDisruptionBudget from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937830491Z I0109 04:42:12.936585       1 reflector.go:257] Listing and watching *v1.Infrastructure from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937879976Z I0109 04:42:12.937748       1 reflector.go:221] Starting reflector *v1.ServiceAccount (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937879976Z I0109 04:42:12.937847       1 reflector.go:257] Listing and watching *v1.ServiceAccount from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.937904950Z I0109 04:42:12.936574       1 base_controller.go:67] Waiting for caches to sync for StatusSyncer_etcd
2023-01-09T04:42:12.937923175Z I0109 04:42:12.936630       1 base_controller.go:67] Waiting for caches to sync for EtcdCertSignerController
2023-01-09T04:42:12.937933810Z I0109 04:42:12.936634       1 base_controller.go:67] Waiting for caches to sync for LoggingSyncer
2023-01-09T04:42:12.938016724Z I0109 04:42:12.936642       1 base_controller.go:67] Waiting for caches to sync for InstallerStateController
2023-01-09T04:42:12.938016724Z I0109 04:42:12.936649       1 base_controller.go:67] Waiting for caches to sync for StaticPodStateController
2023-01-09T04:42:12.938016724Z I0109 04:42:12.936649       1 reflector.go:221] Starting reflector *v1.ClusterVersion (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938016724Z I0109 04:42:12.937972       1 reflector.go:257] Listing and watching *v1.ClusterVersion from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938016724Z I0109 04:42:12.937977       1 reflector.go:221] Starting reflector *v1.Service (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938016724Z I0109 04:42:12.936655       1 base_controller.go:67] Waiting for caches to sync for PruneController
2023-01-09T04:42:12.938016724Z I0109 04:42:12.938008       1 reflector.go:257] Listing and watching *v1.Service from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938084115Z I0109 04:42:12.937149       1 reflector.go:221] Starting reflector *v1.Pod (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938084115Z I0109 04:42:12.938052       1 reflector.go:257] Listing and watching *v1.Pod from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938084115Z I0109 04:42:12.937344       1 reflector.go:257] Listing and watching *v1.ConfigMap from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938084115Z I0109 04:42:12.936311       1 base_controller.go:67] Waiting for caches to sync for FSyncController
2023-01-09T04:42:12.938097796Z I0109 04:42:12.938088       1 shared_informer.go:285] caches populated
2023-01-09T04:42:12.938097796Z I0109 04:42:12.936315       1 base_controller.go:67] Waiting for caches to sync for ScriptController
2023-01-09T04:42:12.938108046Z I0109 04:42:12.938095       1 base_controller.go:73] Caches are synced for FSyncController 
2023-01-09T04:42:12.938108046Z I0109 04:42:12.938101       1 base_controller.go:110] Starting #1 worker of FSyncController controller ...
2023-01-09T04:42:12.938118140Z I0109 04:42:12.937377       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938140871Z I0109 04:42:12.937423       1 reflector.go:221] Starting reflector *v1.Endpoints (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938151470Z I0109 04:42:12.938138       1 reflector.go:257] Listing and watching *v1.Endpoints from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938182668Z I0109 04:42:12.937458       1 reflector.go:257] Listing and watching *v1.Secret from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938224770Z I0109 04:42:12.937614       1 base_controller.go:110] Starting #1 worker of EtcdMembersController controller ...
2023-01-09T04:42:12.938264591Z I0109 04:42:12.936404       1 reflector.go:221] Starting reflector *v1.Node (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938275114Z I0109 04:42:12.938263       1 reflector.go:257] Listing and watching *v1.Node from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938393779Z I0109 04:42:12.936430       1 reflector.go:221] Starting reflector *v1.ClusterRoleBinding (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938411412Z I0109 04:42:12.938391       1 reflector.go:257] Listing and watching *v1.ClusterRoleBinding from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938543686Z I0109 04:42:12.936604       1 base_controller.go:67] Waiting for caches to sync for TargetConfigController
2023-01-09T04:42:12.938565144Z I0109 04:42:12.936611       1 reflector.go:221] Starting reflector *v1.ClusterOperator (10m0s) from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938575669Z I0109 04:42:12.938564       1 reflector.go:257] Listing and watching *v1.ClusterOperator from k8s.io/client-go@v0.25.0/tools/cache/reflector.go:169
2023-01-09T04:42:12.938575669Z I0109 04:42:12.938559       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ReportEtcdMembersErrorUpdatingStatus' etcds.operator.openshift.io "cluster" not found
2023-01-09T04:42:12.938671113Z E0109 04:42:12.938378       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:12.938909721Z I0109 04:42:12.938868       1 base_controller.go:67] Waiting for caches to sync for ClusterMemberRemovalController
2023-01-09T04:42:12.952959310Z E0109 04:42:12.952917       1 base_controller.go:270] "FSyncController" controller failed to sync "key", err: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:12.955411555Z E0109 04:42:12.955382       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:12.966640971Z E0109 04:42:12.966609       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:12.972533490Z E0109 04:42:12.972506       1 base_controller.go:270] "FSyncController" controller failed to sync "key", err: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:12.987670848Z E0109 04:42:12.987642       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:12.988445410Z E0109 04:42:12.988423       1 base_controller.go:270] "FSyncController" controller failed to sync "key", err: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.013267378Z I0109 04:42:13.013238       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.013267378Z I0109 04:42:13.013258       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
2023-01-09T04:42:13.013291840Z I0109 04:42:13.013241       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.013291840Z I0109 04:42:13.013289       1 shared_informer.go:262] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
2023-01-09T04:42:13.013373559Z I0109 04:42:13.013358       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.013373559Z I0109 04:42:13.013369       1 shared_informer.go:262] Caches are synced for RequestHeaderAuthRequestController
2023-01-09T04:42:13.013520814Z I0109 04:42:13.013504       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:42:13.013472802 +0000 UTC))"
2023-01-09T04:42:13.013655109Z I0109 04:42:13.013640       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:09 +0000 UTC to 2025-01-08 04:41:10 +0000 UTC (now=2023-01-09 04:42:13.013625158 +0000 UTC))"
2023-01-09T04:42:13.013749272Z I0109 04:42:13.013737       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239332\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239332\" (2023-01-09 03:42:12 +0000 UTC to 2024-01-09 03:42:12 +0000 UTC (now=2023-01-09 04:42:13.013725791 +0000 UTC))"
2023-01-09T04:42:13.013852248Z I0109 04:42:13.013841       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:21 +0000 UTC to 2033-01-06 04:28:21 +0000 UTC (now=2023-01-09 04:42:13.013829544 +0000 UTC))"
2023-01-09T04:42:13.013879381Z I0109 04:42:13.013866       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:42:13.013852336 +0000 UTC))"
2023-01-09T04:42:13.013900112Z I0109 04:42:13.013888       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:42:13.01387801 +0000 UTC))"
2023-01-09T04:42:13.013919791Z I0109 04:42:13.013909       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:42:13.013898804 +0000 UTC))"
2023-01-09T04:42:13.013938731Z I0109 04:42:13.013928       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2033-01-06 04:28:22 +0000 UTC (now=2023-01-09 04:42:13.013918318 +0000 UTC))"
2023-01-09T04:42:13.013960540Z I0109 04:42:13.013947       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:42:13.013937783 +0000 UTC))"
2023-01-09T04:42:13.014133601Z I0109 04:42:13.014116       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:09 +0000 UTC to 2025-01-08 04:41:10 +0000 UTC (now=2023-01-09 04:42:13.014093681 +0000 UTC))"
2023-01-09T04:42:13.014267649Z I0109 04:42:13.014245       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239332\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239332\" (2023-01-09 03:42:12 +0000 UTC to 2024-01-09 03:42:12 +0000 UTC (now=2023-01-09 04:42:13.01422621 +0000 UTC))"
2023-01-09T04:42:13.015233472Z E0109 04:42:13.015198       1 base_controller.go:270] "FSyncController" controller failed to sync "key", err: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.028555533Z E0109 04:42:13.028527       1 base_controller.go:270] "EtcdMembersController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.037471762Z I0109 04:42:13.037447       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.037646679Z I0109 04:42:13.037629       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-01-09T04:42:13.037675929Z I0109 04:42:13.037644       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-01-09T04:42:13.037717516Z I0109 04:42:13.037529       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.037820415Z I0109 04:42:13.037808       1 base_controller.go:73] Caches are synced for RemoveStaleConditionsController 
2023-01-09T04:42:13.037850805Z I0109 04:42:13.037840       1 base_controller.go:110] Starting #1 worker of RemoveStaleConditionsController controller ...
2023-01-09T04:42:13.037890336Z I0109 04:42:13.037533       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.037918590Z I0109 04:42:13.037908       1 base_controller.go:73] Caches are synced for DefragController 
2023-01-09T04:42:13.037946187Z I0109 04:42:13.037937       1 base_controller.go:110] Starting #1 worker of DefragController controller ...
2023-01-09T04:42:13.038373424Z I0109 04:42:13.037547       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.039689798Z I0109 04:42:13.039667       1 base_controller.go:73] Caches are synced for MachineDeletionHooksController 
2023-01-09T04:42:13.039734419Z I0109 04:42:13.039723       1 base_controller.go:110] Starting #1 worker of MachineDeletionHooksController controller ...
2023-01-09T04:42:13.039827140Z I0109 04:42:13.037557       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.039827140Z I0109 04:42:13.039808       1 base_controller.go:73] Caches are synced for BootstrapTeardownController 
2023-01-09T04:42:13.039827140Z I0109 04:42:13.039816       1 base_controller.go:110] Starting #1 worker of BootstrapTeardownController controller ...
2023-01-09T04:42:13.040373114Z I0109 04:42:13.037610       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.040373114Z I0109 04:42:13.040355       1 base_controller.go:73] Caches are synced for UnsupportedConfigOverridesController 
2023-01-09T04:42:13.040422457Z I0109 04:42:13.040380       1 base_controller.go:110] Starting #1 worker of UnsupportedConfigOverridesController controller ...
2023-01-09T04:42:13.041507912Z E0109 04:42:13.038617       1 base_controller.go:270] "DefragController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.041785947Z E0109 04:42:13.041759       1 base_controller.go:270] "DefragController" controller failed to sync "key", err: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.041796260Z I0109 04:42:13.038680       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.041803715Z I0109 04:42:13.041793       1 base_controller.go:73] Caches are synced for LoggingSyncer 
2023-01-09T04:42:13.041803715Z I0109 04:42:13.041800       1 base_controller.go:110] Starting #1 worker of LoggingSyncer controller ...
2023-01-09T04:42:13.042209571Z I0109 04:42:13.038715       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.042221516Z I0109 04:42:13.042208       1 base_controller.go:73] Caches are synced for PruneController 
2023-01-09T04:42:13.042221516Z I0109 04:42:13.042215       1 base_controller.go:110] Starting #1 worker of PruneController controller ...
2023-01-09T04:42:13.042374593Z I0109 04:42:13.042350       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorLogLevelChange' Operator log level changed from "Debug" to "Normal"
2023-01-09T04:42:13.042396664Z I0109 04:42:13.038741       1 shared_informer.go:285] caches populated
2023-01-09T04:42:13.042396664Z I0109 04:42:13.042392       1 base_controller.go:73] Caches are synced for StatusSyncer_etcd 
2023-01-09T04:42:13.042405897Z I0109 04:42:13.042399       1 base_controller.go:110] Starting #1 worker of StatusSyncer_etcd controller ...
2023-01-09T04:42:13.042495628Z I0109 04:42:13.042481       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.043114039Z I0109 04:42:13.043093       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.047668892Z E0109 04:42:13.047651       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.050248436Z I0109 04:42:13.050223       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.050370842Z I0109 04:42:13.050328       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: giving up getting a cached client after 3 tries\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:13.051578373Z E0109 04:42:13.051553       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.051770149Z I0109 04:42:13.051748       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.052477101Z E0109 04:42:13.052446       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.052825152Z E0109 04:42:13.052802       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.053761789Z E0109 04:42:13.053736       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:13.056211549Z I0109 04:42:13.056189       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.057300780Z E0109 04:42:13.057268       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.064830043Z E0109 04:42:13.060587       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.064830043Z I0109 04:42:13.062578       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.064830043Z I0109 04:42:13.062717       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:13.065894464Z E0109 04:42:13.065872       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:13.068116858Z E0109 04:42:13.068094       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.078272826Z E0109 04:42:13.078249       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.110665768Z E0109 04:42:13.110630       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.118975138Z E0109 04:42:13.118945       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.146330107Z E0109 04:42:13.146301       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.148553034Z E0109 04:42:13.148532       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.199964761Z E0109 04:42:13.199932       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.238043959Z I0109 04:42:13.237969       1 base_controller.go:73] Caches are synced for BackingResourceController 
2023-01-09T04:42:13.238123420Z I0109 04:42:13.238092       1 base_controller.go:110] Starting #1 worker of BackingResourceController controller ...
2023-01-09T04:42:13.271638204Z E0109 04:42:13.271601       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.309248440Z E0109 04:42:13.309200       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.319778252Z E0109 04:42:13.319746       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.361247073Z E0109 04:42:13.361210       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.437239792Z I0109 04:42:13.437053       1 base_controller.go:73] Caches are synced for EtcdStaticResources 
2023-01-09T04:42:13.437239792Z I0109 04:42:13.437077       1 base_controller.go:110] Starting #1 worker of EtcdStaticResources controller ...
2023-01-09T04:42:13.592852444Z E0109 04:42:13.592810       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.630017210Z E0109 04:42:13.629952       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.636868317Z I0109 04:42:13.636818       1 base_controller.go:73] Caches are synced for QuorumGuardCleanupController 
2023-01-09T04:42:13.636868317Z I0109 04:42:13.636834       1 base_controller.go:73] Caches are synced for InstallerController 
2023-01-09T04:42:13.636868317Z I0109 04:42:13.636840       1 base_controller.go:110] Starting #1 worker of QuorumGuardCleanupController controller ...
2023-01-09T04:42:13.636868317Z I0109 04:42:13.636847       1 base_controller.go:110] Starting #1 worker of InstallerController controller ...
2023-01-09T04:42:13.637453359Z I0109 04:42:13.637410       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2
2023-01-09T04:42:13.637586250Z I0109 04:42:13.637567       1 base_controller.go:73] Caches are synced for ClusterBackupController 
2023-01-09T04:42:13.637586250Z I0109 04:42:13.637581       1 base_controller.go:110] Starting #1 worker of ClusterBackupController controller ...
2023-01-09T04:42:13.638032942Z I0109 04:42:13.637977       1 base_controller.go:73] Caches are synced for StaticPodStateController 
2023-01-09T04:42:13.638032942Z I0109 04:42:13.638007       1 base_controller.go:110] Starting #1 worker of StaticPodStateController controller ...
2023-01-09T04:42:13.638032942Z I0109 04:42:13.637980       1 base_controller.go:73] Caches are synced for InstallerStateController 
2023-01-09T04:42:13.638032942Z I0109 04:42:13.638025       1 base_controller.go:110] Starting #1 worker of InstallerStateController controller ...
2023-01-09T04:42:13.646708823Z E0109 04:42:13.646677       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.647283697Z I0109 04:42:13.647257       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.647530007Z E0109 04:42:13.647508       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.647546412Z E0109 04:42:13.647534       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.648102343Z E0109 04:42:13.648079       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.648126211Z I0109 04:42:13.648104       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2
2023-01-09T04:42:13.648328636Z I0109 04:42:13.648310       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.648871220Z E0109 04:42:13.648854       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:13.652074249Z I0109 04:42:13.652044       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2
2023-01-09T04:42:13.652205987Z E0109 04:42:13.652191       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.655585082Z I0109 04:42:13.655550       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:13.673741816Z E0109 04:42:13.673710       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.673775433Z I0109 04:42:13.673742       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2
2023-01-09T04:42:13.681673131Z E0109 04:42:13.681646       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.714833136Z I0109 04:42:13.714792       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2
2023-01-09T04:42:13.714914881Z E0109 04:42:13.714892       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.739523311Z I0109 04:42:13.739494       1 base_controller.go:73] Caches are synced for ClusterMemberRemovalController 
2023-01-09T04:42:13.739523311Z I0109 04:42:13.739510       1 base_controller.go:110] Starting #1 worker of ClusterMemberRemovalController controller ...
2023-01-09T04:42:13.795466431Z I0109 04:42:13.795409       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-2
2023-01-09T04:42:13.804768502Z E0109 04:42:13.804731       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.805127416Z I0109 04:42:13.805104       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.805202413Z E0109 04:42:13.805125       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.805326120Z E0109 04:42:13.805296       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.805840917Z E0109 04:42:13.805822       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.805899412Z I0109 04:42:13.805875       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-2
2023-01-09T04:42:13.805962012Z I0109 04:42:13.805936       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.813022347Z I0109 04:42:13.812961       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [configmaps: etcd-metrics-proxy-client-ca,etcd-metrics-proxy-serving-ca,etcd-peer-client-ca,etcd-scripts,etcd-serving-ca,restore-etcd-pod, secrets: etcd-all-certs, configmaps: etcd-endpoints-2,etcd-metrics-proxy-client-ca-2,etcd-metrics-proxy-serving-ca-2,etcd-peer-client-ca-2,etcd-pod-2,etcd-serving-ca-2, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:13.837761431Z I0109 04:42:13.837725       1 base_controller.go:73] Caches are synced for MissingStaticPodController 
2023-01-09T04:42:13.837836201Z I0109 04:42:13.837802       1 base_controller.go:110] Starting #1 worker of MissingStaticPodController controller ...
2023-01-09T04:42:13.837836201Z I0109 04:42:13.837772       1 base_controller.go:73] Caches are synced for ClusterMemberController 
2023-01-09T04:42:13.837863065Z I0109 04:42:13.837832       1 base_controller.go:110] Starting #1 worker of ClusterMemberController controller ...
2023-01-09T04:42:13.850489580Z E0109 04:42:13.850459       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.851044843Z E0109 04:42:13.851017       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.851145259Z I0109 04:42:13.851036       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:13.851309052Z I0109 04:42:13.851283       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RequiredInstallerResourcesMissing' secrets: etcd-all-certs, secrets: etcd-all-certs-2
2023-01-09T04:42:13.851447230Z E0109 04:42:13.851427       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.851517866Z E0109 04:42:13.851500       1 base_controller.go:272] InstallerController reconciliation failed: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]
2023-01-09T04:42:13.851579934Z E0109 04:42:13.851565       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.851861929Z I0109 04:42:13.851819       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:13.856537705Z E0109 04:42:13.856511       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.859012995Z I0109 04:42:13.858947       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:13.877675174Z E0109 04:42:13.877636       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.918192643Z E0109 04:42:13.918160       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:13.998589901Z E0109 04:42:13.998559       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.037815279Z I0109 04:42:14.037777       1 base_controller.go:73] Caches are synced for RevisionController 
2023-01-09T04:42:14.037815279Z I0109 04:42:14.037798       1 base_controller.go:110] Starting #1 worker of RevisionController controller ...
2023-01-09T04:42:14.137493305Z I0109 04:42:14.137462       1 request.go:601] Waited for 1.199218929s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/endpoints?limit=500&resourceVersion=0
2023-01-09T04:42:14.159192455Z E0109 04:42:14.159155       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.213461152Z E0109 04:42:14.213421       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.234096416Z E0109 04:42:14.234059       1 base_controller.go:272] EtcdMembersController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.242278909Z I0109 04:42:14.242244       1 base_controller.go:73] Caches are synced for ScriptController 
2023-01-09T04:42:14.243393710Z I0109 04:42:14.243353       1 base_controller.go:110] Starting #1 worker of ScriptController controller ...
2023-01-09T04:42:14.245576910Z E0109 04:42:14.245542       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.259366335Z E0109 04:42:14.259333       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.259820177Z E0109 04:42:14.259803       1 base_controller.go:272] ClusterMemberController reconciliation failed: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.260352087Z I0109 04:42:14.260320       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.260421886Z E0109 04:42:14.260402       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.260928995Z E0109 04:42:14.260891       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.261226791Z E0109 04:42:14.261210       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.262052932Z I0109 04:42:14.262023       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.265049654Z E0109 04:42:14.265027       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.268207534Z I0109 04:42:14.268171       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:42:14.271116095Z E0109 04:42:14.271076       1 base_controller.go:272] DefragController reconciliation failed: getting cache client could not retrieve endpoints: node lister not synced
2023-01-09T04:42:14.286874702Z E0109 04:42:14.286844       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.296139172Z E0109 04:42:14.296101       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:14.327944187Z E0109 04:42:14.327900       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.409423004Z E0109 04:42:14.409381       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": missing env var values
2023-01-09T04:42:14.437061032Z I0109 04:42:14.437014       1 base_controller.go:73] Caches are synced for NodeController 
2023-01-09T04:42:14.437061032Z I0109 04:42:14.437032       1 base_controller.go:73] Caches are synced for EtcdEndpointsController 
2023-01-09T04:42:14.437061032Z I0109 04:42:14.437042       1 base_controller.go:110] Starting #1 worker of NodeController controller ...
2023-01-09T04:42:14.437061032Z I0109 04:42:14.437045       1 base_controller.go:110] Starting #1 worker of EtcdEndpointsController controller ...
2023-01-09T04:42:14.437097224Z I0109 04:42:14.437017       1 base_controller.go:73] Caches are synced for GuardController 
2023-01-09T04:42:14.437097224Z I0109 04:42:14.437074       1 base_controller.go:110] Starting #1 worker of GuardController controller ...
2023-01-09T04:42:14.437145490Z I0109 04:42:14.437131       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:42:14.437731441Z I0109 04:42:14.437705       1 base_controller.go:73] Caches are synced for ConfigObserver 
2023-01-09T04:42:14.437787788Z I0109 04:42:14.437764       1 base_controller.go:110] Starting #1 worker of ConfigObserver controller ...
2023-01-09T04:42:14.437826268Z I0109 04:42:14.437708       1 envvarcontroller.go:178] caches synced
2023-01-09T04:42:14.438605313Z I0109 04:42:14.438583       1 base_controller.go:73] Caches are synced for TargetConfigController 
2023-01-09T04:42:14.438605313Z I0109 04:42:14.438598       1 base_controller.go:110] Starting #1 worker of TargetConfigController controller ...
2023-01-09T04:42:14.458339024Z E0109 04:42:14.458306       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.459386332Z I0109 04:42:14.459355       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:42:14.459751774Z I0109 04:42:14.459726       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:42:14.460304603Z I0109 04:42:14.460275       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:42:14.460530458Z I0109 04:42:14.460489       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.460632632Z I0109 04:42:14.460311       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.468873062Z I0109 04:42:14.468832       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:14.473625779Z E0109 04:42:14.473590       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.476072741Z E0109 04:42:14.476047       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.483430315Z I0109 04:42:14.483399       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.483490545Z I0109 04:42:14.483429       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.485263163Z E0109 04:42:14.485238       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.488909470Z E0109 04:42:14.488886       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.489794606Z I0109 04:42:14.489763       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nBootstrapTeardownDegraded: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:14.498780758Z I0109 04:42:14.498751       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:14.499420948Z I0109 04:42:14.499392       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:14.500486391Z E0109 04:42:14.500460       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.510504902Z I0109 04:42:14.510467       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nClusterMemberControllerDegraded: could not get list of unhealthy members: getting cache client could not retrieve endpoints: node lister not synced\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:14.527688147Z E0109 04:42:14.527652       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.637474627Z I0109 04:42:14.637441       1 base_controller.go:73] Caches are synced for ResourceSyncController 
2023-01-09T04:42:14.637474627Z I0109 04:42:14.637459       1 base_controller.go:110] Starting #1 worker of ResourceSyncController controller ...
2023-01-09T04:42:14.638556063Z I0109 04:42:14.638524       1 base_controller.go:73] Caches are synced for EtcdCertSignerController 
2023-01-09T04:42:14.638556063Z I0109 04:42:14.638542       1 base_controller.go:110] Starting #1 worker of EtcdCertSignerController controller ...
2023-01-09T04:42:14.666199376Z E0109 04:42:14.666159       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:14.851096749Z E0109 04:42:14.851062       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:15.192966919Z E0109 04:42:15.192928       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:15.337513973Z I0109 04:42:15.337479       1 request.go:601] Waited for 1.699236159s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:15.537926315Z I0109 04:42:15.537887       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:15.539171237Z I0109 04:42:15.539097       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:15.542648497Z E0109 04:42:15.542606       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:15.547419725Z I0109 04:42:15.547382       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "EtcdMembersControllerDegraded: getting cache client could not retrieve endpoints: node lister not synced\nNodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:15.579646080Z E0109 04:42:15.579615       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:15.748914860Z I0109 04:42:15.748873       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:15.749302729Z I0109 04:42:15.749273       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:15.751028245Z E0109 04:42:15.750966       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:15.759950799Z I0109 04:42:15.759912       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nScriptControllerDegraded: \"configmap/etcd-pod\": missing env var values\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:16.134182692Z E0109 04:42:16.134149       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:16.221567494Z E0109 04:42:16.221505       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:16.537450503Z I0109 04:42:16.537417       1 request.go:601] Waited for 1.397823099s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:42:16.939058183Z I0109 04:42:16.939019       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:16.961659010Z I0109 04:42:16.961589       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:16.964635364Z I0109 04:42:16.964598       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:16.966166265Z E0109 04:42:16.966135       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:16.976289141Z I0109 04:42:16.976221       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nInstallerControllerDegraded: missing required resources: [secrets: etcd-all-certs, secrets: etcd-all-certs-2]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:17.149917456Z E0109 04:42:17.149870       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:17.156378408Z E0109 04:42:17.153741       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:17.246753624Z E0109 04:42:17.246708       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:17.537502760Z I0109 04:42:17.537455       1 request.go:601] Waited for 1.396282841s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:42:18.147132556Z E0109 04:42:18.147092       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:18.341683792Z E0109 04:42:18.341649       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:18.736777152Z I0109 04:42:18.736737       1 request.go:601] Waited for 1.193948668s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:42:19.234699957Z E0109 04:42:19.234647       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:19.339073721Z I0109 04:42:19.339036       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:19.737185759Z I0109 04:42:19.737146       1 request.go:601] Waited for 1.198389271s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:42:21.339176383Z I0109 04:42:21.339137       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:23.270624756Z E0109 04:42:23.270579       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:23.365262243Z E0109 04:42:23.365225       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:25.232453978Z E0109 04:42:25.232418       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:25.518146806Z E0109 04:42:25.518100       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:26.379880365Z I0109 04:42:26.379843       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:26.381827992Z I0109 04:42:26.381799       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:26.381827992Z I0109 04:42:26.381817       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:26.381856078Z I0109 04:42:26.381826       1 clustermembercontroller.go:203] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up: no Machine found referencing this node's internal IP (10.0.145.4)
2023-01-09T04:42:26.402206533Z I0109 04:42:26.402164       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:26.402206533Z I0109 04:42:26.402181       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:26.402206533Z I0109 04:42:26.402190       1 clustermembercontroller.go:203] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up: no Machine found referencing this node's internal IP (10.0.145.4)
2023-01-09T04:42:28.362622525Z E0109 04:42:28.362588       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:28.367924824Z I0109 04:42:28.367875       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:28.379159977Z I0109 04:42:28.379128       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:28.379184260Z I0109 04:42:28.379157       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:28.379184260Z I0109 04:42:28.379172       1 clustermembercontroller.go:203] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up: no Machine found referencing this node's internal IP (10.0.145.4)
2023-01-09T04:42:33.515501337Z E0109 04:42:33.515461       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:34.545173466Z I0109 04:42:34.545126       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:34.551805042Z I0109 04:42:34.551772       1 clustermembercontroller.go:203] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up: no Machine found referencing this node's internal IP (10.0.145.4)
2023-01-09T04:42:34.551805042Z I0109 04:42:34.551790       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:34.551837264Z I0109 04:42:34.551801       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:34.793962099Z I0109 04:42:34.793926       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.02 %, dbSize: 48742400
2023-01-09T04:42:35.354345536Z I0109 04:42:35.354306       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:35.360961799Z I0109 04:42:35.360933       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:35.361640210Z I0109 04:42:35.361618       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:35.361662709Z I0109 04:42:35.361642       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:35.361662709Z I0109 04:42:35.361656       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:35.381799191Z I0109 04:42:35.381759       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:35.381799191Z I0109 04:42:35.381780       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:35.381799191Z I0109 04:42:35.381789       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:36.497261494Z I0109 04:42:36.497219       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:36.507049976Z I0109 04:42:36.507024       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:36.507049976Z I0109 04:42:36.507041       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:36.507079507Z I0109 04:42:36.507051       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:36.617466839Z E0109 04:42:36.617426       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:40.909902601Z E0109 04:42:40.909853       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:40.914700083Z I0109 04:42:40.914673       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:40.929193024Z I0109 04:42:40.929161       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:40.929193024Z I0109 04:42:40.929182       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:40.929220476Z I0109 04:42:40.929194       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:42.886780302Z E0109 04:42:42.886735       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:42.891863872Z I0109 04:42:42.891831       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:42.904340475Z I0109 04:42:42.904309       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:42.904340475Z I0109 04:42:42.904330       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:42.904379652Z I0109 04:42:42.904342       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:44.631543760Z E0109 04:42:44.631511       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:44.634450089Z I0109 04:42:44.634418       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:44.644373927Z I0109 04:42:44.644341       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:44.644373927Z I0109 04:42:44.644366       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:44.644407460Z I0109 04:42:44.644376       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.468012544Z I0109 04:42:46.467948       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:46.471040244Z I0109 04:42:46.470970       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/etcd version "etcd" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:46.471040244Z I0109 04:42:46.471027       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/etcd version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:46.471276729Z I0109 04:42:46.471258       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"etcd","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:42:46.472214724Z E0109 04:42:46.472182       1 base_controller.go:272] MachineDeletionHooksController reconciliation failed: [Operation cannot be fulfilled on machines.machine.openshift.io "sn-loggvls-jsm-pkkrc-master-2": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io "sn-loggvls-jsm-pkkrc-master-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io "sn-loggvls-jsm-pkkrc-master-1": the object has been modified; please apply your changes to the latest version and try again]
2023-01-09T04:42:46.475384655Z I0109 04:42:46.475355       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:46.477753783Z E0109 04:42:46.477723       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:46.479672026Z I0109 04:42:46.479645       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"NodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}],"versions":[{"name":"raw-internal","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"etcd","version":"4.12.0-0.nightly-2023-01-08-142418"},{"name":"operator","version":"4.12.0-0.nightly-2023-01-08-142418"}]}}
2023-01-09T04:42:46.479953592Z I0109 04:42:46.479927       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: status.versions changed from [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"}] to [{"raw-internal" "4.12.0-0.nightly-2023-01-08-142418"} {"etcd" "4.12.0-0.nightly-2023-01-08-142418"} {"operator" "4.12.0-0.nightly-2023-01-08-142418"}]
2023-01-09T04:42:46.481207796Z I0109 04:42:46.481180       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/etcd version "etcd" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:46.481254932Z I0109 04:42:46.481238       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorVersionChanged' clusteroperator/etcd version "operator" changed from "" to "4.12.0-0.nightly-2023-01-08-142418"
2023-01-09T04:42:46.486126724Z E0109 04:42:46.486100       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:42:46.487242474Z I0109 04:42:46.487216       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:46.488075760Z I0109 04:42:46.487519       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"NodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:46.498663778Z I0109 04:42:46.498619       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "NodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:46.499217928Z I0109 04:42:46.499185       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:46.502171625Z I0109 04:42:46.502139       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.502200709Z I0109 04:42:46.502167       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.502200709Z I0109 04:42:46.502189       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.509737539Z I0109 04:42:46.509703       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:46.520901207Z I0109 04:42:46.520857       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.24 %, dbSize: 49958912
2023-01-09T04:42:46.527136643Z I0109 04:42:46.527106       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.527136643Z I0109 04:42:46.527126       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:46.527175347Z I0109 04:42:46.527136       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.006565352Z I0109 04:42:47.006519       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.013352029Z E0109 04:42:47.013317       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.013352029Z E0109 04:42:47.013347       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.013386565Z E0109 04:42:47.013363       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.013454392Z I0109 04:42:47.013431       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodDisruptionBudgetCreated' Created PodDisruptionBudget.policy/etcd-guard-pdb -n openshift-etcd because it was missing
2023-01-09T04:42:47.023321884Z E0109 04:42:47.023281       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.024626469Z I0109 04:42:47.024596       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:47.024759047Z I0109 04:42:47.024723       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:47.026580015Z E0109 04:42:47.026552       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:47.026851728Z I0109 04:42:47.026830       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.026868268Z I0109 04:42:47.026856       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.026911117Z I0109 04:42:47.026887       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.031154861Z E0109 04:42:47.031129       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.031154861Z E0109 04:42:47.031149       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.031183380Z E0109 04:42:47.031156       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.032372676Z I0109 04:42:47.032331       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:47.034022036Z I0109 04:42:47.033475       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.040726195Z E0109 04:42:47.040698       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal]
2023-01-09T04:42:47.041323565Z I0109 04:42:47.041296       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:47.041572007Z I0109 04:42:47.041523       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:47.044400514Z E0109 04:42:47.044375       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:47.049748940Z I0109 04:42:47.049713       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:47.051387423Z I0109 04:42:47.051358       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.051889038Z I0109 04:42:47.051866       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.14 %, dbSize: 49958912
2023-01-09T04:42:47.053954211Z E0109 04:42:47.053922       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.053976999Z E0109 04:42:47.053942       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.053976999Z E0109 04:42:47.053952       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.056789291Z I0109 04:42:47.056768       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.056821064Z I0109 04:42:47.056786       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.056821064Z I0109 04:42:47.056798       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.063152843Z E0109 04:42:47.063130       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.063771072Z I0109 04:42:47.063749       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:47.064406428Z I0109 04:42:47.064380       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:47.070711920Z E0109 04:42:47.070682       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:47.071939519Z I0109 04:42:47.071875       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:47.074687203Z I0109 04:42:47.074646       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.085427602Z E0109 04:42:47.085398       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.085448451Z E0109 04:42:47.085426       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.085448451Z E0109 04:42:47.085437       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.085699031Z E0109 04:42:47.085676       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.086602704Z I0109 04:42:47.086567       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.14 %, dbSize: 49958912
2023-01-09T04:42:47.089726732Z I0109 04:42:47.089707       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.089746913Z I0109 04:42:47.089723       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.089746913Z I0109 04:42:47.089732       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.109870463Z I0109 04:42:47.109843       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.14 %, dbSize: 49958912
2023-01-09T04:42:47.127896137Z E0109 04:42:47.127871       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.127896137Z E0109 04:42:47.127892       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.127919807Z E0109 04:42:47.127902       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.128114434Z E0109 04:42:47.128101       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.210582122Z E0109 04:42:47.210549       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.210582122Z E0109 04:42:47.210573       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.210615706Z E0109 04:42:47.210583       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.210813017Z E0109 04:42:47.210800       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.373204353Z E0109 04:42:47.373166       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.373204353Z E0109 04:42:47.373186       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.373204353Z E0109 04:42:47.373198       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.381534216Z E0109 04:42:47.381499       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:42:47.382663396Z I0109 04:42:47.382630       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:47.384695040Z I0109 04:42:47.384663       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:47.386680226Z E0109 04:42:47.386651       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:47.391798409Z I0109 04:42:47.391032       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:47.393560988Z I0109 04:42:47.393531       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.406373499Z I0109 04:42:47.406347       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.406373499Z I0109 04:42:47.406368       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.406398709Z I0109 04:42:47.406377       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.413942384Z I0109 04:42:47.413914       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.05 %, dbSize: 49975296
2023-01-09T04:42:47.464827144Z I0109 04:42:47.464790       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:42:47.704428258Z E0109 04:42:47.704388       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:47.704428258Z E0109 04:42:47.704416       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:47.704463257Z E0109 04:42:47.704429       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:47.714062696Z E0109 04:42:47.714025       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:47.714405274Z I0109 04:42:47.714386       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:47.715394493Z I0109 04:42:47.715357       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:47.717642635Z E0109 04:42:47.717610       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:47.721821101Z I0109 04:42:47.721780       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:47.723968469Z I0109 04:42:47.722509       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:47.732690423Z I0109 04:42:47.732666       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.732711072Z I0109 04:42:47.732692       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.732711072Z I0109 04:42:47.732707       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:47.742837325Z I0109 04:42:47.742805       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 50020352
2023-01-09T04:42:48.062621285Z I0109 04:42:48.062579       1 request.go:601] Waited for 1.056076866s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:42:48.357980592Z E0109 04:42:48.357940       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:48.357980592Z E0109 04:42:48.357960       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:48.357980592Z E0109 04:42:48.357972       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:48.366329021Z E0109 04:42:48.366289       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:42:48.367047714Z I0109 04:42:48.367024       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:48.369011606Z I0109 04:42:48.368923       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:48.375661192Z I0109 04:42:48.375618       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:48.375874235Z E0109 04:42:48.375853       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:48.376976786Z I0109 04:42:48.376950       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:48.394663441Z I0109 04:42:48.394629       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:48.394663441Z I0109 04:42:48.394650       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:48.394690919Z I0109 04:42:48.394661       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:48.404015057Z I0109 04:42:48.403977       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 50024448
2023-01-09T04:42:49.019265855Z I0109 04:42:49.019209       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:49.021414900Z E0109 04:42:49.021380       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:49.021414900Z E0109 04:42:49.021408       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:49.021446835Z E0109 04:42:49.021425       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:49.029814515Z E0109 04:42:49.029780       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:49.030850294Z I0109 04:42:49.030822       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:49.032548119Z I0109 04:42:49.032522       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:49.033695484Z E0109 04:42:49.033670       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:49.041109746Z I0109 04:42:49.041069       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.041109746Z I0109 04:42:49.041098       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.041140267Z I0109 04:42:49.041113       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.041547436Z I0109 04:42:49.041524       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:49.042143910Z I0109 04:42:49.042101       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:49.059814259Z I0109 04:42:49.059786       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.02 %, dbSize: 50065408
2023-01-09T04:42:49.062441818Z I0109 04:42:49.062422       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.062459894Z I0109 04:42:49.062440       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.062459894Z I0109 04:42:49.062450       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:49.262474804Z I0109 04:42:49.262435       1 request.go:601] Waited for 1.397503849s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:49.649544218Z E0109 04:42:49.649510       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:49.649544218Z E0109 04:42:49.649532       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:49.649581467Z E0109 04:42:49.649543       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:49.649778529Z E0109 04:42:49.649765       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:50.022570854Z I0109 04:42:50.022529       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:50.040483807Z I0109 04:42:50.040451       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:50.040483807Z I0109 04:42:50.040472       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:50.040516439Z I0109 04:42:50.040482       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:50.262777799Z I0109 04:42:50.262736       1 request.go:601] Waited for 1.243499844s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:42:51.026638858Z I0109 04:42:51.026598       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:51.043194315Z I0109 04:42:51.043156       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:51.043194315Z I0109 04:42:51.043178       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:51.043232304Z I0109 04:42:51.043191       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:51.263093090Z I0109 04:42:51.263045       1 request.go:601] Waited for 1.397621662s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:42:51.664975928Z I0109 04:42:51.664938       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:42:52.029597066Z I0109 04:42:52.029544       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:52.046883703Z I0109 04:42:52.046852       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:52.046883703Z I0109 04:42:52.046871       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:52.046921509Z I0109 04:42:52.046881       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:52.463259061Z I0109 04:42:52.463214       1 request.go:601] Waited for 1.398219943s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:42:53.034383563Z I0109 04:42:53.034335       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:53.053227694Z I0109 04:42:53.053184       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:53.053227694Z I0109 04:42:53.053216       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:53.053260970Z I0109 04:42:53.053227       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:53.662669336Z I0109 04:42:53.662626       1 request.go:601] Waited for 1.397953968s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:54.002449589Z E0109 04:42:54.002413       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:42:54.267547031Z E0109 04:42:54.267489       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:54.267547031Z E0109 04:42:54.267519       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:54.267547031Z I0109 04:42:54.267533       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:54.267685098Z I0109 04:42:54.267642       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:42:54.274753067Z I0109 04:42:54.274719       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:54.277957705Z E0109 04:42:54.277935       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:54.279017967Z I0109 04:42:54.278971       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:54.279369459Z I0109 04:42:54.279337       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:54.282716800Z E0109 04:42:54.282687       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:54.286181775Z I0109 04:42:54.286152       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:54.288589799Z I0109 04:42:54.287915       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-199-219.us-east-2.compute.internal on node ip-10-0-199-219.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:54.289125309Z I0109 04:42:54.289098       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:54.292256444Z I0109 04:42:54.292226       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.292256444Z I0109 04:42:54.292251       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.292283077Z I0109 04:42:54.292260       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.307301264Z I0109 04:42:54.307270       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.11 %, dbSize: 50454528
2023-01-09T04:42:54.313280889Z I0109 04:42:54.313255       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.313280889Z I0109 04:42:54.313273       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.313307346Z I0109 04:42:54.313284       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.662984419Z I0109 04:42:54.662945       1 request.go:601] Waited for 1.397015544s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:42:54.821965320Z I0109 04:42:54.821916       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:54.838130007Z I0109 04:42:54.838097       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.838130007Z I0109 04:42:54.838116       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:54.838162125Z I0109 04:42:54.838125       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:55.375212441Z I0109 04:42:55.375148       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:55.863084791Z I0109 04:42:55.863044       1 request.go:601] Waited for 1.580225001s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:42:56.033106828Z I0109 04:42:56.033057       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:56.050400636Z I0109 04:42:56.050363       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.050400636Z I0109 04:42:56.050383       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.050400636Z I0109 04:42:56.050393       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.065469231Z I0109 04:42:56.065432       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:42:56.518367599Z I0109 04:42:56.518337       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:42:56.524074066Z I0109 04:42:56.524043       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:56.528507719Z I0109 04:42:56.528480       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.528507719Z I0109 04:42:56.528499       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.528530664Z I0109 04:42:56.528509       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:56.540844845Z I0109 04:42:56.540453       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:57.063293783Z I0109 04:42:57.063256       1 request.go:601] Waited for 1.398056351s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:42:57.264623518Z E0109 04:42:57.264580       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:57.264623518Z E0109 04:42:57.264606       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:57.264839514Z E0109 04:42:57.264823       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:42:57.266866519Z E0109 04:42:57.266847       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:42:58.063377637Z I0109 04:42:58.063329       1 request.go:601] Waited for 1.398413943s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:42:58.465418227Z I0109 04:42:58.465377       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:58.587144966Z I0109 04:42:58.587102       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:58.974754986Z I0109 04:42:58.974713       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:59.464530154Z E0109 04:42:59.464490       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:42:59.472207908Z E0109 04:42:59.472174       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:42:59.473879887Z I0109 04:42:59.473840       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:42:59.474042310Z I0109 04:42:59.474017       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:42:59.475870166Z E0109 04:42:59.475841       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:42:59.483211180Z I0109 04:42:59.483182       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:42:59.483547375Z I0109 04:42:59.483515       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:42:59.491776319Z I0109 04:42:59.491751       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:59.491804056Z I0109 04:42:59.491774       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:59.491804056Z I0109 04:42:59.491788       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:42:59.667353193Z I0109 04:42:59.667312       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:00.663457155Z I0109 04:43:00.663416       1 request.go:601] Waited for 1.189571458s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:43:00.777083137Z I0109 04:43:00.777042       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:01.983363478Z I0109 04:43:01.983329       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:02.665786669Z I0109 04:43:02.665744       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:02.775310286Z I0109 04:43:02.775267       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:03.375699669Z I0109 04:43:03.375660       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:04.999211867Z I0109 04:43:04.999171       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:05.010865255Z I0109 04:43:05.010825       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:07.169885141Z I0109 04:43:07.169837       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:07.797281209Z I0109 04:43:07.797241       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:10.400411045Z I0109 04:43:10.400299       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:10.410162498Z I0109 04:43:10.410119       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:10.413889962Z I0109 04:43:10.413848       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:11.760272060Z I0109 04:43:11.760234       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:11.771698445Z I0109 04:43:11.771646       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:12.746525617Z I0109 04:43:12.746484       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:12.770938855Z I0109 04:43:12.770899       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:12.942671277Z E0109 04:43:12.942632       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:43:13.566069935Z E0109 04:43:13.566031       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:13.572921805Z I0109 04:43:13.572889       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:43:13.573580329Z E0109 04:43:13.573529       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:13.573580329Z E0109 04:43:13.573551       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:13.581828965Z I0109 04:43:13.581794       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.581828965Z I0109 04:43:13.581817       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.581857851Z I0109 04:43:13.581827       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.582836400Z E0109 04:43:13.582808       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:13.585377854Z I0109 04:43:13.585342       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:13.588861233Z I0109 04:43:13.588832       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:13.591280062Z E0109 04:43:13.591250       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:13.595290395Z I0109 04:43:13.592178       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:43:13.595290395Z I0109 04:43:13.593705       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:13.597782583Z I0109 04:43:13.597744       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:13.612449813Z I0109 04:43:13.612419       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.612449813Z I0109 04:43:13.612439       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.612473739Z I0109 04:43:13.612449       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.637759392Z I0109 04:43:13.637725       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:13.854742473Z I0109 04:43:13.854708       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.854742473Z I0109 04:43:13.854727       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:13.854742473Z I0109 04:43:13.854737       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:14.440793811Z E0109 04:43:14.440751       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:14.639393073Z I0109 04:43:14.639351       1 request.go:601] Waited for 1.052075229s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:43:15.027211344Z E0109 04:43:15.027170       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:15.028589611Z E0109 04:43:15.028555       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:15.032640846Z I0109 04:43:15.032610       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:43:15.043184973Z I0109 04:43:15.043157       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:15.043184973Z I0109 04:43:15.043175       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-160-211.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-1) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:15.043210636Z I0109 04:43:15.043185       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:15.169757050Z I0109 04:43:15.169720       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:15.640186813Z I0109 04:43:15.640144       1 request.go:601] Waited for 1.198444706s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-2-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:15.706898572Z I0109 04:43:15.706858       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:16.516347228Z I0109 04:43:16.516307       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:43:16.523056817Z E0109 04:43:16.518842       1 base_controller.go:272] MachineDeletionHooksController reconciliation failed: [Operation cannot be fulfilled on machines.machine.openshift.io "sn-loggvls-jsm-pkkrc-master-2": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io "sn-loggvls-jsm-pkkrc-master-0": the object has been modified; please apply your changes to the latest version and try again]
2023-01-09T04:43:16.523056817Z I0109 04:43:16.520014       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:16.523056817Z I0109 04:43:16.521449       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:16.523056817Z E0109 04:43:16.522052       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:16.531775721Z I0109 04:43:16.531070       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:16.532984869Z I0109 04:43:16.532936       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-1\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:43:16.533455862Z I0109 04:43:16.533432       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:16.533486483Z I0109 04:43:16.533471       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:16.556467828Z I0109 04:43:16.556433       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:16.556467828Z I0109 04:43:16.556461       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:16.839806929Z I0109 04:43:16.839766       1 request.go:601] Waited for 1.197399827s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:16.842404911Z I0109 04:43:16.842377       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:17.241892830Z E0109 04:43:17.241848       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:17.250164312Z E0109 04:43:17.250135       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:43:17.251453723Z I0109 04:43:17.251430       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:17.251831401Z I0109 04:43:17.251791       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:17.257376048Z E0109 04:43:17.257343       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:17.261281885Z I0109 04:43:17.261242       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:43:17.265648364Z I0109 04:43:17.265619       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:17.274164246Z I0109 04:43:17.274132       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:17.274185542Z I0109 04:43:17.274169       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:17.839839187Z I0109 04:43:17.839800       1 request.go:601] Waited for 1.319839434s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:43:17.999167704Z I0109 04:43:17.999128       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:18.000882434Z I0109 04:43:18.000856       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:19.039519455Z I0109 04:43:19.039462       1 request.go:601] Waited for 1.291378529s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:19.114268879Z I0109 04:43:19.114228       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:19.436468765Z I0109 04:43:19.436436       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:43:19.442324780Z I0109 04:43:19.442297       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:19.442362939Z I0109 04:43:19.442325       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:20.239463671Z I0109 04:43:20.239428       1 request.go:601] Waited for 1.197661997s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:20.241526466Z E0109 04:43:20.241490       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:20.241526466Z E0109 04:43:20.241512       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:20.249902697Z E0109 04:43:20.249872       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:20.251299305Z I0109 04:43:20.251270       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:20.251570399Z I0109 04:43:20.251524       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:20.253338561Z E0109 04:43:20.253312       1 base_controller.go:272] EtcdEndpointsController reconciliation failed: no etcd members are present
2023-01-09T04:43:20.266967646Z I0109 04:43:20.259034       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present"
2023-01-09T04:43:20.266967646Z I0109 04:43:20.262299       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:20.270058202Z I0109 04:43:20.270029       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-199-219.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-2) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:20.270086379Z I0109 04:43:20.270060       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:20.278691713Z I0109 04:43:20.278659       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.02 %, dbSize: 53157888
2023-01-09T04:43:20.492750310Z I0109 04:43:20.492706       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:20.842651851Z I0109 04:43:20.842608       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:21.440310876Z I0109 04:43:21.440274       1 request.go:601] Waited for 1.188641414s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:43:22.197508200Z I0109 04:43:22.197461       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:22.225271636Z I0109 04:43:22.225231       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:23.842229014Z I0109 04:43:23.842186       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:25.710014679Z I0109 04:43:25.709957       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:26.536495960Z I0109 04:43:26.536463       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (1) < desired number of control-plane replicas (3) 
2023-01-09T04:43:26.550857268Z I0109 04:43:26.550816       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberAddAsLearner' successfully added new member https://10.0.199.219:2380
2023-01-09T04:43:26.558174021Z I0109 04:43:26.558145       1 clustermembercontroller.go:283] Not ready for promotion: etcd learner member (https://10.0.199.219:2380) is not yet in sync with leader's log 
2023-01-09T04:43:28.171266482Z I0109 04:43:28.171228       1 quorumguardcleanupcontroller.go:134] 0/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:30.056069104Z I0109 04:43:30.056024       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:30.065636690Z E0109 04:43:30.065598       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:30.065636690Z E0109 04:43:30.065618       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:30.065856414Z E0109 04:43:30.065840       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:30.068041868Z I0109 04:43:30.068013       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:30.081767026Z I0109 04:43:30.081740       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:30.088490605Z I0109 04:43:30.088456       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberPromote' successfully promoted learner member https://10.0.199.219:2380
2023-01-09T04:43:31.949433825Z I0109 04:43:31.949400       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:31.972448640Z I0109 04:43:31.972415       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:32.022738262Z E0109 04:43:32.022705       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:32.022738262Z E0109 04:43:32.022732       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:32.022962853Z E0109 04:43:32.022945       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:32.023436877Z I0109 04:43:32.023410       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:32.023870023Z W0109 04:43:32.023706       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379]] vs. [[https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:43:32.027790123Z I0109 04:43:32.027756       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2023-01-09T04:43:32.027790123Z cause by changes in data.cebc4266beb3f2a1
2023-01-09T04:43:32.030563753Z I0109 04:43:32.030533       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 3 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:43:32.036076685Z W0109 04:43:32.036047       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379]] vs. [[https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:43:32.043572568Z W0109 04:43:32.043543       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379]] vs. [[https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:43:32.046714145Z I0109 04:43:32.046651       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:43:32.047513937Z I0109 04:43:32.047470       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:32.049770844Z W0109 04:43:32.049741       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379]] vs. [[https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:43:32.055228308Z I0109 04:43:32.050677       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-3 -n openshift-etcd because it was missing
2023-01-09T04:43:32.055228308Z I0109 04:43:32.052057       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:32.057358567Z I0109 04:43:32.057321       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nEtcdEndpointsDegraded: no etcd members are present" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:32.060161481Z I0109 04:43:32.057946       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:32.060834486Z I0109 04:43:32.060781       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:32.072727227Z I0109 04:43:32.072695       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:32.091025639Z I0109 04:43:32.090905       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:32.097684861Z I0109 04:43:32.097652       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:32.098210719Z I0109 04:43:32.098178       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:32.127680576Z I0109 04:43:32.127645       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:32.166401550Z I0109 04:43:32.166367       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:32.178102890Z I0109 04:43:32.178072       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:32.812636825Z I0109 04:43:32.812584       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-3 -n openshift-etcd because it was missing
2023-01-09T04:43:32.831170219Z I0109 04:43:32.831134       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:32.857069590Z I0109 04:43:32.857039       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:33.206157087Z I0109 04:43:33.206114       1 request.go:601] Waited for 1.154292175s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:34.014534429Z I0109 04:43:34.014485       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-3 -n openshift-etcd because it was missing
2023-01-09T04:43:34.040284071Z I0109 04:43:34.040242       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:34.057379332Z I0109 04:43:34.057339       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:34.206355242Z I0109 04:43:34.206316       1 request.go:601] Waited for 1.196141903s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:43:34.613275888Z I0109 04:43:34.613229       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2023-01-09T04:43:34.613275888Z cause by changes in data.etcd.env
2023-01-09T04:43:34.634706248Z I0109 04:43:34.634669       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:34.659726292Z I0109 04:43:34.659641       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:34.969757132Z E0109 04:43:34.969719       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:43:35.011007123Z I0109 04:43:35.010953       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 2, but has not made progress because static pod is pending
2023-01-09T04:43:35.211127555Z I0109 04:43:35.211079       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-3 -n openshift-etcd because it was missing
2023-01-09T04:43:35.238132992Z I0109 04:43:35.238094       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:35.255939886Z I0109 04:43:35.255908       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:35.421683368Z I0109 04:43:35.421643       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:35.422110199Z I0109 04:43:35.422081       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:35.429750467Z I0109 04:43:35.429710       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready"
2023-01-09T04:43:35.432243039Z I0109 04:43:35.432193       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:35.471295966Z I0109 04:43:35.471260       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:35.478836125Z I0109 04:43:35.478796       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:35.494977762Z I0109 04:43:35.494944       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 54153216
2023-01-09T04:43:35.617548925Z I0109 04:43:35.617499       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2023-01-09T04:43:35.617548925Z cause by changes in data.pod.yaml
2023-01-09T04:43:35.636077039Z I0109 04:43:35.636045       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:35.661729606Z I0109 04:43:35.661697       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:35.813172397Z I0109 04:43:35.813124       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-3 -n openshift-etcd because it was missing
2023-01-09T04:43:35.833294870Z I0109 04:43:35.833258       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:35.856815598Z I0109 04:43:35.856783       1 clustermembercontroller.go:214] Ignoring node (ip-10-0-145-4.us-east-2.compute.internal) for scale-up since its machine (sn-loggvls-jsm-pkkrc-master-0) is missing the PreDrain deletion hook (name: EtcdQuorumOperator, owner: clusteroperator/etcd)
2023-01-09T04:43:36.552947126Z I0109 04:43:36.552910       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"StaticPodsAvailable: 0 nodes are active; 3 nodes are at revision 0; 0 nodes have achieved new revision 2","reason":"StaticPods_ZeroNodesActive","status":"False","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:36.553023004Z I0109 04:43:36.552986       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:36.561577849Z I0109 04:43:36.561534       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nMachineDeletionHooksControllerDegraded: [Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-2\": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on machines.machine.openshift.io \"sn-loggvls-jsm-pkkrc-master-0\": the object has been modified; please apply your changes to the latest version and try again]\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready"
2023-01-09T04:43:36.564230741Z I0109 04:43:36.564194       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:36.571039336Z I0109 04:43:36.571012       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:36.606312665Z I0109 04:43:36.606281       1 request.go:601] Waited for 1.183787981s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:43:36.790740832Z I0109 04:43:36.790701       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:36.813419722Z I0109 04:43:36.813384       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:37.212527594Z I0109 04:43:37.212480       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-3 -n openshift-etcd because it was missing
2023-01-09T04:43:37.243507786Z I0109 04:43:37.243471       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:37.333023268Z I0109 04:43:37.332962       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:37.806105538Z I0109 04:43:37.806065       1 request.go:601] Waited for 1.253862843s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:43:37.864041139Z I0109 04:43:37.863182       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:37.904963304Z I0109 04:43:37.904912       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:38.268506667Z I0109 04:43:38.268452       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:38.806769567Z I0109 04:43:38.806725       1 request.go:601] Waited for 1.79687985s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:43:38.813982936Z I0109 04:43:38.813945       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2023-01-09T04:43:38.813982936Z cause by changes in data.pod.yaml
2023-01-09T04:43:38.838829000Z I0109 04:43:38.838793       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:39.014780336Z I0109 04:43:39.014735       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-3 -n openshift-etcd because it was missing
2023-01-09T04:43:39.038163388Z I0109 04:43:39.038128       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:39.806856938Z I0109 04:43:39.806821       1 request.go:601] Waited for 1.596387682s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:40.009714772Z E0109 04:43:40.009681       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:40.009714772Z E0109 04:43:40.009703       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:40.009956026Z E0109 04:43:40.009926       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:40.210283291Z I0109 04:43:40.210240       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:40.210283291Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:43:40.210283291Z  CurrentRevision: (int32) 2,
2023-01-09T04:43:40.210283291Z  TargetRevision: (int32) 0,
2023-01-09T04:43:40.210283291Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:40.210283291Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:40.210283291Z  LastFailedReason: (string) "",
2023-01-09T04:43:40.210283291Z  LastFailedCount: (int) 0,
2023-01-09T04:43:40.210283291Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:40.210283291Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:40.210283291Z }
2023-01-09T04:43:40.210283291Z  because static pod is ready
2023-01-09T04:43:40.220889600Z I0109 04:43:40.220840       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 0 to 2 because static pod is ready
2023-01-09T04:43:40.221761159Z I0109 04:43:40.221720       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:40.222301513Z I0109 04:43:40.222275       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:40.231075065Z I0109 04:43:40.231027       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 0; 0 nodes have achieved new revision 2" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2",Available changed from False to True ("StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2\nEtcdMembersAvailable: 1 members are available")
2023-01-09T04:43:40.231469938Z I0109 04:43:40.231437       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:40.288297750Z I0109 04:43:40.288262       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 54743040
2023-01-09T04:43:40.613683463Z I0109 04:43:40.613640       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-3 -n openshift-etcd because it was missing
2023-01-09T04:43:40.625208030Z I0109 04:43:40.625172       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:40.626011836Z I0109 04:43:40.625946       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 2 created because configmap/etcd-endpoints has changed
2023-01-09T04:43:40.630332940Z I0109 04:43:40.630296       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 4 triggered by "configmap/etcd-pod has changed"
2023-01-09T04:43:41.006038005Z I0109 04:43:41.005978       1 request.go:601] Waited for 1.595706902s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:43:42.206166384Z I0109 04:43:42.206128       1 request.go:601] Waited for 1.575826684s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2023-01-09T04:43:42.211047575Z I0109 04:43:42.210986       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-4 -n openshift-etcd because it was missing
2023-01-09T04:43:42.227763399Z I0109 04:43:42.227732       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:43.010485999Z E0109 04:43:43.010447       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:43.010485999Z E0109 04:43:43.010473       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:43.010746459Z E0109 04:43:43.010731       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:43.206634224Z I0109 04:43:43.206597       1 request.go:601] Waited for 1.593734105s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:43:43.410798820Z I0109 04:43:43.410765       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal static pod not found and needs new revision 2
2023-01-09T04:43:43.410827681Z I0109 04:43:43.410804       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:43.410827681Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:43.410827681Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:43.410827681Z  TargetRevision: (int32) 2,
2023-01-09T04:43:43.410827681Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:43.410827681Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:43.410827681Z  LastFailedReason: (string) "",
2023-01-09T04:43:43.410827681Z  LastFailedCount: (int) 0,
2023-01-09T04:43:43.410827681Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:43.410827681Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:43.410827681Z }
2023-01-09T04:43:43.421451377Z I0109 04:43:43.421413       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:43.422516505Z I0109 04:43:43.421033       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 0 to 2 because node ip-10-0-160-211.us-east-2.compute.internal static pod not found
2023-01-09T04:43:43.422766193Z I0109 04:43:43.422738       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:43.432437463Z I0109 04:43:43.432393       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2\nEtcdMembersAvailable: 1 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available"
2023-01-09T04:43:43.432548656Z I0109 04:43:43.432524       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:43.457715407Z I0109 04:43:43.456795       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:43.469031537Z I0109 04:43:43.468978       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:43.496064606Z I0109 04:43:43.496029       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 54353920
2023-01-09T04:43:43.496064606Z I0109 04:43:43.496043       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.03 %, dbSize: 54931456
2023-01-09T04:43:43.812875132Z I0109 04:43:43.812813       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-4 -n openshift-etcd because it was missing
2023-01-09T04:43:43.839788741Z I0109 04:43:43.839757       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:44.020972427Z I0109 04:43:44.020934       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:44.021440764Z I0109 04:43:44.021408       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:44.029226015Z I0109 04:43:44.029175       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found\nStaticPodsDegraded: pod/etcd-ip-10-0-199-219.us-east-2.compute.internal container \"etcd\" started at 2023-01-09 04:42:52 +0000 UTC is still not ready" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:44.030753177Z I0109 04:43:44.030708       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:44.092224818Z I0109 04:43:44.092181       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 54460416
2023-01-09T04:43:44.216914465Z I0109 04:43:44.216866       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:44.221813461Z I0109 04:43:44.221061       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:44.407074438Z I0109 04:43:44.407037       1 request.go:601] Waited for 1.59690168s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:43:45.212737648Z I0109 04:43:45.212690       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-4 -n openshift-etcd because it was missing
2023-01-09T04:43:45.236925086Z I0109 04:43:45.236884       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:45.450528645Z I0109 04:43:45.450492       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:45.462434577Z I0109 04:43:45.462394       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:45.606232208Z I0109 04:43:45.606190       1 request.go:601] Waited for 1.41261957s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:46.411101610Z I0109 04:43:46.411062       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:46.411101610Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:46.411101610Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:46.411101610Z  TargetRevision: (int32) 3,
2023-01-09T04:43:46.411101610Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:46.411101610Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:46.411101610Z  LastFailedReason: (string) "",
2023-01-09T04:43:46.411101610Z  LastFailedCount: (int) 0,
2023-01-09T04:43:46.411101610Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:46.411101610Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:46.411101610Z }
2023-01-09T04:43:46.411101610Z  because new revision pending
2023-01-09T04:43:46.422789620Z I0109 04:43:46.422747       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:46.486760883Z I0109 04:43:46.486723       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 54579200
2023-01-09T04:43:46.486760883Z I0109 04:43:46.486738       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 55144448
2023-01-09T04:43:46.606385835Z I0109 04:43:46.606345       1 request.go:601] Waited for 1.59481808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:43:46.811738574Z I0109 04:43:46.811689       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-4 -n openshift-etcd because it was missing
2023-01-09T04:43:46.829936666Z I0109 04:43:46.829897       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:47.209356146Z E0109 04:43:47.209321       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:47.209356146Z E0109 04:43:47.209342       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:47.209577573Z E0109 04:43:47.209563       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:47.212732590Z E0109 04:43:47.212706       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:47.607025250Z I0109 04:43:47.606973       1 request.go:601] Waited for 1.595980343s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:43:47.867042028Z I0109 04:43:47.866986       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:47.886043397Z I0109 04:43:47.886005       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:48.414703946Z I0109 04:43:48.414659       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-4 -n openshift-etcd because it was missing
2023-01-09T04:43:48.433105633Z I0109 04:43:48.433069       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:48.806460246Z I0109 04:43:48.806424       1 request.go:601] Waited for 1.593350453s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:49.409242590Z I0109 04:43:49.409205       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:49.409242590Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:49.409242590Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:49.409242590Z  TargetRevision: (int32) 3,
2023-01-09T04:43:49.409242590Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:49.409242590Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:49.409242590Z  LastFailedReason: (string) "",
2023-01-09T04:43:49.409242590Z  LastFailedCount: (int) 0,
2023-01-09T04:43:49.409242590Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:49.409242590Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:49.409242590Z }
2023-01-09T04:43:49.409242590Z  because new revision pending
2023-01-09T04:43:49.806642259Z I0109 04:43:49.806604       1 request.go:601] Waited for 1.391939204s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2023-01-09T04:43:49.815643826Z I0109 04:43:49.815600       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-4 -n openshift-etcd because it was missing
2023-01-09T04:43:49.833791916Z I0109 04:43:49.833755       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:50.211150613Z E0109 04:43:50.211108       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:50.224501661Z E0109 04:43:50.224467       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:43:50.225711723Z I0109 04:43:50.225679       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:50.225790310Z I0109 04:43:50.225766       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:50.236105684Z I0109 04:43:50.235481       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:50.236861587Z I0109 04:43:50.236828       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:50.292886770Z I0109 04:43:50.292850       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 54677504
2023-01-09T04:43:50.853965794Z I0109 04:43:50.853807       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:51.006217587Z I0109 04:43:51.006174       1 request.go:601] Waited for 1.394537951s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:43:51.214148687Z I0109 04:43:51.214093       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-4 -n openshift-etcd because it was missing
2023-01-09T04:43:51.239000887Z I0109 04:43:51.238952       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:43:52.206480091Z I0109 04:43:52.206443       1 request.go:601] Waited for 1.396861826s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2023-01-09T04:43:52.216188970Z I0109 04:43:52.216146       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-3-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:43:52.216514377Z I0109 04:43:52.216479       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.221259473Z E0109 04:43:52.221229       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:52.224915090Z I0109 04:43:52.224702       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.236202915Z I0109 04:43:52.236168       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.613540807Z I0109 04:43:52.613481       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-4 -n openshift-etcd because it was missing
2023-01-09T04:43:52.630122024Z I0109 04:43:52.630058       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 3 created because configmap/etcd-pod has changed
2023-01-09T04:43:52.631609790Z I0109 04:43:52.631572       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:52.641507591Z I0109 04:43:52.641471       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 4 triggered by "configmap/etcd-pod has changed"
2023-01-09T04:43:52.646934918Z W0109 04:43:52.646893       1 staticpod.go:38] revision 4 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:43:52.657364218Z E0109 04:43:52.657330       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 4
2023-01-09T04:43:52.658251608Z I0109 04:43:52.658157       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:52.659083725Z I0109 04:43:52.659056       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nRevisionControllerDegraded: conflicting latestAvailableRevision 4\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:52.668458702Z I0109 04:43:52.668425       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nRevisionControllerDegraded: conflicting latestAvailableRevision 4\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:52.668612484Z I0109 04:43:52.668593       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.674969209Z I0109 04:43:52.674939       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:52.676530271Z I0109 04:43:52.676503       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:52.683931248Z I0109 04:43:52.683896       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nRevisionControllerDegraded: conflicting latestAvailableRevision 4\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:52.686110211Z I0109 04:43:52.686085       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.819286620Z I0109 04:43:52.819247       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:52.874945836Z I0109 04:43:52.874895       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:53.204248507Z I0109 04:43:53.204201       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:53.379630129Z I0109 04:43:53.379579       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:53.406602532Z I0109 04:43:53.406570       1 request.go:601] Waited for 1.190447962s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:53.563277581Z I0109 04:43:53.563225       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:53.610899576Z I0109 04:43:53.610853       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 3, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:43:53.622529090Z I0109 04:43:53.622480       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:53.623025829Z I0109 04:43:53.622963       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:53.630305546Z I0109 04:43:53.630251       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 3\nEtcdMembersAvailable: 1 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available"
2023-01-09T04:43:53.632514539Z I0109 04:43:53.632485       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:53.823219778Z I0109 04:43:53.823181       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:54.406628779Z I0109 04:43:54.406594       1 request.go:601] Waited for 1.59628881s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:55.390834709Z I0109 04:43:55.390799       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:55.416508304Z E0109 04:43:55.416471       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:55.416721073Z E0109 04:43:55.416707       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:43:55.457619701Z I0109 04:43:55.457566       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:55.606152528Z I0109 04:43:55.606114       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:55.606724115Z I0109 04:43:55.606286       1 request.go:601] Waited for 1.594418992s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:43:55.620531547Z I0109 04:43:55.620501       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:56.606491542Z I0109 04:43:56.606452       1 request.go:601] Waited for 1.595968922s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:43:56.827318880Z I0109 04:43:56.827282       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:56.827318880Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:56.827318880Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:56.827318880Z  TargetRevision: (int32) 4,
2023-01-09T04:43:56.827318880Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:56.827318880Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:56.827318880Z  LastFailedReason: (string) "",
2023-01-09T04:43:56.827318880Z  LastFailedCount: (int) 0,
2023-01-09T04:43:56.827318880Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:56.827318880Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:56.827318880Z }
2023-01-09T04:43:56.827318880Z  because new revision pending
2023-01-09T04:43:56.828936024Z I0109 04:43:56.828900       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:56.870686092Z I0109 04:43:56.870646       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:56.949511439Z I0109 04:43:56.949476       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 55619584
2023-01-09T04:43:56.949511439Z I0109 04:43:56.949494       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.06 %, dbSize: 56213504
2023-01-09T04:43:57.806964840Z I0109 04:43:57.806918       1 request.go:601] Waited for 1.395594973s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:43:57.883080456Z I0109 04:43:57.883039       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:58.609493019Z E0109 04:43:58.609447       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:43:58.609493019Z E0109 04:43:58.609481       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:58.620127138Z E0109 04:43:58.620095       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:43:58.620852263Z I0109 04:43:58.620824       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:43:58.621891670Z I0109 04:43:58.621856       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:43:58.623303104Z E0109 04:43:58.623273       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:43:58.629355414Z I0109 04:43:58.629322       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:43:58.631885038Z I0109 04:43:58.631859       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:58.687643986Z I0109 04:43:58.687609       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.72 %, dbSize: 56651776
2023-01-09T04:43:58.687643986Z I0109 04:43:58.687625       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.73 %, dbSize: 57245696
2023-01-09T04:43:59.007052274Z I0109 04:43:59.007009       1 request.go:601] Waited for 1.596900391s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:43:59.233707287Z I0109 04:43:59.233668       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:59.378041034Z I0109 04:43:59.377983       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:59.402059092Z I0109 04:43:59.402022       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:43:59.611870933Z I0109 04:43:59.611832       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:43:59.611870933Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:43:59.611870933Z  CurrentRevision: (int32) 0,
2023-01-09T04:43:59.611870933Z  TargetRevision: (int32) 4,
2023-01-09T04:43:59.611870933Z  LastFailedRevision: (int32) 0,
2023-01-09T04:43:59.611870933Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:43:59.611870933Z  LastFailedReason: (string) "",
2023-01-09T04:43:59.611870933Z  LastFailedCount: (int) 0,
2023-01-09T04:43:59.611870933Z  LastFallbackCount: (int) 0,
2023-01-09T04:43:59.611870933Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:43:59.611870933Z }
2023-01-09T04:43:59.611870933Z  because new revision pending
2023-01-09T04:44:00.206879974Z I0109 04:44:00.206836       1 request.go:601] Waited for 1.397377513s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:44:00.219428518Z I0109 04:44:00.219383       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:00.247015700Z I0109 04:44:00.245910       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:01.034499327Z I0109 04:44:01.034457       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:01.209492847Z E0109 04:44:01.209456       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:01.220112242Z E0109 04:44:01.220070       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:44:01.220957518Z I0109 04:44:01.220912       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:01.222236499Z I0109 04:44:01.222204       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:01.230447699Z I0109 04:44:01.230385       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:44:01.230516609Z I0109 04:44:01.230498       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:01.283715062Z I0109 04:44:01.283671       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.17 %, dbSize: 56651776
2023-01-09T04:44:01.283715062Z I0109 04:44:01.283690       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.18 %, dbSize: 57245696
2023-01-09T04:44:01.407058750Z I0109 04:44:01.407016       1 request.go:601] Waited for 1.19496161s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:44:02.218414835Z I0109 04:44:02.218358       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-4-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:44:02.218500939Z I0109 04:44:02.218469       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.228233318Z I0109 04:44:02.228201       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.238569138Z I0109 04:44:02.238535       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.566581840Z I0109 04:44:02.566534       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.569655182Z I0109 04:44:02.569619       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.606662219Z I0109 04:44:02.606610       1 request.go:601] Waited for 1.195183527s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:44:02.633359154Z I0109 04:44:02.633323       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:44:02.846383430Z I0109 04:44:02.846346       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:02.894433987Z I0109 04:44:02.894393       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:03.410268234Z I0109 04:44:03.410231       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:44:03.594886658Z I0109 04:44:03.594847       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:03.609247158Z I0109 04:44:03.609208       1 request.go:601] Waited for 1.39071715s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:44:03.977196282Z I0109 04:44:03.977144       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:04.806397854Z I0109 04:44:04.806355       1 request.go:601] Waited for 1.59482946s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:44:05.411027358Z E0109 04:44:05.410961       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:05.411027358Z E0109 04:44:05.410985       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:05.425138088Z E0109 04:44:05.425103       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:05.425602968Z I0109 04:44:05.425574       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:05.427146493Z I0109 04:44:05.427115       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:05.437664475Z I0109 04:44:05.437625       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:44:05.438847267Z I0109 04:44:05.438808       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:05.806766653Z I0109 04:44:05.806735       1 request.go:601] Waited for 1.59623535s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:06.609935407Z I0109 04:44:06.609880       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:06.768540151Z I0109 04:44:06.768498       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:06.781499999Z I0109 04:44:06.781460       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:06.806822785Z I0109 04:44:06.806786       1 request.go:601] Waited for 1.3756077s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:07.445022174Z I0109 04:44:07.444946       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:07.628358671Z I0109 04:44:07.628318       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:07.645805719Z I0109 04:44:07.645771       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:08.006684455Z I0109 04:44:08.006645       1 request.go:601] Waited for 1.395695487s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:08.210898172Z E0109 04:44:08.210858       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:08.210939448Z E0109 04:44:08.210894       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:08.211231079Z E0109 04:44:08.211214       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:08.240029894Z I0109 04:44:08.239975       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:08.630423918Z I0109 04:44:08.630389       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:09.206528720Z I0109 04:44:09.206482       1 request.go:601] Waited for 1.195523817s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:09.210647109Z I0109 04:44:09.210607       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:11.640831572Z I0109 04:44:11.640787       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:12.947117972Z E0109 04:44:12.947083       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:44:12.981135680Z I0109 04:44:12.981097       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:12.983520485Z I0109 04:44:12.983438       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:12.991909282Z I0109 04:44:12.991868       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 1 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available"
2023-01-09T04:44:12.993516432Z I0109 04:44:12.993483       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:13.185463984Z I0109 04:44:13.185428       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:13.638353034Z I0109 04:44:13.638308       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:14.631466779Z I0109 04:44:14.631429       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:14.871573395Z I0109 04:44:14.871521       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:44:14.873391989Z I0109 04:44:14.872819       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:14.878600262Z I0109 04:44:14.878570       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:15.928673909Z I0109 04:44:15.928477       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:44:15.929230068Z I0109 04:44:15.929201       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:16.428561313Z I0109 04:44:16.428523       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:16.437863427Z I0109 04:44:16.437833       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:16.784721201Z E0109 04:44:16.784683       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:16.784721201Z E0109 04:44:16.784709       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:16.784946754Z E0109 04:44:16.784934       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:17.786567718Z E0109 04:44:17.786526       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:17.786567718Z E0109 04:44:17.786557       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:17.786884217Z E0109 04:44:17.786866       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:18.230466052Z I0109 04:44:18.230427       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:19.666346852Z I0109 04:44:19.666305       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:20.857591329Z I0109 04:44:20.857536       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:24.655229515Z I0109 04:44:24.655188       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:24.662075642Z I0109 04:44:24.662037       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:24.665704980Z I0109 04:44:24.665670       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:24.665704980Z I0109 04:44:24.665682       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:24.670377694Z E0109 04:44:24.670341       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:24.670377694Z E0109 04:44:24.670363       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:24.670573022Z E0109 04:44:24.670557       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:25.258880010Z I0109 04:44:25.258839       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:25.432172802Z I0109 04:44:25.432132       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:25.859385879Z E0109 04:44:25.859352       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:25.859385879Z E0109 04:44:25.859376       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:25.859614359Z E0109 04:44:25.859600       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:27.233026081Z I0109 04:44:27.232956       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:29.031478494Z I0109 04:44:29.031411       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:30.831056956Z I0109 04:44:30.830965       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:32.667466990Z I0109 04:44:32.667425       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:32.999829788Z I0109 04:44:32.999779       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.030088712Z I0109 04:44:38.030052       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.617379421Z I0109 04:44:38.617342       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.619336773Z E0109 04:44:38.619303       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.619399133Z E0109 04:44:38.619388       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.619754674Z E0109 04:44:38.619736       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:38.634478507Z E0109 04:44:38.634447       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:38.634537299Z E0109 04:44:38.634526       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:38.655559107Z I0109 04:44:38.655521       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.667855175Z I0109 04:44:38.662969       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.678052283Z I0109 04:44:38.678012       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.735256419Z I0109 04:44:38.735215       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.900905912Z I0109 04:44:38.900853       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.938136221Z I0109 04:44:38.936918       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:38.981971845Z I0109 04:44:38.981932       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:39.051171112Z I0109 04:44:39.051131       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:39.390674474Z E0109 04:44:39.390635       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:39.830117393Z I0109 04:44:39.830076       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:39.850712979Z I0109 04:44:39.850677       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:40.190504791Z E0109 04:44:40.190465       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:40.190504791Z E0109 04:44:40.190489       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:40.190746953Z E0109 04:44:40.190729       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:40.250771883Z I0109 04:44:40.250713       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:41.390884529Z E0109 04:44:41.390842       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:41.390884529Z E0109 04:44:41.390866       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:41.391148315Z E0109 04:44:41.391131       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:41.629534835Z I0109 04:44:41.629494       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:43.191312682Z E0109 04:44:43.191273       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:43.191312682Z E0109 04:44:43.191300       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:43.191538818Z E0109 04:44:43.191524       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:43.601447546Z E0109 04:44:43.601409       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:43.626751193Z I0109 04:44:43.626714       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:44.590281961Z E0109 04:44:44.590240       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:44.600934761Z E0109 04:44:44.600899       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:44:44.601598759Z I0109 04:44:44.601568       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:44.603507425Z I0109 04:44:44.603475       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:44.611571809Z I0109 04:44:44.611537       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:44:44.612601273Z I0109 04:44:44.612577       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:44.670601121Z I0109 04:44:44.670563       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 60059648
2023-01-09T04:44:44.702695883Z I0109 04:44:44.696104       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:45.787174488Z I0109 04:44:45.787135       1 request.go:601] Waited for 1.183168114s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:45.867479948Z I0109 04:44:45.867419       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:44:45.877139659Z I0109 04:44:45.877104       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:45.883495839Z I0109 04:44:45.883461       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:45.965026348Z I0109 04:44:45.964955       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:46.790962231Z I0109 04:44:46.790917       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:46.987295428Z I0109 04:44:46.987259       1 request.go:601] Waited for 1.140886148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:44:47.030430072Z I0109 04:44:47.030389       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:47.030755600Z I0109 04:44:47.030727       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:47.038241328Z I0109 04:44:47.038205       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from False to True ("GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]")
2023-01-09T04:44:47.038354605Z I0109 04:44:47.038334       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:47.990255976Z E0109 04:44:47.990218       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:47.990255976Z E0109 04:44:47.990241       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:48.000319639Z E0109 04:44:48.000298       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:48.005689838Z I0109 04:44:48.005655       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:48.005689838Z I0109 04:44:48.005087       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:48.017531865Z I0109 04:44:48.017487       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:48.017725741Z I0109 04:44:48.017696       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:48.384569219Z E0109 04:44:48.384532       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:49.187288196Z I0109 04:44:49.187238       1 request.go:601] Waited for 1.180258549s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:44:49.269815484Z I0109 04:44:49.269773       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:49.843136112Z I0109 04:44:49.843094       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:50.187619793Z I0109 04:44:50.187583       1 request.go:601] Waited for 1.396439416s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:44:50.250749670Z I0109 04:44:50.250711       1 tlsconfig.go:178] "Loaded client CA" index=0 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"admin-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:21 +0000 UTC to 2033-01-06 04:28:21 +0000 UTC (now=2023-01-09 04:44:50.250677554 +0000 UTC))"
2023-01-09T04:44:50.250788660Z I0109 04:44:50.250757       1 tlsconfig.go:178] "Loaded client CA" index=1 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.250737476 +0000 UTC))"
2023-01-09T04:44:50.250788660Z I0109 04:44:50.250785       1 tlsconfig.go:178] "Loaded client CA" index=2 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-control-plane-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.250766806 +0000 UTC))"
2023-01-09T04:44:50.250837516Z I0109 04:44:50.250812       1 tlsconfig.go:178] "Loaded client CA" index=3 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-apiserver-to-kubelet-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:24 +0000 UTC to 2024-01-09 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.250794565 +0000 UTC))"
2023-01-09T04:44:50.250910048Z I0109 04:44:50.250899       1 tlsconfig.go:178] "Loaded client CA" index=4 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kubelet-bootstrap-kubeconfig-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2033-01-06 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.25087975 +0000 UTC))"
2023-01-09T04:44:50.250964406Z I0109 04:44:50.250954       1 tlsconfig.go:178] "Loaded client CA" index=5 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"kube-csr-signer_@1673239265\" [] issuer=\"kubelet-signer\" (2023-01-09 04:41:04 +0000 UTC to 2023-01-10 04:28:24 +0000 UTC (now=2023-01-09 04:44:50.250933031 +0000 UTC))"
2023-01-09T04:44:50.251035955Z I0109 04:44:50.251024       1 tlsconfig.go:178] "Loaded client CA" index=6 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"openshift-kube-apiserver-operator_node-system-admin-signer@1673239266\" [] issuer=\"<self>\" (2023-01-09 04:41:06 +0000 UTC to 2024-01-09 04:41:07 +0000 UTC (now=2023-01-09 04:44:50.250984201 +0000 UTC))"
2023-01-09T04:44:50.251093613Z I0109 04:44:50.251083       1 tlsconfig.go:178] "Loaded client CA" index=7 certName="client-ca::kube-system::extension-apiserver-authentication::client-ca-file,client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file" certDetail="\"aggregator-signer\" [] issuer=\"<self>\" (2023-01-09 04:28:22 +0000 UTC to 2023-01-10 04:28:22 +0000 UTC (now=2023-01-09 04:44:50.251062642 +0000 UTC))"
2023-01-09T04:44:50.251304806Z I0109 04:44:50.251293       1 tlsconfig.go:200] "Loaded serving cert" certName="serving-cert::/var/run/secrets/serving-cert/tls.crt::/var/run/secrets/serving-cert/tls.key" certDetail="\"metrics.openshift-etcd-operator.svc\" [serving] validServingFor=[metrics.openshift-etcd-operator.svc,metrics.openshift-etcd-operator.svc.cluster.local] issuer=\"openshift-service-serving-signer@1673239264\" (2023-01-09 04:41:09 +0000 UTC to 2025-01-08 04:41:10 +0000 UTC (now=2023-01-09 04:44:50.251267957 +0000 UTC))"
2023-01-09T04:44:50.251474767Z I0109 04:44:50.251461       1 named_certificates.go:53] "Loaded SNI cert" index=0 certName="self-signed loopback" certDetail="\"apiserver-loopback-client@1673239332\" [serving] validServingFor=[apiserver-loopback-client] issuer=\"apiserver-loopback-client-ca@1673239332\" (2023-01-09 03:42:12 +0000 UTC to 2024-01-09 03:42:12 +0000 UTC (now=2023-01-09 04:44:50.251439706 +0000 UTC))"
2023-01-09T04:44:50.391149350Z I0109 04:44:50.391109       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:50.636131462Z I0109 04:44:50.636094       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:50.790272955Z E0109 04:44:50.790236       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:50.800120342Z E0109 04:44:50.800093       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]
2023-01-09T04:44:50.800788666Z I0109 04:44:50.800756       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:50.802276402Z I0109 04:44:50.802243       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:50.810223688Z I0109 04:44:50.810190       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]"
2023-01-09T04:44:50.813386776Z I0109 04:44:50.813361       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:51.090880494Z I0109 04:44:51.090842       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:51.987282997Z I0109 04:44:51.987239       1 request.go:601] Waited for 1.184530513s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:52.997297235Z I0109 04:44:52.997256       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:44:53.419722033Z I0109 04:44:53.419677       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:53.669910782Z I0109 04:44:53.669872       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:56.472171114Z I0109 04:44:56.472129       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:57.952477062Z E0109 04:44:57.952386       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:44:57.952550380Z E0109 04:44:57.952531       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:44:58.008691679Z I0109 04:44:58.008657       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.042448864Z E0109 04:44:58.040304       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:44:58.045439597Z I0109 04:44:58.045409       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:44:58.049668857Z I0109 04:44:58.049635       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:44:58.079346969Z I0109 04:44:58.078453       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.080499490Z I0109 04:44:58.079975       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-145-4.us-east-2.compute.internal, Missing operand on node ip-10-0-160-211.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:44:58.193817126Z I0109 04:44:58.193781       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.243920894Z I0109 04:44:58.243869       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.300138434Z I0109 04:44:58.300099       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.310492540Z I0109 04:44:58.310461       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.343035213Z I0109 04:44:58.340543       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.721294984Z I0109 04:44:58.721258       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.825835943Z I0109 04:44:58.825795       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:58.925550403Z I0109 04:44:58.925495       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:44:59.145943194Z I0109 04:44:59.145906       1 request.go:601] Waited for 1.082279602s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:44:59.605985384Z I0109 04:44:59.605935       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:00.148729423Z I0109 04:45:00.148685       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:00.290019750Z I0109 04:45:00.289954       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:00.345860216Z I0109 04:45:00.345816       1 request.go:601] Waited for 1.195113805s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:45:00.412327921Z I0109 04:45:00.410487       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:00.549259380Z E0109 04:45:00.549221       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:00.549259380Z E0109 04:45:00.549247       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:00.549502315Z E0109 04:45:00.549483       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:00.810722446Z I0109 04:45:00.810682       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:01.210623923Z I0109 04:45:01.210587       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:01.748160323Z I0109 04:45:01.748115       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:01.782854397Z I0109 04:45:01.782808       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:02.007254177Z I0109 04:45:02.006388       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:02.149069164Z E0109 04:45:02.149025       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:02.149069164Z E0109 04:45:02.149051       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:02.149314178Z E0109 04:45:02.149293       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:02.281657686Z I0109 04:45:02.276614       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:03.150021502Z E0109 04:45:03.149978       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:03.150063771Z E0109 04:45:03.150031       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:03.150315175Z E0109 04:45:03.150273       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:03.153725181Z E0109 04:45:03.153703       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:03.153752059Z E0109 04:45:03.153722       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:04.148212466Z I0109 04:45:04.148174       1 clustermemberremovalcontroller.go:135] Ignoring scale-down since the number of etcd members (2) < desired number of control-plane replicas (3) 
2023-01-09T04:45:04.350060331Z E0109 04:45:04.350013       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:05.156186790Z I0109 04:45:05.156147       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:05.209191038Z I0109 04:45:05.209159       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:05.225889128Z I0109 04:45:05.225223       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:05.549169669Z E0109 04:45:05.549134       1 guard_controller.go:250] Missing operand on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:05.549169669Z E0109 04:45:05.549156       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:05.549391145Z E0109 04:45:05.549378       1 base_controller.go:272] GuardController reconciliation failed: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:05.783294439Z I0109 04:45:05.783257       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:05.825521739Z I0109 04:45:05.825482       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:06.341949199Z I0109 04:45:06.341911       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:06.345746418Z I0109 04:45:06.345711       1 request.go:601] Waited for 1.188844765s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:06.386112170Z I0109 04:45:06.386078       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:07.345951102Z I0109 04:45:07.345914       1 request.go:601] Waited for 1.1954211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:07.533088073Z I0109 04:45:07.533048       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:07.550775757Z I0109 04:45:07.550739       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:07.687628190Z I0109 04:45:07.687583       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:07.803189061Z I0109 04:45:07.803138       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:07.849848994Z I0109 04:45:07.849805       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:07.949652900Z E0109 04:45:07.949617       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:07.949652900Z E0109 04:45:07.949641       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:07.960126701Z E0109 04:45:07.960092       1 base_controller.go:272] GuardController reconciliation failed: [Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]
2023-01-09T04:45:07.961611751Z I0109 04:45:07.961576       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:07.962295102Z I0109 04:45:07.962271       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:07.973510098Z I0109 04:45:07.973467       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing operand on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]"
2023-01-09T04:45:07.973602310Z I0109 04:45:07.973574       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.029413280Z I0109 04:45:08.029371       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:08.107491585Z I0109 04:45:08.107451       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.545286044Z I0109 04:45:08.545252       1 request.go:601] Waited for 1.194083236s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:08.703866512Z I0109 04:45:08.703830       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.808061941Z I0109 04:45:08.807982       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.815265742Z I0109 04:45:08.815228       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.831549890Z I0109 04:45:08.831494       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:08.862152101Z I0109 04:45:08.862114       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:09.107026811Z I0109 04:45:09.106977       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:09.546103762Z I0109 04:45:09.546068       1 request.go:601] Waited for 1.581798628s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:09.815185614Z I0109 04:45:09.815142       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:09.859194246Z I0109 04:45:09.859160       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:09.959589943Z I0109 04:45:09.959506       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:09.993566274Z I0109 04:45:09.993523       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:10.746101740Z I0109 04:45:10.746061       1 request.go:601] Waited for 1.395348424s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:45:10.813665697Z I0109 04:45:10.813619       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:10.858256538Z I0109 04:45:10.858219       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-160-211.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:45:11.746172270Z I0109 04:45:11.746130       1 request.go:601] Waited for 1.397192695s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:11.750896174Z I0109 04:45:11.750869       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:45:11.824310670Z I0109 04:45:11.824243       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:11.876714125Z I0109 04:45:11.876660       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberAddAsLearner' successfully added new member https://10.0.160.211:2380
2023-01-09T04:45:11.899980262Z I0109 04:45:11.899944       1 clustermembercontroller.go:283] Not ready for promotion: etcd learner member (https://10.0.160.211:2380) is not yet in sync with leader's log 
2023-01-09T04:45:12.943368716Z E0109 04:45:12.943329       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:45:12.945272446Z I0109 04:45:12.945239       1 request.go:601] Waited for 1.394775409s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:12.996034445Z I0109 04:45:12.995955       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:12.996353258Z I0109 04:45:12.996047       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:13.006455310Z I0109 04:45:13.004015       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 2 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:45:13.006963638Z I0109 04:45:13.006923       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:13.118890561Z I0109 04:45:13.118853       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.04 %, dbSize: 66211840
2023-01-09T04:45:13.123952488Z I0109 04:45:13.123918       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberPromote' successfully promoted learner member https://10.0.160.211:2380
2023-01-09T04:45:13.639007052Z I0109 04:45:13.638950       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.145811260Z I0109 04:45:14.145770       1 request.go:601] Waited for 1.150279894s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:45:14.269657709Z I0109 04:45:14.269606       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.451256803Z W0109 04:45:14.451211       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:45:14.451838411Z I0109 04:45:14.451809       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2023-01-09T04:45:14.451838411Z cause by changes in data.4cfac3d64d9ed566
2023-01-09T04:45:14.453760119Z W0109 04:45:14.453636       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:45:14.453892620Z W0109 04:45:14.453838       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:45:14.458136476Z I0109 04:45:14.458106       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 5 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:45:14.458271696Z W0109 04:45:14.458248       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:45:14.468971967Z W0109 04:45:14.468885       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:45:14.528639682Z I0109 04:45:14.528598       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.772482660Z I0109 04:45:14.772445       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.810429210Z I0109 04:45:14.805963       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.929752828Z I0109 04:45:14.929701       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.951949878Z E0109 04:45:14.951913       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:14.952778500Z I0109 04:45:14.952750       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:45:14.952974254Z I0109 04:45:14.952960       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.961073180Z I0109 04:45:14.960709       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.962132259Z E0109 04:45:14.962104       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:14.965239715Z I0109 04:45:14.965210       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:14.966960653Z I0109 04:45:14.966928       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:14.972047981Z I0109 04:45:14.971983       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:14.976347200Z I0109 04:45:14.974624       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: [Missing PodIP in operand etcd-ip-10-0-160-211.us-east-2.compute.internal on node ip-10-0-160-211.us-east-2.compute.internal, Missing operand on node ip-10-0-145-4.us-east-2.compute.internal]" to "GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:45:14.976347200Z I0109 04:45:14.975945       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:15.146028700Z I0109 04:45:15.145957       1 request.go:601] Waited for 1.596660616s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:45:15.546207163Z I0109 04:45:15.546165       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:16.149806455Z I0109 04:45:16.149746       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-5 -n openshift-etcd because it was missing
2023-01-09T04:45:16.345938799Z I0109 04:45:16.345900       1 request.go:601] Waited for 1.79512796s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:45:16.548653731Z I0109 04:45:16.548617       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:45:16.834187886Z I0109 04:45:16.834135       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:16.860766915Z I0109 04:45:16.860725       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:17.093685958Z I0109 04:45:17.093649       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:17.546248228Z I0109 04:45:17.546212       1 request.go:601] Waited for 1.794427044s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:45:17.952769104Z I0109 04:45:17.952715       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-5 -n openshift-etcd because it was missing
2023-01-09T04:45:18.745732985Z I0109 04:45:18.745694       1 request.go:601] Waited for 1.796880629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:45:19.552086654Z I0109 04:45:19.552038       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-5 -n openshift-etcd because it was missing
2023-01-09T04:45:19.695637549Z I0109 04:45:19.695595       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:19.745791981Z I0109 04:45:19.745752       1 request.go:601] Waited for 1.596717455s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:45:19.818814864Z I0109 04:45:19.818760       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2023-01-09T04:45:19.818814864Z cause by changes in data.etcd.env
2023-01-09T04:45:20.351877128Z I0109 04:45:20.351822       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2023-01-09T04:45:20.351877128Z cause by changes in data.pod.yaml
2023-01-09T04:45:20.746171925Z I0109 04:45:20.746136       1 request.go:601] Waited for 1.39640967s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:45:20.788043693Z I0109 04:45:20.787964       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:20.814856905Z I0109 04:45:20.814807       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:20.951236162Z I0109 04:45:20.951192       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-5 -n openshift-etcd because it was missing
2023-01-09T04:45:21.349616077Z I0109 04:45:21.349578       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:45:21.483253566Z I0109 04:45:21.483205       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:21.516030305Z I0109 04:45:21.515976       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:21.527682578Z I0109 04:45:21.527514       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:21.549229956Z E0109 04:45:21.549196       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:21.549443175Z E0109 04:45:21.549429       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:21.945540721Z I0109 04:45:21.945500       1 request.go:601] Waited for 1.396348507s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:22.353974210Z I0109 04:45:22.353922       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-5 -n openshift-etcd because it was missing
2023-01-09T04:45:22.946268428Z I0109 04:45:22.946229       1 request.go:601] Waited for 1.197064921s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:45:22.953520684Z I0109 04:45:22.953472       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2023-01-09T04:45:22.953520684Z cause by changes in data.pod.yaml
2023-01-09T04:45:23.096217046Z I0109 04:45:23.096174       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:23.151304737Z I0109 04:45:23.151255       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-5 -n openshift-etcd because it was missing
2023-01-09T04:45:23.950510919Z I0109 04:45:23.950456       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-5 -n openshift-etcd because it was missing
2023-01-09T04:45:23.973898517Z I0109 04:45:23.973861       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:24.134595799Z I0109 04:45:24.134554       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:24.148824985Z I0109 04:45:24.148781       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 4, but has not made progress because static pod is pending
2023-01-09T04:45:24.296702252Z I0109 04:45:24.296661       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:24.374685771Z I0109 04:45:24.374625       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:24.952651605Z I0109 04:45:24.952592       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-5 -n openshift-etcd because it was missing
2023-01-09T04:45:24.963797633Z I0109 04:45:24.963746       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 4 created because configmap/etcd-endpoints has changed
2023-01-09T04:45:24.964121450Z I0109 04:45:24.964093       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:24.971004855Z I0109 04:45:24.970948       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 6 triggered by "configmap/etcd-pod has changed"
2023-01-09T04:45:25.057388959Z I0109 04:45:25.057349       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 67993600
2023-01-09T04:45:25.349212227Z E0109 04:45:25.349177       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:25.349426943Z E0109 04:45:25.349411       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:25.498284598Z I0109 04:45:25.498228       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:25.525193076Z I0109 04:45:25.525140       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.149438891Z I0109 04:45:26.149392       1 request.go:601] Waited for 1.183231931s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-4-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:26.157491250Z I0109 04:45:26.157452       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:45:26.157491250Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:45:26.157491250Z  CurrentRevision: (int32) 0,
2023-01-09T04:45:26.157491250Z  TargetRevision: (int32) 5,
2023-01-09T04:45:26.157491250Z  LastFailedRevision: (int32) 0,
2023-01-09T04:45:26.157491250Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:45:26.157491250Z  LastFailedReason: (string) "",
2023-01-09T04:45:26.157491250Z  LastFailedCount: (int) 0,
2023-01-09T04:45:26.157491250Z  LastFallbackCount: (int) 0,
2023-01-09T04:45:26.157491250Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:45:26.157491250Z }
2023-01-09T04:45:26.157491250Z  because new revision pending
2023-01-09T04:45:26.177489349Z I0109 04:45:26.177447       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:26.180362608Z I0109 04:45:26.180169       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:26.196553853Z I0109 04:45:26.196515       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.198851961Z I0109 04:45:26.197285       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 4\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:45:26.203879169Z I0109 04:45:26.203832       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.246946908Z I0109 04:45:26.246902       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.325620151Z I0109 04:45:26.325513       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.355211449Z I0109 04:45:26.355164       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:26.552882276Z I0109 04:45:26.552809       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-6 -n openshift-etcd because it was missing
2023-01-09T04:45:26.992159769Z I0109 04:45:26.992118       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:27.345849161Z I0109 04:45:27.345813       1 request.go:601] Waited for 1.797051964s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:45:27.803542494Z I0109 04:45:27.803503       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:28.346162036Z I0109 04:45:28.346121       1 request.go:601] Waited for 1.793488851s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2023-01-09T04:45:28.355138942Z I0109 04:45:28.352844       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-6 -n openshift-etcd because it was missing
2023-01-09T04:45:29.545514222Z I0109 04:45:29.545468       1 request.go:601] Waited for 1.791985371s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:45:29.753525343Z I0109 04:45:29.753473       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-5-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:45:29.754801543Z I0109 04:45:29.754727       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:29.766106598Z I0109 04:45:29.766061       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:29.778963612Z I0109 04:45:29.778925       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:30.151961650Z I0109 04:45:30.151918       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-6 -n openshift-etcd because it was missing
2023-01-09T04:45:30.413217776Z I0109 04:45:30.413166       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:30.745894099Z I0109 04:45:30.745845       1 request.go:601] Waited for 1.594965926s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:45:30.900209463Z I0109 04:45:30.900152       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:30.979258875Z I0109 04:45:30.978931       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:31.007221421Z I0109 04:45:31.007185       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:31.037732554Z I0109 04:45:31.037682       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:31.361694090Z I0109 04:45:31.361628       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:31.745984359Z I0109 04:45:31.745947       1 request.go:601] Waited for 1.797111663s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:31.954847555Z I0109 04:45:31.954791       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-6 -n openshift-etcd because it was missing
2023-01-09T04:45:32.294646962Z I0109 04:45:32.294612       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:32.348790864Z E0109 04:45:32.348753       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:32.349029513Z E0109 04:45:32.349015       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:32.746217863Z I0109 04:45:32.746174       1 request.go:601] Waited for 1.795655855s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:45:33.530771234Z I0109 04:45:33.529235       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:33.751307540Z I0109 04:45:33.751249       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-6 -n openshift-etcd because it was missing
2023-01-09T04:45:33.946099642Z I0109 04:45:33.946062       1 request.go:601] Waited for 1.796596446s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:45:34.176443654Z I0109 04:45:34.176383       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:34.264219118Z I0109 04:45:34.264179       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:34.300074073Z I0109 04:45:34.300010       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:34.865188710Z I0109 04:45:34.863726       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:34.945107500Z I0109 04:45:34.945073       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:34.950130387Z I0109 04:45:34.950098       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 5, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:35.145475957Z I0109 04:45:35.145439       1 request.go:601] Waited for 1.596005454s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:35.356508296Z I0109 04:45:35.356453       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-6 -n openshift-etcd because it was missing
2023-01-09T04:45:35.748597637Z E0109 04:45:35.748562       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:36.145609420Z I0109 04:45:36.145576       1 request.go:601] Waited for 1.595870608s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:45:36.499561908Z I0109 04:45:36.499515       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:36.552585739Z I0109 04:45:36.552527       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-6 -n openshift-etcd because it was missing
2023-01-09T04:45:37.146038385Z I0109 04:45:37.145965       1 request.go:601] Waited for 1.196526171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:45:37.475356379Z I0109 04:45:37.475303       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:37.699049210Z I0109 04:45:37.699010       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:37.722309255Z I0109 04:45:37.722266       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:37.954550491Z I0109 04:45:37.954493       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-6 -n openshift-etcd because it was missing
2023-01-09T04:45:37.967634230Z I0109 04:45:37.967592       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:37.968068079Z I0109 04:45:37.968030       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 5 created because configmap/etcd-pod has changed
2023-01-09T04:45:38.146086656Z I0109 04:45:38.146036       1 request.go:601] Waited for 1.335865845s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:45:38.349201004Z E0109 04:45:38.349161       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:39.253794915Z I0109 04:45:39.253746       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:39.278627797Z I0109 04:45:39.277625       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:39.345510240Z I0109 04:45:39.345470       1 request.go:601] Waited for 1.378102682s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-5-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:40.114735441Z I0109 04:45:40.114692       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:40.345780381Z I0109 04:45:40.345740       1 request.go:601] Waited for 1.393179188s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:45:40.753958495Z I0109 04:45:40.753902       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:45:40.753958495Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:45:40.753958495Z  CurrentRevision: (int32) 0,
2023-01-09T04:45:40.753958495Z  TargetRevision: (int32) 6,
2023-01-09T04:45:40.753958495Z  LastFailedRevision: (int32) 0,
2023-01-09T04:45:40.753958495Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:45:40.753958495Z  LastFailedReason: (string) "",
2023-01-09T04:45:40.753958495Z  LastFailedCount: (int) 0,
2023-01-09T04:45:40.753958495Z  LastFallbackCount: (int) 0,
2023-01-09T04:45:40.753958495Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:45:40.753958495Z }
2023-01-09T04:45:40.753958495Z  because new revision pending
2023-01-09T04:45:40.754447238Z I0109 04:45:40.754410       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:40.769079511Z I0109 04:45:40.769045       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:45:40.769143894Z I0109 04:45:40.769110       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:45:40.777409156Z I0109 04:45:40.777369       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5" to "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 5\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:45:40.777779127Z I0109 04:45:40.777751       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:40.849801783Z I0109 04:45:40.849763       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 75730944
2023-01-09T04:45:40.849801783Z I0109 04:45:40.849781       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 75870208
2023-01-09T04:45:40.849801783Z I0109 04:45:40.849786       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.01 %, dbSize: 76468224
2023-01-09T04:45:41.148514375Z E0109 04:45:41.148474       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:41.545909188Z I0109 04:45:41.545869       1 request.go:601] Waited for 1.39722808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:45:42.107305852Z I0109 04:45:42.107253       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:42.584280568Z I0109 04:45:42.584244       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:42.746003197Z I0109 04:45:42.745952       1 request.go:601] Waited for 1.597091497s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:45:43.553286083Z I0109 04:45:43.553225       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:45:43.553387225Z I0109 04:45:43.553363       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:43.562140280Z I0109 04:45:43.562101       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:43.569619674Z I0109 04:45:43.569584       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:43.746128238Z I0109 04:45:43.746090       1 request.go:601] Waited for 1.19714294s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:45:43.949679765Z E0109 04:45:43.949633       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:44.149731611Z I0109 04:45:44.149691       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:44.749117681Z I0109 04:45:44.749077       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:45:44.926301437Z I0109 04:45:44.926258       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:44.946144097Z I0109 04:45:44.946111       1 request.go:601] Waited for 1.392668599s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:45:45.946164710Z I0109 04:45:45.946125       1 request.go:601] Waited for 1.19612196s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:46.100569868Z I0109 04:45:46.100517       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:47.145863487Z I0109 04:45:47.145819       1 request.go:601] Waited for 1.196348236s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:45:47.148878754Z I0109 04:45:47.148855       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:47.366520086Z I0109 04:45:47.366479       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:48.348983006Z E0109 04:45:48.348944       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:48.349341931Z E0109 04:45:48.349327       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:48.789263774Z I0109 04:45:48.789223       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:48.811034791Z I0109 04:45:48.810982       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:49.149296318Z I0109 04:45:49.149258       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:45:49.499196166Z I0109 04:45:49.499156       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:50.749229185Z E0109 04:45:50.749193       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:50.749435995Z E0109 04:45:50.749423       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:56.942968169Z I0109 04:45:56.942925       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:45:58.858205248Z E0109 04:45:58.858172       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:45:58.858412652Z E0109 04:45:58.858396       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:00.445039749Z E0109 04:46:00.444976       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:00.445409909Z E0109 04:46:00.445374       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:01.982577749Z I0109 04:46:01.982535       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:01.989485066Z I0109 04:46:01.989442       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:02.284407551Z I0109 04:46:02.284355       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:02.294332878Z I0109 04:46:02.294299       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:02.445250195Z E0109 04:46:02.445213       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:02.445473980Z E0109 04:46:02.445459       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:03.844679766Z I0109 04:46:03.844640       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:05.049117724Z I0109 04:46:05.049079       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:05.246491444Z E0109 04:46:05.246447       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:05.246704624Z E0109 04:46:05.246678       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:08.281703146Z I0109 04:46:08.281646       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:10.552951064Z I0109 04:46:10.552910       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:10.567016076Z I0109 04:46:10.566946       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:10.932045573Z I0109 04:46:10.932007       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:11.136756685Z I0109 04:46:11.136707       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:11.753103095Z I0109 04:46:11.753060       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:12.946111651Z E0109 04:46:12.946078       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:46:13.639711361Z I0109 04:46:13.639668       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:15.038112927Z E0109 04:46:15.038058       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:15.206933498Z I0109 04:46:15.201863       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:15.250185217Z I0109 04:46:15.250139       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:15.250427900Z I0109 04:46:15.250410       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:15.530762486Z I0109 04:46:15.530725       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:15.844772622Z E0109 04:46:15.844733       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:17.229605157Z I0109 04:46:17.229565       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:17.445435337Z E0109 04:46:17.445400       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:17.445667249Z E0109 04:46:17.445651       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:17.839165599Z I0109 04:46:17.839130       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:18.813847971Z E0109 04:46:18.813815       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp: lookup thanos-querier.openshift-monitoring.svc on 172.30.0.10:53: no such host
2023-01-09T04:46:19.245523785Z E0109 04:46:19.245489       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:19.245740794Z E0109 04:46:19.245726       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:21.438789222Z I0109 04:46:21.438748       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:23.130632079Z I0109 04:46:23.130552       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:25.414343527Z I0109 04:46:25.414305       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:25.839577046Z I0109 04:46:25.839527       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:25.855955140Z I0109 04:46:25.853233       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:26.036883882Z I0109 04:46:26.036844       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:27.265258417Z I0109 04:46:27.265220       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:27.511809347Z I0109 04:46:27.511773       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:28.513799262Z I0109 04:46:28.513724       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:30.279152315Z E0109 04:46:30.279113       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:30.279760516Z E0109 04:46:30.279725       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:30.533602306Z I0109 04:46:30.533559       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:33.160075096Z I0109 04:46:33.160037       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:34.185034984Z I0109 04:46:34.184957       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:35.075963174Z I0109 04:46:35.075913       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:37.107437482Z I0109 04:46:37.107377       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:37.115189506Z I0109 04:46:37.115152       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:46:37.131156925Z E0109 04:46:37.131121       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:37.131383869Z E0109 04:46:37.131364       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:38.767424597Z E0109 04:46:38.767389       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:38.767654706Z E0109 04:46:38.767635       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:39.123708411Z I0109 04:46:39.123667       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:40.452129989Z I0109 04:46:40.452087       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:40.711424312Z I0109 04:46:40.711379       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2023-01-09T04:46:41.111228483Z E0109 04:46:41.111194       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:41.111435903Z E0109 04:46:41.111423       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:42.820288493Z I0109 04:46:42.820234       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:43.007595541Z I0109 04:46:43.007545       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:43.723588354Z I0109 04:46:43.723543       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:44.245570132Z I0109 04:46:44.245048       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:45.259155861Z I0109 04:46:45.259114       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:45.526576658Z I0109 04:46:45.526529       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:45.607336536Z I0109 04:46:45.607286       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:47.726026242Z I0109 04:46:47.725967       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:48.247377519Z I0109 04:46:48.247329       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:48.351732299Z I0109 04:46:48.351677       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:50.292167830Z I0109 04:46:50.292133       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:50.313397658Z I0109 04:46:50.313364       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2023-01-09T04:46:50.562042246Z I0109 04:46:50.562004       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:51.724096807Z I0109 04:46:51.724045       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:52.149745694Z I0109 04:46:52.149708       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:46:52.161004528Z E0109 04:46:52.160952       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:46:52.162007034Z I0109 04:46:52.161953       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"ClusterMemberController_SyncError::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:46:52.162099538Z I0109 04:46:52.162064       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:46:52.174588076Z I0109 04:46:52.174542       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:46:52.174663489Z I0109 04:46:52.174645       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:52.566479141Z I0109 04:46:52.566443       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because waiting for static pod of revision 6, found 4
2023-01-09T04:46:52.599671943Z I0109 04:46:52.599631       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:52.714002878Z I0109 04:46:52.713937       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:53.768730576Z E0109 04:46:53.768695       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:54.166374869Z E0109 04:46:54.166340       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:54.568316686Z E0109 04:46:54.568275       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:54.965936822Z E0109 04:46:54.965901       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:46:55.053064427Z I0109 04:46:55.052978       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:55.689055323Z I0109 04:46:55.687266       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:55.752651290Z I0109 04:46:55.752609       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:56.057604734Z I0109 04:46:56.057563       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:46:56.109962925Z I0109 04:46:56.109914       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:00.388025881Z E0109 04:47:00.387960       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:00.388227696Z E0109 04:47:00.388209       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:00.657621732Z I0109 04:47:00.657583       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:01.152979475Z I0109 04:47:01.152921       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:01.181888625Z I0109 04:47:01.181834       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:01.966822938Z E0109 04:47:01.966787       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:01.967075373Z E0109 04:47:01.967059       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:06.680076971Z E0109 04:47:06.680045       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:06.680271797Z E0109 04:47:06.680255       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:07.206020766Z I0109 04:47:07.205944       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:07.206164740Z E0109 04:47:07.206148       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:47:07.208141312Z E0109 04:47:07.208111       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:47:11.760885079Z E0109 04:47:11.760847       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:11.761112626Z E0109 04:47:11.761098       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:12.944542623Z E0109 04:47:12.944507       1 base_controller.go:272] FSyncController reconciliation failed: Post "https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query": dial tcp 172.30.74.101:9091: connect: connection refused
2023-01-09T04:47:13.640553303Z I0109 04:47:13.640503       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:13.972810172Z I0109 04:47:13.972755       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:18.880960279Z E0109 04:47:18.880926       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:18.881217600Z E0109 04:47:18.881200       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:19.342053693Z I0109 04:47:19.342013       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:19.352292383Z I0109 04:47:19.352251       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:19.361282316Z I0109 04:47:19.361243       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:19.382161736Z I0109 04:47:19.382123       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:20.314690636Z I0109 04:47:20.314643       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:20.453786012Z I0109 04:47:20.453743       1 request.go:601] Waited for 1.100389013s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:21.061395382Z I0109 04:47:21.061358       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:21.313441829Z I0109 04:47:21.313403       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:21.653089837Z I0109 04:47:21.653059       1 request.go:601] Waited for 1.191563352s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:22.212354672Z I0109 04:47:22.212314       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:22.252721483Z I0109 04:47:22.252677       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:22.252878874Z E0109 04:47:22.252856       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:47:22.254916028Z E0109 04:47:22.254896       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:47:22.320064689Z I0109 04:47:22.316649       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:22.853206292Z I0109 04:47:22.853167       1 request.go:601] Waited for 1.194887695s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:23.344597553Z I0109 04:47:23.344555       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:23.444624295Z I0109 04:47:23.444587       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:23.444624295Z I0109 04:47:23.444607       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:23.457886394Z I0109 04:47:23.457852       1 quorumguardcleanupcontroller.go:134] 1/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:23.458282767Z I0109 04:47:23.457859       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal" to "GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:47:23.853824403Z I0109 04:47:23.853791       1 request.go:601] Waited for 1.396563895s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:23.857623026Z E0109 04:47:23.857592       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:23.857896407Z E0109 04:47:23.857876       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:25.053182255Z I0109 04:47:25.053141       1 request.go:601] Waited for 1.587831019s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:47:25.257320030Z I0109 04:47:25.257281       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:25.316589224Z I0109 04:47:25.316547       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:26.253221865Z I0109 04:47:26.253182       1 request.go:601] Waited for 1.396570331s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:47:27.253567156Z I0109 04:47:27.253531       1 request.go:601] Waited for 1.394673171s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:47:28.658232943Z I0109 04:47:28.658195       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:28.669788838Z I0109 04:47:28.669744       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:28.857234065Z E0109 04:47:28.857196       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:28.857447745Z E0109 04:47:28.857434       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:30.057216792Z I0109 04:47:30.057178       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:47:30.457343391Z E0109 04:47:30.457303       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:30.457570301Z E0109 04:47:30.457547       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:31.364823643Z I0109 04:47:31.364774       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:31.828275843Z I0109 04:47:31.828228       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:32.657052738Z E0109 04:47:32.656982       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:32.657266544Z E0109 04:47:32.657251       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:33.300918521Z I0109 04:47:33.300876       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:33.425467468Z I0109 04:47:33.424801       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:33.820359387Z I0109 04:47:33.819037       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:33.913380920Z I0109 04:47:33.913337       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:34.460428290Z E0109 04:47:34.460395       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:34.460665165Z E0109 04:47:34.460651       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:34.885007982Z I0109 04:47:34.884899       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:35.673649494Z I0109 04:47:35.673597       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:35.884701785Z I0109 04:47:35.879828       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:35.965624555Z I0109 04:47:35.963397       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.033088093Z I0109 04:47:36.033042       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.148909217Z I0109 04:47:36.148649       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.164882591Z I0109 04:47:36.164846       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.217945192Z I0109 04:47:36.217900       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.322816217Z I0109 04:47:36.322777       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.334258599Z I0109 04:47:36.334217       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.441410089Z I0109 04:47:36.441350       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:36.901727907Z I0109 04:47:36.899790       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:37.302551789Z I0109 04:47:37.302507       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:37.457568867Z E0109 04:47:37.457530       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:37.457792125Z E0109 04:47:37.457775       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:38.658147643Z E0109 04:47:38.658107       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:38.658552910Z E0109 04:47:38.658517       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:39.122086848Z I0109 04:47:39.122044       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:39.430895825Z I0109 04:47:39.430840       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:39.461827604Z I0109 04:47:39.460241       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:39.466895145Z I0109 04:47:39.466860       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:39.533457222Z I0109 04:47:39.532304       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:40.653771774Z I0109 04:47:40.653735       1 request.go:601] Waited for 1.182210709s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:47:41.731984157Z I0109 04:47:41.731924       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:41.852783147Z I0109 04:47:41.852747       1 request.go:601] Waited for 1.194906374s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:47:41.931064148Z I0109 04:47:41.931022       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:42.457888454Z I0109 04:47:42.457849       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:42.457888454Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:47:42.457888454Z  CurrentRevision: (int32) 6,
2023-01-09T04:47:42.457888454Z  TargetRevision: (int32) 0,
2023-01-09T04:47:42.457888454Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:42.457888454Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:42.457888454Z  LastFailedReason: (string) "",
2023-01-09T04:47:42.457888454Z  LastFailedCount: (int) 0,
2023-01-09T04:47:42.457888454Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:42.457888454Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:42.457888454Z }
2023-01-09T04:47:42.457888454Z  because static pod is ready
2023-01-09T04:47:42.468347679Z I0109 04:47:42.468296       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 0 to 6 because static pod is ready
2023-01-09T04:47:42.475293853Z I0109 04:47:42.475255       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:42.475560132Z I0109 04:47:42.475402       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:47:42.486742602Z I0109 04:47:42.486696       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 1 nodes are active; 2 nodes are at revision 0; 1 nodes are at revision 2; 0 nodes have achieved new revision 6\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:47:42.486923168Z I0109 04:47:42.486882       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:42.657566694Z E0109 04:47:42.657528       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:42.657831356Z E0109 04:47:42.657815       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:43.239883001Z I0109 04:47:43.239844       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:43.272894229Z I0109 04:47:43.272840       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:43.653667052Z I0109 04:47:43.653624       1 request.go:601] Waited for 1.18000916s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:47:44.653798033Z I0109 04:47:44.653758       1 request.go:601] Waited for 1.393177581s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:47:44.921144885Z I0109 04:47:44.921104       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:45.066597007Z I0109 04:47:45.066552       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:45.853648811Z I0109 04:47:45.853607       1 request.go:601] Waited for 1.195496103s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:46.057799403Z I0109 04:47:46.057747       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:46.057799403Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:47:46.057799403Z  CurrentRevision: (int32) 6,
2023-01-09T04:47:46.057799403Z  TargetRevision: (int32) 0,
2023-01-09T04:47:46.057799403Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:46.057799403Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:46.057799403Z  LastFailedReason: (string) "",
2023-01-09T04:47:46.057799403Z  LastFailedCount: (int) 0,
2023-01-09T04:47:46.057799403Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:46.057799403Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:46.057799403Z }
2023-01-09T04:47:46.057799403Z  because static pod is ready
2023-01-09T04:47:46.299345479Z I0109 04:47:46.299293       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:46.311890333Z I0109 04:47:46.311845       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:46.324131309Z I0109 04:47:46.323825       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:47.053097455Z I0109 04:47:47.053057       1 request.go:601] Waited for 1.135532083s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:47:47.269106474Z I0109 04:47:47.268481       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:47.456908644Z E0109 04:47:47.456868       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:47.457132807Z E0109 04:47:47.457117       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:48.857279087Z I0109 04:47:48.857242       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal static pod not found and needs new revision 6
2023-01-09T04:47:48.857323047Z I0109 04:47:48.857280       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:47:48.857323047Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:47:48.857323047Z  CurrentRevision: (int32) 0,
2023-01-09T04:47:48.857323047Z  TargetRevision: (int32) 6,
2023-01-09T04:47:48.857323047Z  LastFailedRevision: (int32) 0,
2023-01-09T04:47:48.857323047Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:47:48.857323047Z  LastFailedReason: (string) "",
2023-01-09T04:47:48.857323047Z  LastFailedCount: (int) 0,
2023-01-09T04:47:48.857323047Z  LastFallbackCount: (int) 0,
2023-01-09T04:47:48.857323047Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:47:48.857323047Z }
2023-01-09T04:47:48.869975752Z I0109 04:47:48.869926       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 6 because node ip-10-0-145-4.us-east-2.compute.internal static pod not found
2023-01-09T04:47:48.871723596Z I0109 04:47:48.871682       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:47:49.057393554Z E0109 04:47:49.057351       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:50.052975233Z I0109 04:47:50.052934       1 request.go:601] Waited for 1.177511097s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:51.053719538Z I0109 04:47:51.053673       1 request.go:601] Waited for 1.395820859s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:47:51.476341643Z I0109 04:47:51.476293       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:51.476757406Z I0109 04:47:51.476722       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-6-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:47:51.486304182Z I0109 04:47:51.486264       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:51.495706025Z I0109 04:47:51.495663       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:51.658569488Z E0109 04:47:51.658524       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:52.053745585Z I0109 04:47:52.053705       1 request.go:601] Waited for 1.19634129s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:52.066787475Z I0109 04:47:52.066750       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:52.457319329Z I0109 04:47:52.457274       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:47:53.253835995Z I0109 04:47:53.253793       1 request.go:601] Waited for 1.193200165s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:47:53.539678028Z I0109 04:47:53.539637       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:54.302927540Z I0109 04:47:54.302874       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:54.362099414Z I0109 04:47:54.362048       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:54.456894071Z I0109 04:47:54.456853       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:47:55.133404213Z I0109 04:47:55.133362       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:55.159372170Z I0109 04:47:55.159307       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:55.258342241Z E0109 04:47:55.258304       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:55.260564798Z E0109 04:47:55.258702       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:55.452377480Z I0109 04:47:55.452323       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:56.453542487Z I0109 04:47:56.453503       1 request.go:601] Waited for 1.000358559s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:56.934867083Z I0109 04:47:56.933706       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:57.402082419Z I0109 04:47:57.399852       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:57.414908309Z I0109 04:47:57.414869       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:57.456803997Z I0109 04:47:57.456758       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:47:58.320446633Z I0109 04:47:58.320411       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:47:58.659327354Z E0109 04:47:58.659287       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:58.659711358Z E0109 04:47:58.659672       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:47:58.956041413Z I0109 04:47:58.955971       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:00.329869114Z I0109 04:48:00.329827       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:00.456594335Z E0109 04:48:00.456555       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:00.456931414Z E0109 04:48:00.456909       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:01.369231485Z I0109 04:48:01.369160       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:01.732034329Z I0109 04:48:01.731508       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:01.930846305Z I0109 04:48:01.930803       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:02.305158639Z I0109 04:48:02.305116       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:02.326946656Z I0109 04:48:02.326900       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:02.359336162Z I0109 04:48:02.359290       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:02.456813849Z E0109 04:48:02.456775       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:02.457115495Z E0109 04:48:02.457097       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:02.711413715Z I0109 04:48:02.711367       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:03.330650229Z I0109 04:48:03.330609       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:04.256884729Z E0109 04:48:04.256846       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:04.257113379Z E0109 04:48:04.257096       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:04.728218975Z I0109 04:48:04.728179       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:05.057024077Z E0109 04:48:05.056962       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:05.457423087Z E0109 04:48:05.457388       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:06.255897765Z I0109 04:48:06.255853       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:06.277911119Z I0109 04:48:06.277868       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:06.564224009Z I0109 04:48:06.563600       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:07.811509038Z E0109 04:48:07.811472       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:07.821279917Z E0109 04:48:07.821242       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:08.129630590Z I0109 04:48:08.129595       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:09.333837668Z I0109 04:48:09.333236       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:09.398762808Z E0109 04:48:09.398723       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:09.399167313Z E0109 04:48:09.399135       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:10.330337622Z I0109 04:48:10.330295       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:11.199766877Z E0109 04:48:11.199731       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:11.199983663Z E0109 04:48:11.199968       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:11.529932974Z I0109 04:48:11.529890       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:12.220070093Z I0109 04:48:12.219436       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:12.535276878Z I0109 04:48:12.535230       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:12.972236923Z I0109 04:48:12.972200       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:13.000794829Z E0109 04:48:13.000752       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:13.000978359Z E0109 04:48:13.000964       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:13.056141805Z I0109 04:48:13.056105       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 101335040
2023-01-09T04:48:13.056141805Z I0109 04:48:13.056119       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.20 %, dbSize: 101605376
2023-01-09T04:48:13.056141805Z I0109 04:48:13.056124       1 defragcontroller.go:289] etcd member "etcd-bootstrap" backend store fragmented: 0.19 %, dbSize: 101994496
2023-01-09T04:48:13.640587687Z I0109 04:48:13.640547       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:13.730483829Z I0109 04:48:13.730426       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:13.992975054Z I0109 04:48:13.992935       1 request.go:601] Waited for 1.020238013s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:14.731440641Z I0109 04:48:14.730577       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:14.993114949Z I0109 04:48:14.993074       1 request.go:601] Waited for 1.352524911s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:48:15.397469315Z I0109 04:48:15.397429       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:48:15.533307332Z I0109 04:48:15.533262       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:15.993889930Z I0109 04:48:15.993849       1 request.go:601] Waited for 1.196055609s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:48:16.133079406Z I0109 04:48:16.133040       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:16.527304505Z I0109 04:48:16.527249       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:17.333253483Z I0109 04:48:17.333186       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:18.731857249Z I0109 04:48:18.731817       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:19.731883561Z I0109 04:48:19.731843       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:19.998270829Z E0109 04:48:19.998234       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:19.998573012Z E0109 04:48:19.998545       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:20.735276857Z I0109 04:48:20.734577       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:21.011616918Z E0109 04:48:21.011580       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:21.163045525Z I0109 04:48:21.162970       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:21.797198184Z E0109 04:48:21.797161       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:22.562847834Z I0109 04:48:22.562792       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:22.598602764Z I0109 04:48:22.598563       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:22.797350518Z E0109 04:48:22.797316       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:22.797567580Z E0109 04:48:22.797555       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:24.116118710Z I0109 04:48:24.116077       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:24.598147090Z E0109 04:48:24.598106       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:24.598374196Z E0109 04:48:24.598356       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:26.693699539Z E0109 04:48:26.693664       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:26.693941304Z E0109 04:48:26.693926       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:26.919265996Z I0109 04:48:26.919222       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:26.968704167Z I0109 04:48:26.968668       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:27.432040308Z I0109 04:48:27.431965       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:27.567035357Z I0109 04:48:27.565094       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:27.628152869Z I0109 04:48:27.628117       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:28.666212036Z I0109 04:48:28.666173       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:48:29.554225006Z I0109 04:48:29.554180       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:29.609512130Z I0109 04:48:29.609477       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:29.866381208Z E0109 04:48:29.866330       1 guard_controller.go:250] Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:29.866580577Z E0109 04:48:29.866566       1 base_controller.go:272] GuardController reconciliation failed: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:29.870264759Z E0109 04:48:29.870236       1 guard_controller.go:256] Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:30.556342906Z I0109 04:48:30.556300       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:30.613970321Z I0109 04:48:30.613933       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:30.662701962Z I0109 04:48:30.662667       1 request.go:601] Waited for 1.108465951s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:48:30.916516192Z I0109 04:48:30.916474       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:31.266575824Z I0109 04:48:31.266513       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:31.560333498Z I0109 04:48:31.560291       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:31.612679717Z I0109 04:48:31.612641       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:32.579616751Z I0109 04:48:32.579570       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:32.643546058Z I0109 04:48:32.643510       1 clustermembercontroller.go:364] Skipping etcd-ip-10-0-145-4.us-east-2.compute.internal as the etcd container is in incorrect state, isEtcdContainerRunning = false, isEtcdContainerReady = false
2023-01-09T04:48:33.570496482Z I0109 04:48:33.570425       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:33.630315312Z I0109 04:48:33.630269       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberAddAsLearner' successfully added new member https://10.0.145.4:2380
2023-01-09T04:48:33.641821798Z I0109 04:48:33.641789       1 clustermembercontroller.go:283] Not ready for promotion: etcd learner member (https://10.0.145.4:2380) is not yet in sync with leader's log 
2023-01-09T04:48:33.676375904Z E0109 04:48:33.676329       1 base_controller.go:272] GuardController reconciliation failed: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:33.677578696Z I0109 04:48:33.677544       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:33.678825971Z I0109 04:48:33.678795       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:33.687416535Z I0109 04:48:33.687365       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing operand on node ip-10-0-145-4.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:48:33.691536905Z I0109 04:48:33.691505       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:33.739553177Z I0109 04:48:33.736408       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:33.742055444Z I0109 04:48:33.742011       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.742055444Z I0109 04:48:33.742037       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.752537625Z E0109 04:48:33.752506       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:33.756067376Z I0109 04:48:33.756032       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.756067376Z I0109 04:48:33.756056       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.757904053Z E0109 04:48:33.757693       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:33.759216636Z I0109 04:48:33.759138       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:33.759577095Z I0109 04:48:33.759530       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"ClusterMemberController_SyncError::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:33.768502643Z I0109 04:48:33.768465       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:48:33.770044915Z I0109 04:48:33.769980       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:33.810115316Z I0109 04:48:33.810014       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.810168607Z I0109 04:48:33.810100       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.810420187Z E0109 04:48:33.810390       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:33.815783853Z E0109 04:48:33.815756       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:33.820842457Z I0109 04:48:33.820789       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.820842457Z I0109 04:48:33.820815       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.867672019Z I0109 04:48:33.867626       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.867672019Z I0109 04:48:33.867654       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.867876811Z E0109 04:48:33.867842       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:33.869533705Z E0109 04:48:33.869504       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:33.922714838Z I0109 04:48:33.922667       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.922714838Z I0109 04:48:33.922697       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.922871226Z E0109 04:48:33.922853       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:33.924527737Z E0109 04:48:33.924498       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:33.975767453Z I0109 04:48:33.975721       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.975767453Z I0109 04:48:33.975748       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:33.975920369Z E0109 04:48:33.975902       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:33.980201470Z E0109 04:48:33.980173       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:34.021887717Z I0109 04:48:34.021835       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnstartedEtcdMember' unstarted members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:34.021887717Z I0109 04:48:34.021868       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: NAME-PENDING-10.0.145.4
2023-01-09T04:48:34.022124885Z E0109 04:48:34.022104       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:48:34.027798864Z E0109 04:48:34.027775       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 3 of 4 members are available, NAME-PENDING-10.0.145.4 has not started
2023-01-09T04:48:34.070024491Z I0109 04:48:34.069975       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:34.862361423Z I0109 04:48:34.862323       1 request.go:601] Waited for 1.184117972s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:48:35.205238667Z I0109 04:48:35.205197       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberPromote' successfully promoted learner member https://10.0.145.4:2380
2023-01-09T04:48:35.217524982Z I0109 04:48:35.217492       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:35.217674722Z I0109 04:48:35.217646       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:48:35.227649499Z I0109 04:48:35.226663       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "ClusterMemberControllerDegraded: unhealthy members found during reconciling members\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:48:35.230229595Z I0109 04:48:35.230194       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:35.235438319Z W0109 04:48:35.235402       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:48:35.235745874Z I0109 04:48:35.235711       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2023-01-09T04:48:35.235745874Z cause by changes in data.a9212f5cca23387a
2023-01-09T04:48:35.236756891Z W0109 04:48:35.236704       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:48:35.244127267Z I0109 04:48:35.244093       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:35.297501777Z W0109 04:48:35.297450       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:48:35.305241231Z W0109 04:48:35.305207       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:48:35.309157490Z I0109 04:48:35.309118       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DelayingBootstrapTeardown' cluster-bootstrap is not yet finished - ConfigMap 'kube-system/bootstrap' not found
2023-01-09T04:48:35.310893347Z W0109 04:48:35.310815       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]]
2023-01-09T04:48:35.311267869Z I0109 04:48:35.311232       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:48:35.406431067Z I0109 04:48:35.406387       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DelayingBootstrapTeardown' cluster-bootstrap is not yet finished - ConfigMap 'kube-system/bootstrap' not found
2023-01-09T04:48:35.914927942Z I0109 04:48:35.914885       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:36.032350982Z E0109 04:48:36.032232       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e330b98d", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"RevisionTriggered", Message:"new revision 7 triggered by \"configmap/etcd-endpoints has changed\"", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-revisioncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:36.062454448Z I0109 04:48:36.062401       1 request.go:601] Waited for 1.394978929s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:48:36.064326049Z E0109 04:48:36.064297       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.231794808Z E0109 04:48:36.231689       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e70c51c5", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"DelayingBootstrapTeardown", Message:"cluster-bootstrap is not yet finished - ConfigMap 'kube-system/bootstrap' not found", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-bootstrap-teardown-controller-bootstrapteardowncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:36.432122187Z E0109 04:48:36.432023       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e0dffe9a", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"MemberPromote", Message:"successfully promoted learner member https://10.0.145.4:2380", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-etcd-client", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:36.532629887Z E0109 04:48:36.532594       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.532803526Z I0109 04:48:36.532772       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.632084837Z E0109 04:48:36.631941       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa82fffb991", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"ScriptControllerErrorUpdatingStatus", Message:"Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-script-controller-scriptcontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 532566417, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 532566417, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:36.662953573Z I0109 04:48:36.662908       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.664070507Z I0109 04:48:36.664035       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.664967577Z E0109 04:48:36.664942       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:36.669389474Z I0109 04:48:36.669357       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:36.863282471Z E0109 04:48:36.863233       1 guard_controller.go:304] Unable to apply pod etcd-guard-ip-10-0-199-219.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.262049499Z I0109 04:48:37.262008       1 request.go:601] Waited for 1.595245557s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:37.264118304Z E0109 04:48:37.264096       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:37.465886611Z E0109 04:48:37.465847       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:37.863748857Z E0109 04:48:37.863706       1 base_controller.go:272] EtcdStaticResources reconciliation failed: ["etcd/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:38.064757848Z E0109 04:48:38.064721       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.064839353Z I0109 04:48:38.064812       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.262523256Z I0109 04:48:38.262484       1 request.go:601] Waited for 1.593105659s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2023-01-09T04:48:38.263301238Z I0109 04:48:38.263273       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.264435514Z I0109 04:48:38.264402       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.265338714Z E0109 04:48:38.265318       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.269721442Z I0109 04:48:38.269690       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:38.621092500Z E0109 04:48:38.620973       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e0dffe9a", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"MemberPromote", Message:"successfully promoted learner member https://10.0.145.4:2380", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-etcd-client", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:38.863259139Z I0109 04:48:38.863199       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:38.863872314Z E0109 04:48:38.863780       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa8bae89e27", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 6 count 0 on node \"ip-10-0-145-4.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 863085095, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 863085095, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:38.864184833Z E0109 04:48:38.864166       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.065615935Z E0109 04:48:39.065577       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:39.263010289Z I0109 04:48:39.262950       1 request.go:601] Waited for 1.59987888s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:48:39.264689948Z E0109 04:48:39.264669       1 base_controller.go:270] "TargetConfigController" controller failed to sync "", err: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.664609242Z E0109 04:48:39.664569       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.664668955Z I0109 04:48:39.664642       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.862881208Z I0109 04:48:39.862838       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.863856187Z I0109 04:48:39.863832       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.864726246Z E0109 04:48:39.864710       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:39.869235001Z I0109 04:48:39.869207       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:40.064215670Z W0109 04:48:40.064174       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.064245063Z E0109 04:48:40.064214       1 base_controller.go:272] GuardController reconciliation failed: [Unable to apply pod etcd-guard-ip-10-0-199-219.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused, Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused, Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:40.064941157Z E0109 04:48:40.064919       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.065862224Z W0109 04:48:40.065840       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.065873326Z E0109 04:48:40.065864       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.263874761Z E0109 04:48:40.263839       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.462828664Z I0109 04:48:40.462789       1 request.go:601] Waited for 1.5978009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:40.463568896Z I0109 04:48:40.463543       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.464795196Z E0109 04:48:40.464778       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:40.628768046Z E0109 04:48:40.628665       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e70c51c5", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"DelayingBootstrapTeardown", Message:"cluster-bootstrap is not yet finished - ConfigMap 'kube-system/bootstrap' not found", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-bootstrap-teardown-controller-bootstrapteardowncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:40.665099090Z E0109 04:48:40.665059       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:41.263822430Z E0109 04:48:41.263786       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.263891493Z I0109 04:48:41.263865       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.462874003Z I0109 04:48:41.462793       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.463783316Z I0109 04:48:41.463754       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.464626239Z E0109 04:48:41.464602       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.469542235Z I0109 04:48:41.469502       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:41.662161230Z I0109 04:48:41.662121       1 request.go:601] Waited for 1.3928997s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:48:41.863194336Z I0109 04:48:41.863142       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.864122725Z E0109 04:48:41.864099       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:41.897808508Z E0109 04:48:41.897710       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa82fffb991", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"ScriptControllerErrorUpdatingStatus", Message:"Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-script-controller-scriptcontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 532566417, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 532566417, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:42.065172033Z E0109 04:48:42.065131       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:42.263819589Z E0109 04:48:42.263779       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.464073325Z E0109 04:48:42.464031       1 base_controller.go:272] EtcdStaticResources reconciliation failed: ["etcd/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:42.662294811Z I0109 04:48:42.662259       1 request.go:601] Waited for 1.398174978s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:48:42.663938213Z E0109 04:48:42.663917       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.664028725Z I0109 04:48:42.664004       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.863303481Z I0109 04:48:42.863249       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.864369237Z I0109 04:48:42.864324       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.865163229Z E0109 04:48:42.865142       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:42.869578893Z I0109 04:48:42.869551       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:43.228201923Z E0109 04:48:43.228146       1 leaderelection.go:330] error retrieving resource lock openshift-etcd-operator/openshift-cluster-etcd-operator-lock: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/configmaps/openshift-cluster-etcd-operator-lock?timeout=1m47s": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.263021195Z I0109 04:48:43.262949       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.263923999Z E0109 04:48:43.263903       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:43.465359289Z E0109 04:48:43.465317       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:43.662806900Z I0109 04:48:43.662767       1 request.go:601] Waited for 1.398402865s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:48:44.063686387Z E0109 04:48:44.063643       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.063745622Z I0109 04:48:44.063720       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.263104589Z I0109 04:48:44.263049       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.264230281Z I0109 04:48:44.264193       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.265122429Z E0109 04:48:44.265102       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.269530842Z I0109 04:48:44.269494       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:44.464732393Z E0109 04:48:44.464696       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.663948456Z I0109 04:48:44.663888       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.664313131Z E0109 04:48:44.664282       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:44.862927249Z I0109 04:48:44.862889       1 request.go:601] Waited for 1.316977891s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:48:44.866125899Z E0109 04:48:44.866089       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:45.065161210Z E0109 04:48:45.065124       1 base_controller.go:270] "TargetConfigController" controller failed to sync "", err: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.464018787Z E0109 04:48:45.463963       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.464083581Z I0109 04:48:45.464047       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.663158501Z I0109 04:48:45.663109       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.664177934Z I0109 04:48:45.664146       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.665136129Z E0109 04:48:45.665111       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:45.669702306Z I0109 04:48:45.669670       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:45.895948482Z E0109 04:48:45.895853       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e330b98d", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"RevisionTriggered", Message:"new revision 7 triggered by \"configmap/etcd-endpoints has changed\"", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-revisioncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:46.063050295Z I0109 04:48:46.063010       1 request.go:601] Waited for 1.397355878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:48:46.063819064Z I0109 04:48:46.063779       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.064905069Z E0109 04:48:46.064873       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.265532370Z E0109 04:48:46.265496       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:46.663909820Z E0109 04:48:46.663871       1 base_controller.go:272] EtcdStaticResources reconciliation failed: ["etcd/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:46.864278616Z E0109 04:48:46.864242       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:46.864384366Z I0109 04:48:46.864344       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.063605196Z I0109 04:48:47.063561       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.064644106Z I0109 04:48:47.064620       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.065640650Z E0109 04:48:47.065615       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.070322229Z I0109 04:48:47.070291       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:47.262630314Z I0109 04:48:47.262593       1 request.go:601] Waited for 1.39934191s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:48:47.463539618Z I0109 04:48:47.463492       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.467418649Z E0109 04:48:47.467380       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.664593820Z E0109 04:48:47.664550       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.864577444Z E0109 04:48:47.864541       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:47.931107986Z E0109 04:48:47.931064       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.932097839Z W0109 04:48:47.932071       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:47.932110343Z E0109 04:48:47.932101       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.262854494Z I0109 04:48:48.262815       1 request.go:601] Waited for 1.39826535s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:48:48.264628578Z E0109 04:48:48.264600       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.264710584Z I0109 04:48:48.264685       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.463419067Z I0109 04:48:48.463369       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.464375371Z I0109 04:48:48.464340       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.465222907Z E0109 04:48:48.465194       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.469566587Z I0109 04:48:48.469532       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:48.622417133Z E0109 04:48:48.622317       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e0dffe9a", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"MemberPromote", Message:"successfully promoted learner member https://10.0.145.4:2380", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-etcd-client", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 205086874, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:48.664184226Z E0109 04:48:48.664144       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.740292743Z E0109 04:48:48.740186       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa8bae89e27", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 6 count 0 on node \"ip-10-0-145-4.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 863085095, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 38, 863085095, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:48.863191599Z I0109 04:48:48.863147       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:48.864182489Z E0109 04:48:48.864156       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.462507515Z I0109 04:48:49.462469       1 request.go:601] Waited for 1.197609725s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:48:49.464274612Z E0109 04:48:49.464244       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.464369304Z I0109 04:48:49.464336       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.663577745Z I0109 04:48:49.663530       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.664585495Z I0109 04:48:49.664562       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.665469755Z E0109 04:48:49.665450       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:49.750381510Z I0109 04:48:49.750334       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:49.865422815Z E0109 04:48:49.865382       1 base_controller.go:272] BackingResourceController reconciliation failed: ["manifests/installer-sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa": dial tcp 172.30.0.1:443: connect: connection refused, "manifests/installer-cluster-rolebinding.yaml" (string): Get "https://172.30.0.1:443/apis/rbac.authorization.k8s.io/v1/clusterrolebindings/system:openshift:operator:openshift-etcd-installer": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:50.263536157Z I0109 04:48:50.263481       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 6 count 0 on node "ip-10-0-145-4.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:50.264561045Z E0109 04:48:50.264537       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-6-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:50.463806812Z E0109 04:48:50.463768       1 base_controller.go:270] "TargetConfigController" controller failed to sync "", err: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:50.630194190Z E0109 04:48:50.630083       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e70c51c5", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"DelayingBootstrapTeardown", Message:"cluster-bootstrap is not yet finished - ConfigMap 'kube-system/bootstrap' not found", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-bootstrap-teardown-controller-bootstrapteardowncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 308655045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:48:50.662159518Z I0109 04:48:50.662125       1 request.go:601] Waited for 1.398932735s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:48:50.663801309Z E0109 04:48:50.663774       1 base_controller.go:272] EtcdStaticResources reconciliation failed: ["etcd/ns.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/sa.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa": dial tcp 172.30.0.1:443: connect: connection refused, "etcd/svc.yaml" (string): Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd": dial tcp 172.30.0.1:443: connect: connection refused, Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:48:50.864299206Z E0109 04:48:50.864255       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:50.864350549Z I0109 04:48:50.864331       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:51.062914397Z I0109 04:48:51.062864       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ConfigMapCreateFailed' Failed to create ConfigMap/revision-status-7 -n openshift-etcd: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:51.063957922Z I0109 04:48:51.063923       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'RevisionCreateFailed' Failed to create revision 7: Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:51.064857063Z E0109 04:48:51.064826       1 base_controller.go:272] RevisionController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:48:51.662703355Z I0109 04:48:51.662664       1 request.go:601] Waited for 1.198419226s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:48:53.935397220Z I0109 04:48:53.935347       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2023-01-09T04:48:53.935397220Z cause by changes in data.pod.yaml
2023-01-09T04:48:53.958308843Z I0109 04:48:53.958242       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2023-01-09T04:48:53.958308843Z cause by changes in data.etcd.env
2023-01-09T04:48:53.980463234Z I0109 04:48:53.980427       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 6, but has not made progress because static pod is pending
2023-01-09T04:48:54.498183982Z I0109 04:48:54.498130       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2023-01-09T04:48:54.498183982Z cause by changes in data.pod.yaml
2023-01-09T04:48:55.581731620Z I0109 04:48:55.581682       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RemoveBootstrapEtcd' removing etcd-bootstrap member
2023-01-09T04:48:55.599806579Z I0109 04:48:55.599725       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'MemberRemove' removed member with ID: 3851333976381647311
2023-01-09T04:48:56.062702852Z I0109 04:48:56.062664       1 request.go:601] Waited for 1.113132181s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/configmaps?resourceVersion=23008
2023-01-09T04:48:56.190724710Z I0109 04:48:56.190545       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-endpoints has changed"
2023-01-09T04:48:57.090808569Z I0109 04:48:57.090611       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:48:57.090808569Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:48:57.090808569Z  CurrentRevision: (int32) 6,
2023-01-09T04:48:57.090808569Z  TargetRevision: (int32) 0,
2023-01-09T04:48:57.090808569Z  LastFailedRevision: (int32) 0,
2023-01-09T04:48:57.090808569Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:48:57.090808569Z  LastFailedReason: (string) "",
2023-01-09T04:48:57.090808569Z  LastFailedCount: (int) 0,
2023-01-09T04:48:57.090808569Z  LastFallbackCount: (int) 0,
2023-01-09T04:48:57.090808569Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:48:57.090808569Z }
2023-01-09T04:48:57.090808569Z  because static pod is ready
2023-01-09T04:48:57.262097779Z I0109 04:48:57.262054       1 request.go:601] Waited for 1.976685527s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces?resourceVersion=22903
2023-01-09T04:48:58.194408344Z I0109 04:48:58.194353       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-7 -n openshift-etcd because it was missing
2023-01-09T04:48:58.262437782Z I0109 04:48:58.262395       1 request.go:601] Waited for 1.636328138s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:48:58.581129715Z I0109 04:48:58.581067       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:58.581474117Z I0109 04:48:58.581448       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:58.581855426Z I0109 04:48:58.581836       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:58.582214312Z I0109 04:48:58.582192       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:59.130907565Z I0109 04:48:59.130858       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:59.262981465Z I0109 04:48:59.262947       1 request.go:601] Waited for 1.19459905s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/nodes?labelSelector=node-role.kubernetes.io%2Fmaster&resourceVersion=23017
2023-01-09T04:48:59.403788283Z E0109 04:48:59.403744       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: Operation cannot be fulfilled on etcds.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:48:59.403837664Z I0109 04:48:59.403810       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'BootstrapTeardownErrorUpdatingStatus' Operation cannot be fulfilled on etcds.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:48:59.432281552Z E0109 04:48:59.432247       1 base_controller.go:272] StaticPodStateController reconciliation failed: Operation cannot be fulfilled on etcds.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:48:59.487637146Z I0109 04:48:59.487584       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-7 -n openshift-etcd because it was missing
2023-01-09T04:48:59.555715410Z I0109 04:48:59.555679       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:59.905922668Z I0109 04:48:59.905878       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:48:59.961185726Z I0109 04:48:59.961148       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:00.463112861Z I0109 04:49:00.463051       1 request.go:601] Waited for 1.394686277s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:49:00.740280969Z I0109 04:49:00.740236       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:00.887287550Z I0109 04:49:00.887202       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-7 -n openshift-etcd because it was missing
2023-01-09T04:49:01.070479583Z I0109 04:49:01.070441       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:01.272960034Z E0109 04:49:01.272902       1 base_controller.go:272] InstallerController reconciliation failed: Operation cannot be fulfilled on etcds.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:49:01.463396271Z I0109 04:49:01.463355       1 request.go:601] Waited for 1.40315854s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/kube-system/secrets?resourceVersion=22970
2023-01-09T04:49:01.477462861Z I0109 04:49:01.477424       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:01.673771068Z I0109 04:49:01.673735       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:01.675234304Z I0109 04:49:01.675200       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:01.755633146Z I0109 04:49:01.755592       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 106139648
2023-01-09T04:49:01.755633146Z I0109 04:49:01.755609       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 106704896
2023-01-09T04:49:01.755633146Z I0109 04:49:01.755616       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 106442752
2023-01-09T04:49:02.040116528Z I0109 04:49:02.040075       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.136578291Z E0109 04:49:02.136537       1 base_controller.go:272] BootstrapTeardownController reconciliation failed: Operation cannot be fulfilled on etcds.operator.openshift.io "cluster": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:49:02.138954168Z I0109 04:49:02.138912       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:02.139709520Z I0109 04:49:02.139670       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"BootstrapTeardownDegraded: Operation cannot be fulfilled on etcds.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"BootstrapTeardown_Error::GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:02.148589139Z I0109 04:49:02.148542       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "BootstrapTeardownDegraded: Operation cannot be fulfilled on etcds.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:49:02.152169587Z I0109 04:49:02.152126       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.220276851Z I0109 04:49:02.220239       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 106778624
2023-01-09T04:49:02.276241318Z I0109 04:49:02.276200       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:02.277098178Z I0109 04:49:02.277042       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:02.286306886Z I0109 04:49:02.286273       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.286808750Z I0109 04:49:02.286767       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "BootstrapTeardownDegraded: Operation cannot be fulfilled on etcds.operator.openshift.io \"cluster\": the object has been modified; please apply your changes to the latest version and try again\nGuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:49:02.662944560Z I0109 04:49:02.662904       1 request.go:601] Waited for 1.919696853s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?resourceVersion=23430
2023-01-09T04:49:02.666579281Z I0109 04:49:02.666532       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.667028085Z I0109 04:49:02.666889       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.667252900Z I0109 04:49:02.667232       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.743340234Z I0109 04:49:02.743301       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:02.868119331Z I0109 04:49:02.868053       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-7 -n openshift-etcd because it was missing
2023-01-09T04:49:02.910105297Z E0109 04:49:02.909969       1 event.go:267] Server rejected event '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa7e330b98d", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"RevisionTriggered", Message:"new revision 7 triggered by \"configmap/etcd-endpoints has changed\"", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-revisioncontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 48, 35, 243932045, time.Local), Count:1, Type:"Normal", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'etcdserver: request timed out' (will not retry!)
2023-01-09T04:49:03.325602323Z I0109 04:49:03.325548       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:03.662962655Z I0109 04:49:03.662920       1 request.go:601] Waited for 1.99652257s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:49:03.728092635Z I0109 04:49:03.728037       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:04.044415339Z I0109 04:49:04.044357       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:04.104051693Z I0109 04:49:04.104008       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:04.450055033Z I0109 04:49:04.450018       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:04.523225508Z I0109 04:49:04.523185       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:04.862746340Z I0109 04:49:04.862706       1 request.go:601] Waited for 1.994648412s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps
2023-01-09T04:49:04.866436592Z I0109 04:49:04.866390       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-7 -n openshift-etcd because it was missing
2023-01-09T04:49:05.309906512Z I0109 04:49:05.309856       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:05.617856596Z I0109 04:49:05.617811       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:05.684050635Z I0109 04:49:05.684011       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:06.041236971Z I0109 04:49:06.041159       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:06.063012287Z I0109 04:49:06.062966       1 request.go:601] Waited for 1.796433417s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:06.115509116Z I0109 04:49:06.115454       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:06.668356565Z I0109 04:49:06.668304       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-7 -n openshift-etcd because it was missing
2023-01-09T04:49:06.713214632Z I0109 04:49:06.713144       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.065768034Z I0109 04:49:07.065729       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:07.065768034Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:49:07.065768034Z  CurrentRevision: (int32) 6,
2023-01-09T04:49:07.065768034Z  TargetRevision: (int32) 0,
2023-01-09T04:49:07.065768034Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:07.065768034Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:07.065768034Z  LastFailedReason: (string) "",
2023-01-09T04:49:07.065768034Z  LastFailedCount: (int) 0,
2023-01-09T04:49:07.065768034Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:07.065768034Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:07.065768034Z }
2023-01-09T04:49:07.065768034Z  because static pod is ready
2023-01-09T04:49:07.076097003Z I0109 04:49:07.076059       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:07.076189107Z I0109 04:49:07.076149       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 0 to 6 because static pod is ready
2023-01-09T04:49:07.077261368Z I0109 04:49:07.077224       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:07.084223418Z I0109 04:49:07.084190       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6",Available message changed from "StaticPodsAvailable: 2 nodes are active; 1 nodes are at revision 0; 1 nodes are at revision 2; 1 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:49:07.089350890Z I0109 04:49:07.089321       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.262743412Z I0109 04:49:07.262670       1 request.go:601] Waited for 1.596128655s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:49:07.363277117Z I0109 04:49:07.363225       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.369393218Z I0109 04:49:07.369337       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.697160392Z I0109 04:49:07.697119       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.869949481Z I0109 04:49:07.869892       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/etcd-guard-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:49:07.870142937Z I0109 04:49:07.870107       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.879030585Z I0109 04:49:07.878963       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:07.890793401Z I0109 04:49:07.890755       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:08.066908575Z I0109 04:49:08.066851       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-7 -n openshift-etcd because it was missing
2023-01-09T04:49:08.436317330Z I0109 04:49:08.436260       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:08.462171841Z I0109 04:49:08.462123       1 request.go:601] Waited for 1.386304367s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:49:09.238642010Z I0109 04:49:09.238584       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:09.462827933Z I0109 04:49:09.462789       1 request.go:601] Waited for 1.592701455s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:09.675179514Z I0109 04:49:09.675134       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:49:09.868467256Z I0109 04:49:09.868406       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-7 -n openshift-etcd because it was missing
2023-01-09T04:49:09.881728532Z I0109 04:49:09.881674       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 6 created because configmap/etcd-endpoints has changed
2023-01-09T04:49:09.885447288Z I0109 04:49:09.885411       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:09.888252166Z I0109 04:49:09.888212       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 7 triggered by "configmap/etcd-pod has changed,configmap/etcd-endpoints has changed"
2023-01-09T04:49:10.663067970Z I0109 04:49:10.663009       1 request.go:601] Waited for 1.59826562s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:49:11.667491905Z I0109 04:49:11.667439       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/revision-status-7 -n openshift-etcd:
2023-01-09T04:49:11.667491905Z cause by changes in data.reason
2023-01-09T04:49:11.862319865Z I0109 04:49:11.862281       1 request.go:601] Waited for 1.796918043s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:49:12.862954581Z I0109 04:49:12.862912       1 request.go:601] Waited for 1.796101883s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:13.274234503Z I0109 04:49:13.274188       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod-7 -n openshift-etcd:
2023-01-09T04:49:13.274234503Z cause by changes in data.pod.yaml
2023-01-09T04:49:13.277083761Z W0109 04:49:13.277051       1 staticpod.go:38] revision 7 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:49:13.287701546Z E0109 04:49:13.287671       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 7
2023-01-09T04:49:13.288339832Z I0109 04:49:13.288286       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:13.290721923Z I0109 04:49:13.290677       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 7","reason":"GuardController_SyncError::RevisionController_Error","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:13.299184982Z I0109 04:49:13.299145       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal" to "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 7"
2023-01-09T04:49:13.308519787Z I0109 04:49:13.308491       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:13.308928160Z I0109 04:49:13.308887       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:44:47Z","message":"GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal","reason":"GuardController_SyncError","status":"True","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:13.316629140Z I0109 04:49:13.316592       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal\nRevisionControllerDegraded: conflicting latestAvailableRevision 7" to "GuardControllerDegraded: Missing PodIP in operand etcd-ip-10-0-145-4.us-east-2.compute.internal on node ip-10-0-145-4.us-east-2.compute.internal"
2023-01-09T04:49:13.372124931Z I0109 04:49:13.372090       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 107937792
2023-01-09T04:49:13.372124931Z I0109 04:49:13.372106       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 107646976
2023-01-09T04:49:13.440191885Z I0109 04:49:13.440148       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 107937792
2023-01-09T04:49:13.863095328Z I0109 04:49:13.863059       1 request.go:601] Waited for 1.597612304s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:49:14.476164623Z I0109 04:49:14.476120       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:14.477082685Z I0109 04:49:14.477049       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:14.483748152Z I0109 04:49:14.483700       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded changed from True to False ("NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found")
2023-01-09T04:49:14.550115391Z I0109 04:49:14.550072       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 107757568
2023-01-09T04:49:15.062095969Z I0109 04:49:15.062059       1 request.go:601] Waited for 1.420851449s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:49:16.062412503Z I0109 04:49:16.062372       1 request.go:601] Waited for 1.583377868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:16.665317834Z I0109 04:49:16.665280       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal with revision 2 is the oldest and needs new revision 6
2023-01-09T04:49:16.665356255Z I0109 04:49:16.665323       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:16.665356255Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:49:16.665356255Z  CurrentRevision: (int32) 2,
2023-01-09T04:49:16.665356255Z  TargetRevision: (int32) 6,
2023-01-09T04:49:16.665356255Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:16.665356255Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:16.665356255Z  LastFailedReason: (string) "",
2023-01-09T04:49:16.665356255Z  LastFailedCount: (int) 0,
2023-01-09T04:49:16.665356255Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:16.665356255Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:16.665356255Z }
2023-01-09T04:49:16.675536444Z I0109 04:49:16.675493       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 2 to 6 because node ip-10-0-199-219.us-east-2.compute.internal with revision 2 is the oldest
2023-01-09T04:49:16.677265145Z I0109 04:49:16.677222       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:49:16.677618812Z I0109 04:49:16.677579       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:16.685779648Z I0109 04:49:16.685737       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6" to "NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:49:16.760670747Z I0109 04:49:16.760634       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 107638784
2023-01-09T04:49:17.262292392Z I0109 04:49:17.262252       1 request.go:601] Waited for 1.396120808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:49:18.462162930Z I0109 04:49:18.462123       1 request.go:601] Waited for 1.396430825s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:49:19.266684583Z I0109 04:49:19.266648       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:49:19.266684583Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:49:19.266684583Z  CurrentRevision: (int32) 2,
2023-01-09T04:49:19.266684583Z  TargetRevision: (int32) 7,
2023-01-09T04:49:19.266684583Z  LastFailedRevision: (int32) 0,
2023-01-09T04:49:19.266684583Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:49:19.266684583Z  LastFailedReason: (string) "",
2023-01-09T04:49:19.266684583Z  LastFailedCount: (int) 0,
2023-01-09T04:49:19.266684583Z  LastFallbackCount: (int) 0,
2023-01-09T04:49:19.266684583Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:49:19.266684583Z }
2023-01-09T04:49:19.266684583Z  because new revision pending
2023-01-09T04:49:19.280468462Z I0109 04:49:19.280429       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:49:19.462333194Z I0109 04:49:19.462285       1 request.go:601] Waited for 1.197102789s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:49:20.462340296Z I0109 04:49:20.462300       1 request.go:601] Waited for 1.183037022s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:49:21.662398368Z I0109 04:49:21.662355       1 request.go:601] Waited for 1.396085576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:49:21.868584466Z I0109 04:49:21.868529       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-199-219.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:49:22.662524445Z I0109 04:49:22.662469       1 request.go:601] Waited for 1.19734053s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:49:23.065603949Z I0109 04:49:23.065563       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:49:23.862133206Z I0109 04:49:23.862092       1 request.go:601] Waited for 1.193709293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:25.062279277Z I0109 04:49:25.062240       1 request.go:601] Waited for 1.197103973s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:49:25.265280553Z I0109 04:49:25.265236       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:26.665181506Z I0109 04:49:26.665140       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:55.438662041Z I0109 04:49:55.438620       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:49:57.472690546Z I0109 04:49:57.472637       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 2
2023-01-09T04:50:09.599626293Z I0109 04:50:09.599592       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:09.610118039Z I0109 04:50:09.610084       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 2
2023-01-09T04:50:10.467190945Z I0109 04:50:10.467140       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:10.477618949Z E0109 04:50:10.477583       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:50:10.479194564Z I0109 04:50:10.479159       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:50:10.479505293Z I0109 04:50:10.479483       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:50:10.486850812Z I0109 04:50:10.486794       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:50:10.490676955Z I0109 04:50:10.490638       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:11.599757962Z I0109 04:50:11.599724       1 request.go:601] Waited for 1.114830039s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:50:13.003029403Z I0109 04:50:13.002971       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 2
2023-01-09T04:50:13.641876565Z I0109 04:50:13.641838       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:25.531227857Z I0109 04:50:25.531176       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:25.531408119Z E0109 04:50:25.531391       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:50:25.532121157Z E0109 04:50:25.532100       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:50:27.972499500Z E0109 04:50:27.972461       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-199-219.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.199.219:2379]: context deadline exceeded
2023-01-09T04:50:27.988533374Z I0109 04:50:27.988492       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:27.989026871Z I0109 04:50:27.988973       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:50:27.991112881Z I0109 04:50:27.991066       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:50:28.004591142Z I0109 04:50:28.004549       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:28.005291216Z I0109 04:50:28.005137       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:50:28.020047454Z I0109 04:50:28.019984       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:50:28.020456499Z I0109 04:50:28.020425       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:50:28.027197314Z I0109 04:50:28.027159       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:50:28.029882314Z I0109 04:50:28.029840       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:29.185376815Z I0109 04:50:29.185339       1 request.go:601] Waited for 1.165259398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:50:29.391445293Z I0109 04:50:29.391403       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 2
2023-01-09T04:50:31.989289695Z I0109 04:50:31.989248       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 2
2023-01-09T04:50:37.155913415Z I0109 04:50:37.155870       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:37.162063170Z I0109 04:50:37.162022       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:37.190024713Z I0109 04:50:37.189969       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:37.358492469Z I0109 04:50:37.358454       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:50:37.639255098Z I0109 04:50:37.639209       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:38.355693309Z I0109 04:50:38.355651       1 request.go:601] Waited for 1.165700656s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:50:38.654835424Z I0109 04:50:38.654794       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:39.356058622Z I0109 04:50:39.356019       1 request.go:601] Waited for 1.196871882s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:50:39.654266911Z I0109 04:50:39.654222       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:40.555426507Z I0109 04:50:40.555386       1 request.go:601] Waited for 1.195947324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:50:40.564835136Z I0109 04:50:40.564794       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:40.564956219Z E0109 04:50:40.564940       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:50:40.569516777Z E0109 04:50:40.569480       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:50:40.659599930Z I0109 04:50:40.659548       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:40.958593379Z I0109 04:50:40.958554       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:50:43.359379546Z I0109 04:50:43.359342       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:50:49.895980189Z I0109 04:50:49.895937       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:50.640724522Z I0109 04:50:50.640682       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:52.694285861Z I0109 04:50:52.694243       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:55.601107115Z I0109 04:50:55.601059       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:50:55.601340356Z E0109 04:50:55.601326       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:50:55.604745790Z E0109 04:50:55.604703       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:50:59.477479960Z I0109 04:50:59.477436       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:50:59.894150509Z I0109 04:50:59.894112       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:51:00.680673418Z I0109 04:51:00.680628       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:00.683084157Z I0109 04:51:00.683049       1 prune_controller.go:269] Nothing to prune
2023-01-09T04:51:00.693721135Z I0109 04:51:00.693680       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:51:01.684811856Z I0109 04:51:01.684768       1 request.go:601] Waited for 1.000265141s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:51:03.288758991Z I0109 04:51:03.288714       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:51:07.251092721Z I0109 04:51:07.251055       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:51:08.434897746Z I0109 04:51:08.434863       1 request.go:601] Waited for 1.163264737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:51:10.038376073Z I0109 04:51:10.038333       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:51:10.038376073Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:51:10.038376073Z  CurrentRevision: (int32) 7,
2023-01-09T04:51:10.038376073Z  TargetRevision: (int32) 0,
2023-01-09T04:51:10.038376073Z  LastFailedRevision: (int32) 0,
2023-01-09T04:51:10.038376073Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:51:10.038376073Z  LastFailedReason: (string) "",
2023-01-09T04:51:10.038376073Z  LastFailedCount: (int) 0,
2023-01-09T04:51:10.038376073Z  LastFallbackCount: (int) 0,
2023-01-09T04:51:10.038376073Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:51:10.038376073Z }
2023-01-09T04:51:10.038376073Z  because static pod is ready
2023-01-09T04:51:10.048845370Z I0109 04:51:10.048762       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 2 to 7 because static pod is ready
2023-01-09T04:51:10.050519818Z I0109 04:51:10.050488       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:10.066408489Z I0109 04:51:10.066354       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7" to "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7",Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 2; 2 nodes are at revision 6; 0 nodes have achieved new revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:51:11.235323007Z I0109 04:51:11.235282       1 request.go:601] Waited for 1.184823543s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:51:12.434823717Z I0109 04:51:12.434781       1 request.go:601] Waited for 1.39629383s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:51:12.978699408Z I0109 04:51:12.978651       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.2651839389400688 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003921,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.006244,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.008264. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:51:12.990509054Z I0109 04:51:12.990468       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:13.003575133Z I0109 04:51:13.003529       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:51:13.016444122Z I0109 04:51:13.016406       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:51:13.025703540Z I0109 04:51:13.025660       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:51:13.045068210Z I0109 04:51:13.045020       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:51:13.435577129Z I0109 04:51:13.435537       1 request.go:601] Waited for 1.397437935s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:14.634849576Z I0109 04:51:14.634801       1 request.go:601] Waited for 1.590143065s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:51:15.635788694Z I0109 04:51:15.635751       1 request.go:601] Waited for 1.797362432s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:51:16.647466881Z I0109 04:51:16.647354       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:51:16.834928024Z I0109 04:51:16.834895       1 request.go:601] Waited for 1.796554754s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:51:17.835575520Z I0109 04:51:17.835536       1 request.go:601] Waited for 1.59679922s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:51:19.035322640Z I0109 04:51:19.035286       1 request.go:601] Waited for 1.597126673s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:51:19.237967897Z I0109 04:51:19.237916       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:51:19.238035446Z I0109 04:51:19.237966       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:51:19.238035446Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:51:19.238035446Z  CurrentRevision: (int32) 6,
2023-01-09T04:51:19.238035446Z  TargetRevision: (int32) 7,
2023-01-09T04:51:19.238035446Z  LastFailedRevision: (int32) 0,
2023-01-09T04:51:19.238035446Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:51:19.238035446Z  LastFailedReason: (string) "",
2023-01-09T04:51:19.238035446Z  LastFailedCount: (int) 0,
2023-01-09T04:51:19.238035446Z  LastFallbackCount: (int) 0,
2023-01-09T04:51:19.238035446Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:51:19.238035446Z }
2023-01-09T04:51:19.252966552Z I0109 04:51:19.252920       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 6 to 7 because node ip-10-0-160-211.us-east-2.compute.internal with revision 6 is the oldest
2023-01-09T04:51:19.332878847Z I0109 04:51:19.332842       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 116408320
2023-01-09T04:51:19.332878847Z I0109 04:51:19.332858       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 116256768
2023-01-09T04:51:19.640485043Z I0109 04:51:19.640411       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:51:20.035607904Z I0109 04:51:20.035570       1 request.go:601] Waited for 1.394524293s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:51:21.035639181Z I0109 04:51:21.035597       1 request.go:601] Waited for 1.781024021s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:51:22.235716963Z I0109 04:51:22.235676       1 request.go:601] Waited for 1.796770999s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:23.235745661Z I0109 04:51:23.235708       1 request.go:601] Waited for 1.596872141s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:51:24.435355791Z I0109 04:51:24.435316       1 request.go:601] Waited for 1.39738727s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:51:25.635523557Z I0109 04:51:25.635479       1 request.go:601] Waited for 1.197046645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:51:27.238345158Z I0109 04:51:27.238307       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:51:27.238382173Z I0109 04:51:27.238350       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:51:27.238382173Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:51:27.238382173Z  CurrentRevision: (int32) 6,
2023-01-09T04:51:27.238382173Z  TargetRevision: (int32) 7,
2023-01-09T04:51:27.238382173Z  LastFailedRevision: (int32) 0,
2023-01-09T04:51:27.238382173Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:51:27.238382173Z  LastFailedReason: (string) "",
2023-01-09T04:51:27.238382173Z  LastFailedCount: (int) 0,
2023-01-09T04:51:27.238382173Z  LastFallbackCount: (int) 0,
2023-01-09T04:51:27.238382173Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:51:27.238382173Z }
2023-01-09T04:51:28.442325107Z I0109 04:51:28.442271       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:51:29.039101963Z I0109 04:51:29.039054       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:51:30.638217514Z I0109 04:51:30.638177       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:51:32.038200937Z I0109 04:51:32.038154       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:02.383882392Z I0109 04:52:02.383839       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:52:04.576690082Z I0109 04:52:04.576632       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:12.974234855Z I0109 04:52:12.974182       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.5300208663455654 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003921,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.006244,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.008264. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:52:15.288369780Z I0109 04:52:15.288316       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:15.651900532Z I0109 04:52:15.651851       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:17.414840101Z I0109 04:52:17.414776       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:17.427139333Z E0109 04:52:17.427097       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:52:17.431094119Z I0109 04:52:17.431058       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:17.441309356Z I0109 04:52:17.441255       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:52:17.454726562Z I0109 04:52:17.454682       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:18.383911784Z I0109 04:52:18.383872       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:18.446651979Z I0109 04:52:18.446614       1 request.go:601] Waited for 1.010479529s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:52:20.051859090Z I0109 04:52:20.051819       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:22.057214107Z I0109 04:52:22.057168       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:22.985171759Z I0109 04:52:22.985129       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:26.739184199Z I0109 04:52:26.739137       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:27.655149208Z I0109 04:52:27.655110       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:27.976503507Z E0109 04:52:27.976454       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-160-211.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.160.211:2379]: context deadline exceeded
2023-01-09T04:52:27.989507785Z I0109 04:52:27.989468       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:27.998856668Z I0109 04:52:27.998819       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:52:28.001861379Z I0109 04:52:28.001829       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:28.021085920Z I0109 04:52:28.021049       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:28.030648870Z I0109 04:52:28.029432       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:52:28.031152424Z I0109 04:52:28.031116       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:28.994320967Z I0109 04:52:28.994277       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:29.189178166Z I0109 04:52:29.189137       1 request.go:601] Waited for 1.16947425s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:52:30.057489417Z I0109 04:52:30.057433       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:30.189575595Z I0109 04:52:30.189534       1 request.go:601] Waited for 1.390150503s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:52:31.189590678Z I0109 04:52:31.189549       1 request.go:601] Waited for 1.195923465s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:52:32.465230311Z I0109 04:52:32.465187       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:32.465312492Z E0109 04:52:32.465296       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:52:32.484553184Z E0109 04:52:32.484516       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:52:32.594192336Z I0109 04:52:32.594151       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:52:43.344714614Z I0109 04:52:43.344673       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:43.352553419Z I0109 04:52:43.352516       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:43.365254411Z I0109 04:52:43.365212       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:43.390534100Z I0109 04:52:43.390490       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:43.549575614Z I0109 04:52:43.549533       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:44.544638972Z I0109 04:52:44.544598       1 request.go:601] Waited for 1.172930184s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:52:44.553522256Z I0109 04:52:44.553450       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:45.364144578Z I0109 04:52:45.364086       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:45.559887795Z I0109 04:52:45.559834       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:45.743778854Z I0109 04:52:45.743745       1 request.go:601] Waited for 1.193046287s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:52:45.745238231Z E0109 04:52:45.745204       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:45.946146900Z E0109 04:52:45.946114       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.144554057Z E0109 04:52:46.144507       1 guard_controller.go:304] Unable to apply pod etcd-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.345950054Z E0109 04:52:46.345916       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.346064808Z I0109 04:52:46.346039       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.346795807Z E0109 04:52:46.346719       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa82fffb991", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"23550", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"ScriptControllerErrorUpdatingStatus", Message:"Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-script-controller-scriptcontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 52, 46, 345894393, time.Local), Count:12, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events/etcd-operator.17388aa82fffb991": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:52:46.546429304Z E0109 04:52:46.546390       1 base_controller.go:270] "TargetConfigController" controller failed to sync "", err: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.744116836Z I0109 04:52:46.744084       1 request.go:601] Waited for 1.195405433s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:46.746342317Z E0109 04:52:46.746314       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:46.945099662Z E0109 04:52:46.945061       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.144778193Z I0109 04:52:47.144726       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.145354233Z E0109 04:52:47.145257       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388ae289a3326d", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"InstallerPodFailed", Message:"Failed to create installer pod for revision 7 count 0 on node \"ip-10-0-160-211.us-east-2.compute.internal\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-installer-controller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 52, 47, 144555117, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 52, 47, 144555117, time.Local), Count:1, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Post "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:52:47.145743559Z E0109 04:52:47.145727       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.346822779Z W0109 04:52:47.346776       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.346863922Z E0109 04:52:47.346821       1 base_controller.go:272] GuardController reconciliation failed: [Unable to apply pod etcd-guard-ip-10-0-160-211.us-east-2.compute.internal changes: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused, Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-145-4.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused]
2023-01-09T04:52:47.347752608Z E0109 04:52:47.347713       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.348840428Z W0109 04:52:47.348810       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.348840428Z E0109 04:52:47.348836       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.352806630Z E0109 04:52:47.352773       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.353922092Z W0109 04:52:47.353886       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.353922092Z E0109 04:52:47.353911       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.374888123Z E0109 04:52:47.374852       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.375983647Z W0109 04:52:47.375943       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.376052398Z E0109 04:52:47.375982       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.417637118Z E0109 04:52:47.417590       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.419093173Z W0109 04:52:47.419056       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.419093173Z E0109 04:52:47.419083       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.452712474Z E0109 04:52:47.452617       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388aa82fffb991", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"23550", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"ScriptControllerErrorUpdatingStatus", Message:"Put \"https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status\": dial tcp 172.30.0.1:443: connect: connection refused", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-script-controller-scriptcontroller", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 48, 36, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 52, 46, 345894393, time.Local), Count:12, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events/etcd-operator.17388aa82fffb991": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:52:47.498419518Z I0109 04:52:47.498375       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:52:47.498605787Z E0109 04:52:47.498590       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:52:47.499203652Z E0109 04:52:47.499109       1 event.go:276] Unable to write event: '&v1.Event{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"etcd-operator.17388a8fe2499346", GenerateName:"", Namespace:"openshift-etcd-operator", SelfLink:"", UID:"", ResourceVersion:"26826", Generation:0, CreationTimestamp:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string(nil), Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry(nil)}, InvolvedObject:v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}, Reason:"UnhealthyEtcdMember", Message:"unhealthy members: ip-10-0-160-211.us-east-2.compute.internal", Source:v1.EventSource{Component:"openshift-cluster-etcd-operator-etcd-client", Host:""}, FirstTimestamp:time.Date(2023, time.January, 9, 4, 46, 52, 0, time.Local), LastTimestamp:time.Date(2023, time.January, 9, 4, 52, 47, 498281511, time.Local), Count:6, Type:"Warning", EventTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Series:(*v1.EventSeries)(nil), Action:"", Related:(*v1.ObjectReference)(nil), ReportingController:"", ReportingInstance:""}': 'Patch "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd-operator/events/etcd-operator.17388a8fe2499346": dial tcp 172.30.0.1:443: connect: connection refused'(may retry after sleeping)
2023-01-09T04:52:47.500086090Z E0109 04:52:47.500056       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.501177167Z W0109 04:52:47.501146       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.501177167Z E0109 04:52:47.501170       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.516131157Z E0109 04:52:47.516101       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:52:47.549851003Z E0109 04:52:47.549789       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.550190566Z I0109 04:52:47.550149       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.662422081Z E0109 04:52:47.662383       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.663456953Z W0109 04:52:47.663422       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.663456953Z E0109 04:52:47.663447       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.745859098Z I0109 04:52:47.745654       1 request.go:601] Waited for 1.198721759s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:52:47.984642868Z E0109 04:52:47.984591       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.985736949Z W0109 04:52:47.985699       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:47.985736949Z E0109 04:52:47.985729       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.144791521Z E0109 04:52:48.144750       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.345288217Z I0109 04:52:48.345241       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.346374364Z E0109 04:52:48.346353       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.546010493Z E0109 04:52:48.545962       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.546071731Z I0109 04:52:48.546047       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.627800382Z E0109 04:52:48.627758       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.628802035Z W0109 04:52:48.628775       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.628818404Z E0109 04:52:48.628802       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:48.746191382Z E0109 04:52:48.746146       1 base_controller.go:272] TargetConfigController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.145204132Z E0109 04:52:49.145168       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.344810777Z I0109 04:52:49.344737       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.345927721Z E0109 04:52:49.345894       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.545799392Z E0109 04:52:49.545763       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.545868134Z I0109 04:52:49.545842       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.910122400Z E0109 04:52:49.910080       1 guard_controller.go:214] Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.911134630Z W0109 04:52:49.911103       1 base_controller.go:236] Updating status of "GuardController" failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.911161551Z E0109 04:52:49.911131       1 base_controller.go:272] GuardController reconciliation failed: Unable to apply PodDisruptionBudget changes: Get "https://172.30.0.1:443/apis/policy/v1/namespaces/openshift-etcd/poddisruptionbudgets/etcd-guard-pdb": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:49.945980900Z E0109 04:52:49.945950       1 base_controller.go:272] StaticPodStateController reconciliation failed: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.145286223Z E0109 04:52:50.145247       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.345526164Z I0109 04:52:50.345480       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.346668148Z E0109 04:52:50.346632       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.545599703Z E0109 04:52:50.545562       1 base_controller.go:270] "ScriptController" controller failed to sync "", err: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.545665040Z I0109 04:52:50.545640       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:50.745977472Z E0109 04:52:50.745937       1 base_controller.go:270] "TargetConfigController" controller failed to sync "", err: Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:51.145142749Z E0109 04:52:51.145104       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:51.344695608Z I0109 04:52:51.344648       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:51.345643879Z E0109 04:52:51.345616       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:51.546128797Z E0109 04:52:51.546095       1 base_controller.go:272] ScriptController reconciliation failed: "configmap/etcd-pod": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:51.546196690Z I0109 04:52:51.546172       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'ScriptControllerErrorUpdatingStatus' Put "https://172.30.0.1:443/apis/operator.openshift.io/v1/etcds/cluster/status": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:52.145240830Z E0109 04:52:52.145198       1 base_controller.go:272] InstallerStateController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:52.344888178Z I0109 04:52:52.344838       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'InstallerPodFailed' Failed to create installer pod for revision 7 count 0 on node "ip-10-0-160-211.us-east-2.compute.internal": Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:52.345861528Z E0109 04:52:52.345838       1 base_controller.go:272] InstallerController reconciliation failed: Get "https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-7-ip-10-0-160-211.us-east-2.compute.internal": dial tcp 172.30.0.1:443: connect: connection refused
2023-01-09T04:52:52.944703060Z I0109 04:52:52.944662       1 request.go:601] Waited for 1.000236043s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:52:55.570612767Z I0109 04:52:55.570560       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:55.858590369Z I0109 04:52:55.858551       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:52:58.285516641Z I0109 04:52:58.285475       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.285983382Z I0109 04:52:58.285952       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.286320333Z I0109 04:52:58.286301       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.286544595Z I0109 04:52:58.286529       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.286752842Z I0109 04:52:58.286738       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.287002503Z I0109 04:52:58.286959       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.743199986Z I0109 04:52:58.743148       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.743718184Z I0109 04:52:58.743688       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.744183769Z I0109 04:52:58.744163       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.744568121Z I0109 04:52:58.744528       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.745174000Z I0109 04:52:58.745141       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.746205131Z I0109 04:52:58.746176       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:58.746439367Z I0109 04:52:58.746423       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:59.247117653Z I0109 04:52:59.247077       1 request.go:601] Waited for 1.036378868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces?resourceVersion=26446
2023-01-09T04:52:59.464388609Z I0109 04:52:59.464328       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:59.486713304Z I0109 04:52:59.486671       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused"
2023-01-09T04:52:59.491371814Z I0109 04:52:59.491337       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:59.688476159Z E0109 04:52:59.688439       1 base_controller.go:272] TargetConfigController reconciliation failed: synthetic requeue request
2023-01-09T04:52:59.699526746Z I0109 04:52:59.699463       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:52:59.740060515Z I0109 04:52:59.737734       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:52:59.740060515Z I0109 04:52:59.738069       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused"
2023-01-09T04:53:00.247182347Z I0109 04:53:00.247145       1 request.go:601] Waited for 1.769737463s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:00.847173440Z I0109 04:53:00.847127       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:01.447384791Z I0109 04:53:01.447352       1 request.go:601] Waited for 2.320795492s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-config-managed/secrets?resourceVersion=26454
2023-01-09T04:53:02.538656389Z I0109 04:53:02.538605       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:02.538953719Z E0109 04:53:02.538933       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:53:02.554644126Z E0109 04:53:02.554607       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:53:02.647089401Z I0109 04:53:02.647026       1 request.go:601] Waited for 2.392249486s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:02.931390215Z I0109 04:53:02.931347       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:02.953521618Z I0109 04:53:02.953473       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused" to "NodeControllerDegraded: All master nodes are ready\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused"
2023-01-09T04:53:02.955779450Z I0109 04:53:02.955744       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:02.965233589Z I0109 04:53:02.965198       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.30 %, dbSize: 120430592
2023-01-09T04:53:02.965301949Z I0109 04:53:02.965271       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.14 %, dbSize: 119762944
2023-01-09T04:53:02.989637225Z I0109 04:53:02.989588       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.062153275Z I0109 04:53:03.062119       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.101292750Z I0109 04:53:03.101253       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 119922688
2023-01-09T04:53:03.101347315Z I0109 04:53:03.101332       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.22 %, dbSize: 120430592
2023-01-09T04:53:03.101386374Z I0109 04:53:03.101374       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.11 %, dbSize: 119762944
2023-01-09T04:53:03.102020474Z I0109 04:53:03.101940       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.139971626Z I0109 04:53:03.139927       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.491747298Z I0109 04:53:03.488153       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.528027807Z I0109 04:53:03.527948       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.647604616Z I0109 04:53:03.647569       1 request.go:601] Waited for 1.79400349s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:03.727081079Z I0109 04:53:03.726727       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:03.853094226Z I0109 04:53:03.853037       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:03.853094226Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:53:03.853094226Z  CurrentRevision: (int32) 7,
2023-01-09T04:53:03.853094226Z  TargetRevision: (int32) 0,
2023-01-09T04:53:03.853094226Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:03.853094226Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:03.853094226Z  LastFailedReason: (string) "",
2023-01-09T04:53:03.853094226Z  LastFailedCount: (int) 0,
2023-01-09T04:53:03.853094226Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:03.853094226Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:03.853094226Z }
2023-01-09T04:53:03.853094226Z  because static pod is ready
2023-01-09T04:53:03.866775663Z I0109 04:53:03.866721       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 6 to 7 because static pod is ready
2023-01-09T04:53:03.868206467Z I0109 04:53:03.868174       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:03.882531555Z I0109 04:53:03.882445       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 6; 1 nodes are at revision 7" to "NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 6; 1 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:53:03.886639157Z I0109 04:53:03.886581       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:04.023922285Z I0109 04:53:04.023875       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 120197120
2023-01-09T04:53:04.023922285Z I0109 04:53:04.023891       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.07 %, dbSize: 120479744
2023-01-09T04:53:04.023922285Z I0109 04:53:04.023896       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 119943168
2023-01-09T04:53:04.088730125Z I0109 04:53:04.088689       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:04.107856185Z I0109 04:53:04.107804       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nTargetConfigControllerDegraded: \"configmap/etcd-pod\": Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod\": dial tcp 172.30.0.1:443: connect: connection refused\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused"
2023-01-09T04:53:04.120302992Z I0109 04:53:04.120248       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:53:04.265946424Z I0109 04:53:04.265910       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 120246272
2023-01-09T04:53:04.265946424Z I0109 04:53:04.265929       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.04 %, dbSize: 120479744
2023-01-09T04:53:04.847535268Z I0109 04:53:04.847499       1 request.go:601] Waited for 1.555318661s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:53:05.467720344Z I0109 04:53:05.467593       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:05.482309395Z I0109 04:53:05.479386       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused\nStaticPodsDegraded: Get \"https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal\": dial tcp 172.30.0.1:443: connect: connection refused" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:53:05.847713568Z I0109 04:53:05.847672       1 request.go:601] Waited for 1.761696055s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:53:07.047517234Z I0109 04:53:07.047475       1 request.go:601] Waited for 1.79294212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:53:08.247678964Z I0109 04:53:08.247633       1 request.go:601] Waited for 1.795882153s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:53:09.051111510Z I0109 04:53:09.051068       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:09.051111510Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:53:09.051111510Z  CurrentRevision: (int32) 7,
2023-01-09T04:53:09.051111510Z  TargetRevision: (int32) 0,
2023-01-09T04:53:09.051111510Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:09.051111510Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:09.051111510Z  LastFailedReason: (string) "",
2023-01-09T04:53:09.051111510Z  LastFailedCount: (int) 0,
2023-01-09T04:53:09.051111510Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:09.051111510Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:09.051111510Z }
2023-01-09T04:53:09.051111510Z  because static pod is ready
2023-01-09T04:53:09.447792284Z I0109 04:53:09.447748       1 request.go:601] Waited for 1.596217802s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:53:10.647170807Z I0109 04:53:10.647130       1 request.go:601] Waited for 1.393704156s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:53:11.647174013Z I0109 04:53:11.647140       1 request.go:601] Waited for 1.196292278s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:53:12.982708521Z I0109 04:53:12.982670       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.15838234674827 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003921,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.006208,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.008264. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:53:13.000859535Z I0109 04:53:13.000816       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:13.018339306Z I0109 04:53:13.018294       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:53:13.024590271Z I0109 04:53:13.024526       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:53:13.037884793Z I0109 04:53:13.035509       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:53:13.098652781Z I0109 04:53:13.098595       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 120811520
2023-01-09T04:53:13.098652781Z I0109 04:53:13.098620       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 120528896
2023-01-09T04:53:13.175879949Z I0109 04:53:13.175840       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 120840192
2023-01-09T04:53:14.047521721Z I0109 04:53:14.047472       1 request.go:601] Waited for 1.047498629s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:53:15.247173467Z I0109 04:53:15.247137       1 request.go:601] Waited for 1.603337037s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:53:16.247557811Z I0109 04:53:16.247515       1 request.go:601] Waited for 1.796420154s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:53:16.651695616Z I0109 04:53:16.651649       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:53:16.651733210Z I0109 04:53:16.651702       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:16.651733210Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:53:16.651733210Z  CurrentRevision: (int32) 6,
2023-01-09T04:53:16.651733210Z  TargetRevision: (int32) 7,
2023-01-09T04:53:16.651733210Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:16.651733210Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:16.651733210Z  LastFailedReason: (string) "",
2023-01-09T04:53:16.651733210Z  LastFailedCount: (int) 0,
2023-01-09T04:53:16.651733210Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:16.651733210Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:16.651733210Z }
2023-01-09T04:53:16.661984372Z I0109 04:53:16.661935       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 6 to 7 because node ip-10-0-145-4.us-east-2.compute.internal with revision 6 is the oldest
2023-01-09T04:53:16.749202957Z I0109 04:53:16.749163       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 120958976
2023-01-09T04:53:16.749202957Z I0109 04:53:16.749181       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 121151488
2023-01-09T04:53:16.749202957Z I0109 04:53:16.749186       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 120700928
2023-01-09T04:53:17.447616594Z I0109 04:53:17.447578       1 request.go:601] Waited for 1.395682593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:18.647117117Z I0109 04:53:18.647076       1 request.go:601] Waited for 1.395750707s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:53:19.846806597Z I0109 04:53:19.846756       1 request.go:601] Waited for 1.195267023s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:53:20.846939846Z I0109 04:53:20.846898       1 request.go:601] Waited for 1.395255035s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:53:21.847691843Z I0109 04:53:21.847654       1 request.go:601] Waited for 1.396473381s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:53:23.452073392Z I0109 04:53:23.452033       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal with revision 6 is the oldest and needs new revision 7
2023-01-09T04:53:23.452110676Z I0109 04:53:23.452086       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:53:23.452110676Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:53:23.452110676Z  CurrentRevision: (int32) 6,
2023-01-09T04:53:23.452110676Z  TargetRevision: (int32) 7,
2023-01-09T04:53:23.452110676Z  LastFailedRevision: (int32) 0,
2023-01-09T04:53:23.452110676Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:53:23.452110676Z  LastFailedReason: (string) "",
2023-01-09T04:53:23.452110676Z  LastFailedCount: (int) 0,
2023-01-09T04:53:23.452110676Z  LastFallbackCount: (int) 0,
2023-01-09T04:53:23.452110676Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:53:23.452110676Z }
2023-01-09T04:53:24.661858392Z I0109 04:53:24.661809       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-7-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:53:24.850842920Z I0109 04:53:24.850803       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:53:26.452483995Z I0109 04:53:26.452435       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:28.051136319Z I0109 04:53:28.051095       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:53:58.550283646Z I0109 04:53:58.550243       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:54:00.743204152Z I0109 04:54:00.743158       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:08.200872962Z I0109 04:54:08.200831       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:10.731769990Z I0109 04:54:10.731727       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:12.979308205Z I0109 04:54:12.979256       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.1583823467482697 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003921,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.005605,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.007848. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:54:13.566660048Z I0109 04:54:13.566614       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:54:13.575187242Z E0109 04:54:13.575144       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:54:13.576795247Z I0109 04:54:13.576759       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:54:13.592189666Z I0109 04:54:13.592143       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:54:13.592733525Z I0109 04:54:13.592702       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:13.644145941Z I0109 04:54:13.644102       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:14.728546072Z I0109 04:54:14.728508       1 request.go:601] Waited for 1.146172757s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:54:15.927945550Z I0109 04:54:15.927903       1 request.go:601] Waited for 1.397375365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:54:16.531426625Z I0109 04:54:16.531371       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:26.008197906Z I0109 04:54:26.008148       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:27.969777822Z E0109 04:54:27.969739       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-145-4.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.145.4:2379]: context deadline exceeded
2023-01-09T04:54:27.983009107Z I0109 04:54:27.982524       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:54:27.990844572Z I0109 04:54:27.990787       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T04:54:27.993270776Z I0109 04:54:27.993238       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:28.039579611Z I0109 04:54:28.027562       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:54:28.050866379Z I0109 04:54:28.050822       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:28.051127799Z I0109 04:54:28.051093       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T04:54:28.643669252Z E0109 04:54:28.643624       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:54:28.644192086Z I0109 04:54:28.644113       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:54:28.645739086Z E0109 04:54:28.645707       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy
2023-01-09T04:54:29.105856886Z I0109 04:54:29.105785       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:29.155119341Z I0109 04:54:29.153331       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:29.182334569Z I0109 04:54:29.182293       1 request.go:601] Waited for 1.165655305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:54:29.785322766Z I0109 04:54:29.785280       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because waiting for static pod of revision 7, found 6
2023-01-09T04:54:30.381592125Z I0109 04:54:30.381555       1 request.go:601] Waited for 1.396624301s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:54:31.382953372Z I0109 04:54:31.382899       1 request.go:601] Waited for 1.197812803s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:54:32.182802233Z I0109 04:54:32.181592       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:32.182802233Z I0109 04:54:32.181955       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:32.182853864Z I0109 04:54:32.182833       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:32.195783965Z I0109 04:54:32.195628       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:32.675320994Z I0109 04:54:32.667074       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:32.991807207Z I0109 04:54:32.991430       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:33.019523369Z I0109 04:54:33.019469       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:33.181947674Z I0109 04:54:33.181900       1 request.go:601] Waited for 1.005042193s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:54:33.659158479Z I0109 04:54:33.659112       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:34.381629449Z I0109 04:54:34.381591       1 request.go:601] Waited for 1.188338931s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:54:34.701582111Z I0109 04:54:34.701535       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:35.667690848Z I0109 04:54:35.667648       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:35.984198032Z I0109 04:54:35.984158       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:38.184446538Z I0109 04:54:38.184403       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:54:43.677349102Z I0109 04:54:43.677300       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:54:43.677537976Z E0109 04:54:43.677516       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:54:43.680678745Z E0109 04:54:43.680651       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy
2023-01-09T04:54:58.705930918Z E0109 04:54:58.705890       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy
2023-01-09T04:54:58.706476860Z I0109 04:54:58.706436       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:54:58.706711944Z E0109 04:54:58.706691       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:54:58.928175825Z I0109 04:54:58.928125       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:41:34Z","message":"NodeInstallerProgressing: 1 nodes are at revision 6; 2 nodes are at revision 7","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:54:58.936368667Z I0109 04:54:58.936264       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T04:54:58.939073910Z I0109 04:54:58.938950       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:54:58.943082566Z I0109 04:54:58.943054       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 123846656
2023-01-09T04:54:59.028702342Z I0109 04:54:59.028650       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 123666432
2023-01-09T04:55:00.127204508Z I0109 04:55:00.127146       1 request.go:601] Waited for 1.189459392s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:55:00.330376372Z I0109 04:55:00.330335       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 7, but has not made progress because static pod is pending
2023-01-09T04:55:01.127627534Z I0109 04:55:01.127585       1 request.go:601] Waited for 1.398133469s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:02.730337963Z I0109 04:55:02.730294       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:55:02.730337963Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:55:02.730337963Z  CurrentRevision: (int32) 7,
2023-01-09T04:55:02.730337963Z  TargetRevision: (int32) 0,
2023-01-09T04:55:02.730337963Z  LastFailedRevision: (int32) 0,
2023-01-09T04:55:02.730337963Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:55:02.730337963Z  LastFailedReason: (string) "",
2023-01-09T04:55:02.730337963Z  LastFailedCount: (int) 0,
2023-01-09T04:55:02.730337963Z  LastFallbackCount: (int) 0,
2023-01-09T04:55:02.730337963Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:55:02.730337963Z }
2023-01-09T04:55:02.730337963Z  because static pod is ready
2023-01-09T04:55:02.739369269Z I0109 04:55:02.739309       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 6 to 7 because static pod is ready
2023-01-09T04:55:02.742288088Z I0109 04:55:02.742256       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:02.750560699Z I0109 04:55:02.750523       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 6; 2 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T04:55:02.801261624Z W0109 04:55:02.801221       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.199.219:2379]]
2023-01-09T04:55:02.801515335Z W0109 04:55:02.801491       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.199.219:2379]]
2023-01-09T04:55:02.801670573Z W0109 04:55:02.801646       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.199.219:2379]]
2023-01-09T04:55:02.802075408Z I0109 04:55:02.801984       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-endpoints -n openshift-etcd:
2023-01-09T04:55:02.802636498Z W0109 04:55:02.802604       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.199.219:2379]]
2023-01-09T04:55:02.805194344Z W0109 04:55:02.805163       1 etcdcli_pool.go:87] cached client detected change in endpoints [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.18.164:2379 https://10.0.199.219:2379]] vs. [[https://10.0.145.4:2379 https://10.0.160.211:2379 https://10.0.199.219:2379]]
2023-01-09T04:55:02.827345167Z I0109 04:55:02.827303       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 123826176
2023-01-09T04:55:02.827345167Z I0109 04:55:02.827330       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.04 %, dbSize: 123506688
2023-01-09T04:55:02.827345167Z I0109 04:55:02.827338       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 124018688
2023-01-09T04:55:03.327474882Z I0109 04:55:03.327430       1 request.go:601] Waited for 1.053090263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:55:04.526838962Z I0109 04:55:04.526795       1 request.go:601] Waited for 1.785109219s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:55:05.527181863Z I0109 04:55:05.527135       1 request.go:601] Waited for 1.797318023s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:06.726780679Z I0109 04:55:06.726744       1 request.go:601] Waited for 1.396284577s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:07.130697945Z I0109 04:55:07.130656       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:55:07.130697945Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:55:07.130697945Z  CurrentRevision: (int32) 7,
2023-01-09T04:55:07.130697945Z  TargetRevision: (int32) 0,
2023-01-09T04:55:07.130697945Z  LastFailedRevision: (int32) 0,
2023-01-09T04:55:07.130697945Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:55:07.130697945Z  LastFailedReason: (string) "",
2023-01-09T04:55:07.130697945Z  LastFailedCount: (int) 0,
2023-01-09T04:55:07.130697945Z  LastFallbackCount: (int) 0,
2023-01-09T04:55:07.130697945Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:55:07.130697945Z }
2023-01-09T04:55:07.130697945Z  because static pod is ready
2023-01-09T04:55:07.727390797Z I0109 04:55:07.727346       1 request.go:601] Waited for 1.196811026s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:55:12.975092302Z I0109 04:55:12.975047       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.3144029611986987 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003921,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.003682,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.012175. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:55:12.994066535Z I0109 04:55:12.994020       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:12.997227165Z I0109 04:55:12.997191       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T04:55:13.002638344Z I0109 04:55:13.002581       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:55:13.024211856Z I0109 04:55:13.024158       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:13.030498051Z I0109 04:55:13.030382       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:55:13.127965920Z I0109 04:55:13.127924       1 etcdcli_pool.go:157] closing cached client
2023-01-09T04:55:14.127545493Z I0109 04:55:14.127500       1 request.go:601] Waited for 1.133311634s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:55:15.327070194Z I0109 04:55:15.327027       1 request.go:601] Waited for 1.682548217s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:55:15.936423479Z I0109 04:55:15.936364       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-scripts -n openshift-etcd:
2023-01-09T04:55:15.936423479Z cause by changes in data.etcd.env
2023-01-09T04:55:16.327581912Z I0109 04:55:16.327536       1 request.go:601] Waited for 1.798348776s due to client-side throttling, not priority and fairness, request: PUT:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:55:16.334046408Z I0109 04:55:16.333527       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/etcd-pod -n openshift-etcd:
2023-01-09T04:55:16.334046408Z cause by changes in data.pod.yaml
2023-01-09T04:55:16.341404125Z I0109 04:55:16.341362       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 8 triggered by "configmap/etcd-pod has changed"
2023-01-09T04:55:17.527466052Z I0109 04:55:17.527427       1 request.go:601] Waited for 1.395631249s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-7-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:17.933427795Z I0109 04:55:17.933356       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/revision-status-8 -n openshift-etcd because it was missing
2023-01-09T04:55:18.726658325Z I0109 04:55:18.726614       1 request.go:601] Waited for 1.596156419s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:19.335655215Z I0109 04:55:19.335607       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapUpdated' Updated ConfigMap/restore-etcd-pod -n openshift-etcd:
2023-01-09T04:55:19.335655215Z cause by changes in data.pod.yaml
2023-01-09T04:55:19.537400606Z I0109 04:55:19.537330       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-pod-8 -n openshift-etcd because it was missing
2023-01-09T04:55:19.727076022Z I0109 04:55:19.727036       1 request.go:601] Waited for 1.597210571s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:20.727574149Z I0109 04:55:20.727533       1 request.go:601] Waited for 1.340627412s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:55:20.932202159Z I0109 04:55:20.932143       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-serving-ca-8 -n openshift-etcd because it was missing
2023-01-09T04:55:21.927532586Z I0109 04:55:21.927483       1 request.go:601] Waited for 1.195328019s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:55:22.131807538Z I0109 04:55:22.131758       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-peer-client-ca-8 -n openshift-etcd because it was missing
2023-01-09T04:55:22.734015250Z I0109 04:55:22.733947       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-serving-ca-8 -n openshift-etcd because it was missing
2023-01-09T04:55:23.335578247Z I0109 04:55:23.335516       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-metrics-proxy-client-ca-8 -n openshift-etcd because it was missing
2023-01-09T04:55:23.932555377Z I0109 04:55:23.932489       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'ConfigMapCreated' Created ConfigMap/etcd-endpoints-8 -n openshift-etcd because it was missing
2023-01-09T04:55:24.536497435Z I0109 04:55:24.536436       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'SecretCreated' Created Secret/etcd-all-certs-8 -n openshift-etcd because it was missing
2023-01-09T04:55:24.547214766Z I0109 04:55:24.547166       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionCreate' Revision 7 created because configmap/etcd-pod has changed
2023-01-09T04:55:24.554384470Z I0109 04:55:24.554339       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'RevisionTriggered' new revision 8 triggered by "configmap/etcd-pod has changed"
2023-01-09T04:55:24.560650906Z W0109 04:55:24.560615       1 staticpod.go:38] revision 8 is unexpectedly already the latest available revision. This is a possible race!
2023-01-09T04:55:24.569789924Z E0109 04:55:24.569761       1 base_controller.go:272] RevisionController reconciliation failed: conflicting latestAvailableRevision 8
2023-01-09T04:55:24.572171604Z I0109 04:55:24.572130       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"RevisionControllerDegraded: conflicting latestAvailableRevision 8\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:24.585363999Z I0109 04:55:24.585306       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "RevisionControllerDegraded: conflicting latestAvailableRevision 8\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:55:24.594131108Z I0109 04:55:24.594092       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:02Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:24.601613683Z I0109 04:55:24.601503       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "RevisionControllerDegraded: conflicting latestAvailableRevision 8\nNodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:55:25.727208278Z I0109 04:55:25.727163       1 request.go:601] Waited for 1.17868218s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:26.143077205Z I0109 04:55:26.143031       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:55:26.150896789Z I0109 04:55:26.150855       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing changed from False to True ("NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:55:26.727407846Z I0109 04:55:26.727368       1 request.go:601] Waited for 1.39201816s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:55:27.135235021Z I0109 04:55:27.135186       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-ip-10-0-199-219.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:55:27.927347222Z I0109 04:55:27.927305       1 request.go:601] Waited for 1.394949423s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:29.127386220Z I0109 04:55:29.127343       1 request.go:601] Waited for 1.795473593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:55:30.326689676Z I0109 04:55:30.326654       1 request.go:601] Waited for 1.596462266s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2023-01-09T04:55:30.339466826Z I0109 04:55:30.339409       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:55:31.326922781Z I0109 04:55:31.326876       1 request.go:601] Waited for 1.596751828s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:32.327555748Z I0109 04:55:32.327516       1 request.go:601] Waited for 1.386748429s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:33.134260798Z I0109 04:55:33.134202       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/revision-pruner-8-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:55:33.527491466Z I0109 04:55:33.527444       1 request.go:601] Waited for 1.397825378s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:55:34.730304246Z I0109 04:55:34.730261       1 installer_controller.go:524] node ip-10-0-199-219.us-east-2.compute.internal with revision 7 is the oldest and needs new revision 8
2023-01-09T04:55:34.730345733Z I0109 04:55:34.730307       1 installer_controller.go:532] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:55:34.730345733Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:55:34.730345733Z  CurrentRevision: (int32) 7,
2023-01-09T04:55:34.730345733Z  TargetRevision: (int32) 8,
2023-01-09T04:55:34.730345733Z  LastFailedRevision: (int32) 0,
2023-01-09T04:55:34.730345733Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:55:34.730345733Z  LastFailedReason: (string) "",
2023-01-09T04:55:34.730345733Z  LastFailedCount: (int) 0,
2023-01-09T04:55:34.730345733Z  LastFallbackCount: (int) 0,
2023-01-09T04:55:34.730345733Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:55:34.730345733Z }
2023-01-09T04:55:34.740232754Z I0109 04:55:34.740176       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-199-219.us-east-2.compute.internal" from revision 7 to 8 because node ip-10-0-199-219.us-east-2.compute.internal with revision 7 is the oldest
2023-01-09T04:55:35.927600937Z I0109 04:55:35.927561       1 request.go:601] Waited for 1.186653741s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:55:37.126732418Z I0109 04:55:37.126689       1 request.go:601] Waited for 1.796327996s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:55:37.731696832Z I0109 04:55:37.731642       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-ip-10-0-199-219.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:55:38.127309204Z I0109 04:55:38.127269       1 request.go:601] Waited for 1.595498342s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:55:39.326694496Z I0109 04:55:39.326655       1 request.go:601] Waited for 1.595063775s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:55:39.329359906Z I0109 04:55:39.329334       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:55:40.327173779Z I0109 04:55:40.327139       1 request.go:601] Waited for 1.396931878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:41.526764083Z I0109 04:55:41.526724       1 request.go:601] Waited for 1.196333057s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:55:41.730215512Z I0109 04:55:41.730177       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:55:42.930273737Z I0109 04:55:42.930236       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:56:11.749484389Z I0109 04:56:11.749442       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:56:12.984859952Z I0109 04:56:12.984817       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.0309255436301727 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.005280,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.004163,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.012175. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:56:15.740495798Z I0109 04:56:15.740452       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:56:24.597108176Z I0109 04:56:24.597053       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:24.609871800Z I0109 04:56:24.609833       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:56:26.456288180Z I0109 04:56:26.456202       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:26.766948432Z I0109 04:56:26.766901       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:26.777627696Z E0109 04:56:26.777596       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:56:26.780337853Z I0109 04:56:26.780295       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:56:26.788707400Z I0109 04:56:26.788615       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:56:26.789278204Z I0109 04:56:26.789248       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:27.798110140Z I0109 04:56:27.798069       1 request.go:601] Waited for 1.003462076s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:56:27.975898754Z E0109 04:56:27.975859       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-199-219.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.199.219:2379]: context deadline exceeded
2023-01-09T04:56:27.990751481Z I0109 04:56:27.990701       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:56:28.001881034Z I0109 04:56:28.001835       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:56:28.002412795Z I0109 04:56:28.002371       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:28.018550440Z I0109 04:56:28.018481       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:56:28.028590338Z I0109 04:56:28.028529       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:28.032313528Z I0109 04:56:28.032266       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:56:28.406417141Z I0109 04:56:28.406347       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:56:28.997470551Z I0109 04:56:28.997418       1 request.go:601] Waited for 1.008017888s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:56:29.998024931Z I0109 04:56:29.997959       1 request.go:601] Waited for 1.396766326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:31.197567186Z I0109 04:56:31.197527       1 request.go:601] Waited for 1.396182467s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:32.197761794Z I0109 04:56:32.197726       1 request.go:601] Waited for 1.197297468s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T04:56:32.403192567Z I0109 04:56:32.403151       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:56:41.815355883Z I0109 04:56:41.815301       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:41.815555792Z E0109 04:56:41.815536       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:56:41.818503288Z E0109 04:56:41.818464       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:56:48.156439707Z I0109 04:56:48.156393       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:48.164764671Z I0109 04:56:48.164722       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:48.173715979Z I0109 04:56:48.173680       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:56:48.177978343Z I0109 04:56:48.177944       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:48.203194511Z I0109 04:56:48.203131       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:48.845809160Z I0109 04:56:48.845763       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:49.356610997Z I0109 04:56:49.356571       1 request.go:601] Waited for 1.169796343s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:49.852372318Z I0109 04:56:49.852243       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:50.356630252Z I0109 04:56:50.356587       1 request.go:601] Waited for 1.194255275s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:56:50.855721815Z I0109 04:56:50.855676       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:50.960598047Z I0109 04:56:50.960557       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:56:51.357180818Z I0109 04:56:51.357144       1 request.go:601] Waited for 1.194992808s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:56:51.862156912Z I0109 04:56:51.862118       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:56:52.557105472Z I0109 04:56:52.557063       1 request.go:601] Waited for 1.194517626s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:56:54.363889172Z I0109 04:56:54.363842       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:56:55.962399237Z I0109 04:56:55.962357       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:56:56.851816035Z I0109 04:56:56.851767       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:56:56.851999875Z E0109 04:56:56.851971       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:56:56.852269702Z E0109 04:56:56.852254       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:57:11.885050205Z I0109 04:57:11.884998       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:57:11.885182872Z E0109 04:57:11.885165       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:57:11.885818892Z E0109 04:57:11.885793       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy
2023-01-09T04:57:12.119226052Z I0109 04:57:12.118926       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:57:12.131083520Z I0109 04:57:12.131030       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:57:12.131539391Z I0109 04:57:12.131499       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy"
2023-01-09T04:57:12.132627066Z I0109 04:57:12.132597       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 127418368
2023-01-09T04:57:12.209622851Z I0109 04:57:12.209587       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 127438848
2023-01-09T04:57:12.209622851Z I0109 04:57:12.209606       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 127774720
2023-01-09T04:57:12.982677374Z I0109 04:57:12.982626       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.140836111111111 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003968,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.003997,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.011108. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:57:12.993974502Z I0109 04:57:12.993929       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:57:13.012220631Z I0109 04:57:13.012170       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:57:13.012220631Z I0109 04:57:13.012189       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:57:13.026759995Z I0109 04:57:13.026720       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:57:13.033917917Z I0109 04:57:13.033845       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-199-219.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:57:13.035167804Z I0109 04:57:13.035139       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:57:13.075291901Z I0109 04:57:13.075247       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 127459328
2023-01-09T04:57:13.075291901Z I0109 04:57:13.075264       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 127774720
2023-01-09T04:57:13.721356221Z I0109 04:57:13.721312       1 installer_controller.go:512] "ip-10-0-199-219.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:57:14.118187652Z I0109 04:57:14.118151       1 request.go:601] Waited for 1.125504685s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:57:15.317942273Z I0109 04:57:15.317902       1 request.go:601] Waited for 1.695907893s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:57:16.518176804Z I0109 04:57:16.518139       1 request.go:601] Waited for 1.796681558s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:57:17.717529082Z I0109 04:57:17.717488       1 request.go:601] Waited for 1.594834889s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:57:18.721812038Z I0109 04:57:18.721771       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:57:18.721812038Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:57:18.721812038Z  CurrentRevision: (int32) 8,
2023-01-09T04:57:18.721812038Z  TargetRevision: (int32) 0,
2023-01-09T04:57:18.721812038Z  LastFailedRevision: (int32) 0,
2023-01-09T04:57:18.721812038Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:57:18.721812038Z  LastFailedReason: (string) "",
2023-01-09T04:57:18.721812038Z  LastFailedCount: (int) 0,
2023-01-09T04:57:18.721812038Z  LastFallbackCount: (int) 0,
2023-01-09T04:57:18.721812038Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:57:18.721812038Z }
2023-01-09T04:57:18.721812038Z  because static pod is ready
2023-01-09T04:57:18.732431632Z I0109 04:57:18.732372       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-199-219.us-east-2.compute.internal" from revision 7 to 8 because static pod is ready
2023-01-09T04:57:18.734025736Z I0109 04:57:18.733977       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:57:18.758658316Z I0109 04:57:18.755132       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 3 nodes are at revision 7; 0 nodes have achieved new revision 8" to "NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 7; 0 nodes have achieved new revision 8\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:57:18.822952967Z I0109 04:57:18.822913       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 127700992
2023-01-09T04:57:18.822952967Z I0109 04:57:18.822933       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.04 %, dbSize: 128180224
2023-01-09T04:57:18.822952967Z I0109 04:57:18.822939       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 128053248
2023-01-09T04:57:18.917364088Z I0109 04:57:18.917321       1 request.go:601] Waited for 1.396386323s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:57:19.917466271Z I0109 04:57:19.917427       1 request.go:601] Waited for 1.397256536s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:57:20.918252484Z I0109 04:57:20.918213       1 request.go:601] Waited for 1.797457811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:57:22.118232753Z I0109 04:57:22.118191       1 request.go:601] Waited for 1.597431342s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:57:23.318300827Z I0109 04:57:23.318257       1 request.go:601] Waited for 1.39736834s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:57:23.322668395Z I0109 04:57:23.322626       1 installer_controller.go:500] "ip-10-0-199-219.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:57:23.322668395Z  NodeName: (string) (len=42) "ip-10-0-199-219.us-east-2.compute.internal",
2023-01-09T04:57:23.322668395Z  CurrentRevision: (int32) 8,
2023-01-09T04:57:23.322668395Z  TargetRevision: (int32) 0,
2023-01-09T04:57:23.322668395Z  LastFailedRevision: (int32) 0,
2023-01-09T04:57:23.322668395Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:57:23.322668395Z  LastFailedReason: (string) "",
2023-01-09T04:57:23.322668395Z  LastFailedCount: (int) 0,
2023-01-09T04:57:23.322668395Z  LastFallbackCount: (int) 0,
2023-01-09T04:57:23.322668395Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:57:23.322668395Z }
2023-01-09T04:57:23.322668395Z  because static pod is ready
2023-01-09T04:57:24.518166754Z I0109 04:57:24.518123       1 request.go:601] Waited for 1.194775665s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:57:27.720888583Z I0109 04:57:27.720850       1 installer_controller.go:524] node ip-10-0-160-211.us-east-2.compute.internal with revision 7 is the oldest and needs new revision 8
2023-01-09T04:57:27.720947044Z I0109 04:57:27.720892       1 installer_controller.go:532] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:57:27.720947044Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:57:27.720947044Z  CurrentRevision: (int32) 7,
2023-01-09T04:57:27.720947044Z  TargetRevision: (int32) 8,
2023-01-09T04:57:27.720947044Z  LastFailedRevision: (int32) 0,
2023-01-09T04:57:27.720947044Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:57:27.720947044Z  LastFailedReason: (string) "",
2023-01-09T04:57:27.720947044Z  LastFailedCount: (int) 0,
2023-01-09T04:57:27.720947044Z  LastFallbackCount: (int) 0,
2023-01-09T04:57:27.720947044Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:57:27.720947044Z }
2023-01-09T04:57:27.732111633Z I0109 04:57:27.732051       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-160-211.us-east-2.compute.internal" from revision 7 to 8 because node ip-10-0-160-211.us-east-2.compute.internal with revision 7 is the oldest
2023-01-09T04:57:27.814174114Z I0109 04:57:27.814135       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 128311296
2023-01-09T04:57:27.814174114Z I0109 04:57:27.814151       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 128233472
2023-01-09T04:57:28.917640676Z I0109 04:57:28.917596       1 request.go:601] Waited for 1.183917804s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:57:29.918236503Z I0109 04:57:29.918196       1 request.go:601] Waited for 1.198044238s due to client-side throttling, not priority and fairness, request: POST:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods
2023-01-09T04:57:29.929493833Z I0109 04:57:29.929449       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-ip-10-0-160-211.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:57:30.920900175Z I0109 04:57:30.920853       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:57:31.117962538Z I0109 04:57:31.117923       1 request.go:601] Waited for 1.187766164s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:57:32.117972307Z I0109 04:57:32.117927       1 request.go:601] Waited for 1.192261456s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:57:33.121947061Z I0109 04:57:33.121904       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:57:34.521396581Z I0109 04:57:34.521354       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:58:03.739055265Z I0109 04:58:03.739007       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:58:05.932397663Z I0109 04:58:05.932354       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:58:12.979903744Z I0109 04:58:12.979851       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.0309255436301736 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003975,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.003987,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.013595. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:58:15.292047421Z I0109 04:58:15.292001       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:15.453865185Z I0109 04:58:15.453806       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:58:18.763326713Z I0109 04:58:18.763280       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:18.777345607Z E0109 04:58:18.777307       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:58:18.778399148Z I0109 04:58:18.778362       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:58:18.792097770Z I0109 04:58:18.789893       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:58:18.792869711Z I0109 04:58:18.792835       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:19.849441108Z I0109 04:58:19.849406       1 request.go:601] Waited for 1.070549259s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:21.049360392Z I0109 04:58:21.049316       1 request.go:601] Waited for 1.195611714s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:22.054142898Z I0109 04:58:22.054099       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:58:27.978441727Z E0109 04:58:27.978403       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-160-211.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.160.211:2379]: context deadline exceeded
2023-01-09T04:58:27.991330265Z I0109 04:58:27.991281       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:58:28.000570810Z I0109 04:58:28.000531       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:58:28.002034357Z I0109 04:58:28.002003       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:28.023063007Z I0109 04:58:28.023021       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:58:28.031875046Z I0109 04:58:28.031826       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:58:28.034374601Z I0109 04:58:28.032881       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:28.995766017Z I0109 04:58:28.995723       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:58:29.190681242Z I0109 04:58:29.190640       1 request.go:601] Waited for 1.168322701s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T04:58:30.390458712Z I0109 04:58:30.390419       1 request.go:601] Waited for 1.393118878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:31.590729682Z I0109 04:58:31.590686       1 request.go:601] Waited for 1.195908053s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:32.595279015Z I0109 04:58:32.595243       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T04:58:33.816480995Z I0109 04:58:33.816433       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:33.816753481Z E0109 04:58:33.816732       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:58:33.818822613Z E0109 04:58:33.818794       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:58:43.344086396Z I0109 04:58:43.344041       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:43.354030133Z I0109 04:58:43.353982       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:43.361972711Z I0109 04:58:43.361924       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:43.381427472Z I0109 04:58:43.381385       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:43.748489991Z I0109 04:58:43.748448       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:58:43.883513912Z I0109 04:58:43.883474       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:44.543660122Z I0109 04:58:44.543618       1 request.go:601] Waited for 1.172381362s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:58:44.890808610Z I0109 04:58:44.890770       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:45.543685298Z I0109 04:58:45.543650       1 request.go:601] Waited for 1.195281324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:58:45.894612740Z I0109 04:58:45.894535       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:46.544326254Z I0109 04:58:46.544290       1 request.go:601] Waited for 1.193776009s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:58:46.908910377Z I0109 04:58:46.908869       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:58:47.348647527Z I0109 04:58:47.348609       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:58:47.743561926Z I0109 04:58:47.743520       1 request.go:601] Waited for 1.184419095s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T04:58:48.857344500Z I0109 04:58:48.857277       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:58:48.857502029Z E0109 04:58:48.857474       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:58:48.859568362Z E0109 04:58:48.859535       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:58:49.948406344Z I0109 04:58:49.948361       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:59:03.896817008Z I0109 04:59:03.896768       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:59:03.896922856Z E0109 04:59:03.896906       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T04:59:03.899039925Z E0109 04:59:03.899012       1 base_controller.go:272] DefragController reconciliation failed: cluster is unhealthy: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy
2023-01-09T04:59:08.870294478Z I0109 04:59:08.870255       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:08.878502004Z I0109 04:59:08.878455       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy"
2023-01-09T04:59:08.884661683Z I0109 04:59:08.884587       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T04:59:10.069703568Z I0109 04:59:10.069660       1 request.go:601] Waited for 1.184009447s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:59:10.473888240Z I0109 04:59:10.473849       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:59:11.070042303Z I0109 04:59:11.069964       1 request.go:601] Waited for 1.195482928s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:59:12.988845136Z I0109 04:59:12.988799       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.2222222222222223 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003997,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.003908,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.010184. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T04:59:13.022918646Z I0109 04:59:13.022870       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:13.036068961Z I0109 04:59:13.036022       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T04:59:13.055210851Z I0109 04:59:13.055170       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:13.067366297Z E0109 04:59:13.067330       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:59:13.072871693Z I0109 04:59:13.072849       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:13.110101641Z I0109 04:59:13.110062       1 installer_controller.go:512] "ip-10-0-160-211.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T04:59:13.122746125Z E0109 04:59:13.122714       1 base_controller.go:272] StatusSyncer_etcd reconciliation failed: Operation cannot be fulfilled on clusteroperators.config.openshift.io "etcd": the object has been modified; please apply your changes to the latest version and try again
2023-01-09T04:59:13.127823452Z I0109 04:59:13.127792       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:13.139526420Z I0109 04:59:13.139043       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-160-211.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:59:13.176153457Z I0109 04:59:13.176112       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 131796992
2023-01-09T04:59:13.176153457Z I0109 04:59:13.176129       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 131547136
2023-01-09T04:59:13.257177144Z I0109 04:59:13.257132       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 131809280
2023-01-09T04:59:13.257177144Z I0109 04:59:13.257150       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.01 %, dbSize: 131547136
2023-01-09T04:59:14.070119109Z I0109 04:59:14.070082       1 request.go:601] Waited for 1.048577365s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:59:15.270177949Z I0109 04:59:15.270139       1 request.go:601] Waited for 1.795674313s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T04:59:16.470229822Z I0109 04:59:16.470168       1 request.go:601] Waited for 1.796352308s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:59:17.669600281Z I0109 04:59:17.669552       1 request.go:601] Waited for 1.595405073s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:59:18.073667179Z I0109 04:59:18.073624       1 installer_controller.go:500] "ip-10-0-160-211.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:59:18.073667179Z  NodeName: (string) (len=42) "ip-10-0-160-211.us-east-2.compute.internal",
2023-01-09T04:59:18.073667179Z  CurrentRevision: (int32) 8,
2023-01-09T04:59:18.073667179Z  TargetRevision: (int32) 0,
2023-01-09T04:59:18.073667179Z  LastFailedRevision: (int32) 0,
2023-01-09T04:59:18.073667179Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:59:18.073667179Z  LastFailedReason: (string) "",
2023-01-09T04:59:18.073667179Z  LastFailedCount: (int) 0,
2023-01-09T04:59:18.073667179Z  LastFallbackCount: (int) 0,
2023-01-09T04:59:18.073667179Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:59:18.073667179Z }
2023-01-09T04:59:18.073667179Z  because static pod is ready
2023-01-09T04:59:18.085578722Z I0109 04:59:18.085517       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-160-211.us-east-2.compute.internal" from revision 7 to 8 because static pod is ready
2023-01-09T04:59:18.086907885Z I0109 04:59:18.086869       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T04:59:18.096037731Z I0109 04:59:18.095961       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing message changed from "NodeInstallerProgressing: 2 nodes are at revision 7; 1 nodes are at revision 8" to "NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8",Available message changed from "StaticPodsAvailable: 3 nodes are active; 2 nodes are at revision 7; 1 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T04:59:18.174722703Z I0109 04:59:18.174680       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.02 %, dbSize: 132022272
2023-01-09T04:59:18.174722703Z I0109 04:59:18.174698       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 131772416
2023-01-09T04:59:18.670354106Z I0109 04:59:18.670311       1 request.go:601] Waited for 1.396196061s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T04:59:19.869647429Z I0109 04:59:19.869606       1 request.go:601] Waited for 1.596019492s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:59:21.070036349Z I0109 04:59:21.069971       1 request.go:601] Waited for 1.395387928s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:59:22.070178429Z I0109 04:59:22.070136       1 request.go:601] Waited for 1.396663192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T04:59:23.270328828Z I0109 04:59:23.270288       1 request.go:601] Waited for 1.195829884s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T04:59:24.470269295Z I0109 04:59:24.470230       1 request.go:601] Waited for 1.196150624s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T04:59:25.479184451Z I0109 04:59:25.479117       1 request.go:601] Waited for 1.203036383s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:59:26.075348021Z I0109 04:59:26.075306       1 installer_controller.go:524] node ip-10-0-145-4.us-east-2.compute.internal with revision 7 is the oldest and needs new revision 8
2023-01-09T04:59:26.075394017Z I0109 04:59:26.075348       1 installer_controller.go:532] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T04:59:26.075394017Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T04:59:26.075394017Z  CurrentRevision: (int32) 7,
2023-01-09T04:59:26.075394017Z  TargetRevision: (int32) 8,
2023-01-09T04:59:26.075394017Z  LastFailedRevision: (int32) 0,
2023-01-09T04:59:26.075394017Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T04:59:26.075394017Z  LastFailedReason: (string) "",
2023-01-09T04:59:26.075394017Z  LastFailedCount: (int) 0,
2023-01-09T04:59:26.075394017Z  LastFallbackCount: (int) 0,
2023-01-09T04:59:26.075394017Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T04:59:26.075394017Z }
2023-01-09T04:59:26.104613392Z I0109 04:59:26.103968       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeTargetRevisionChanged' Updating node "ip-10-0-145-4.us-east-2.compute.internal" from revision 7 to 8 because node ip-10-0-145-4.us-east-2.compute.internal with revision 7 is the oldest
2023-01-09T04:59:26.208251953Z I0109 04:59:26.208210       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 57.69 %, dbSize: 133799936
2023-01-09T04:59:26.208341761Z I0109 04:59:26.208316       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-160-211.us-east-2.compute.internal, memberID: 4cfac3d64d9ed566, dbSize: 133799936, dbInUse: 56606720, leader ID: 12187074142216075386
2023-01-09T04:59:26.654404038Z I0109 04:59:26.654361       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-160-211.us-east-2.compute.internal, memberID: 5546961216252859750
2023-01-09T04:59:27.279462403Z I0109 04:59:27.279417       1 request.go:601] Waited for 1.184666722s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T04:59:28.470035179Z I0109 04:59:28.469952       1 request.go:601] Waited for 1.169935483s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T04:59:28.683318196Z I0109 04:59:28.683264       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'PodCreated' Created Pod/installer-8-ip-10-0-145-4.us-east-2.compute.internal -n openshift-etcd because it was missing
2023-01-09T04:59:29.869470462Z I0109 04:59:29.869430       1 request.go:601] Waited for 1.186175796s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T04:59:29.873270555Z I0109 04:59:29.873236       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Pending phase
2023-01-09T04:59:31.674935462Z I0109 04:59:31.674892       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T04:59:32.874451501Z I0109 04:59:32.874413       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T05:00:02.617257850Z I0109 05:00:02.617219       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because installer is not finished, but in Running phase
2023-01-09T05:00:04.916437888Z I0109 05:00:04.916398       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T05:00:12.984592399Z I0109 05:00:12.984538       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 3.0544506740154485 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.004367,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.004174,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.007962. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T05:00:13.200305676Z I0109 05:00:13.200267       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:13.210074356Z I0109 05:00:13.210037       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T05:00:13.646762085Z I0109 05:00:13.646722       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:17.635275610Z I0109 05:00:17.635168       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:00:17.644067834Z E0109 05:00:17.644037       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T05:00:17.645727549Z I0109 05:00:17.645696       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:00:17.653559658Z I0109 05:00:17.653524       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T05:00:17.656086506Z I0109 05:00:17.656057       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:18.800339450Z I0109 05:00:18.800301       1 request.go:601] Waited for 1.150163318s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/revision-pruner-8-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:00:19.603520265Z I0109 05:00:19.603485       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T05:00:19.679932716Z W0109 05:00:19.679891       1 defragcontroller.go:202] cluster is unhealthy: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy
2023-01-09T05:00:27.970598409Z E0109 05:00:27.970545       1 etcdmemberscontroller.go:73] Unhealthy etcd member found: ip-10-0-145-4.us-east-2.compute.internal, took=, err=create client failure: failed to make etcd client for endpoints [https://10.0.145.4:2379]: context deadline exceeded
2023-01-09T05:00:27.985413065Z I0109 05:00:27.985358       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:00:27.997224565Z I0109 05:00:27.997164       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: No unhealthy members found" to "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T05:00:27.997328072Z I0109 05:00:27.997297       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:28.022441532Z I0109 05:00:28.022402       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:00:28.029938981Z I0109 05:00:28.029902       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:28.030593689Z I0109 05:00:28.030547       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available" to "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T05:00:29.182852088Z I0109 05:00:29.182806       1 request.go:601] Waited for 1.16115532s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T05:00:29.786586409Z I0109 05:00:29.786550       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T05:00:30.383180296Z I0109 05:00:30.383143       1 request.go:601] Waited for 1.397220384s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T05:00:31.383468164Z I0109 05:00:31.383428       1 request.go:601] Waited for 1.19814662s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:00:32.675059476Z I0109 05:00:32.674937       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:00:32.675268289Z E0109 05:00:32.675248       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T05:00:32.986745415Z I0109 05:00:32.986698       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because waiting for static pod of revision 8, found 7
2023-01-09T05:00:39.175604315Z I0109 05:00:39.175560       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:39.181970876Z I0109 05:00:39.181931       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:39.202517185Z I0109 05:00:39.202435       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:39.203156976Z I0109 05:00:39.203055       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:00:39.213458100Z I0109 05:00:39.213414       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:39.729293571Z I0109 05:00:39.729243       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:40.376177334Z I0109 05:00:40.376141       1 request.go:601] Waited for 1.170273504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T05:00:40.733586858Z I0109 05:00:40.733536       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:41.376400978Z I0109 05:00:41.376353       1 request.go:601] Waited for 1.197314212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:00:41.734820472Z I0109 05:00:41.734777       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:42.575807508Z I0109 05:00:42.575769       1 request.go:601] Waited for 1.196819575s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:00:42.578775621Z I0109 05:00:42.578740       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:00:42.744142234Z I0109 05:00:42.744095       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:00:43.575928502Z I0109 05:00:43.575874       1 request.go:601] Waited for 1.194431226s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T05:00:45.579799477Z I0109 05:00:45.579754       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:00:46.979454467Z I0109 05:00:46.979408       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:00:47.701244869Z I0109 05:00:47.701202       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:00:47.701373567Z E0109 05:00:47.701357       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T05:00:51.991471266Z I0109 05:00:51.991426       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:01:02.724536933Z I0109 05:01:02.724489       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'UnhealthyEtcdMember' unhealthy members: ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:01:02.724623893Z E0109 05:01:02.724605       1 base_controller.go:272] ClusterMemberController reconciliation failed: unhealthy members found during reconciling members
2023-01-09T05:01:03.934214039Z I0109 05:01:03.934170       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T04:55:26Z","message":"NodeInstallerProgressing: 1 nodes are at revision 7; 2 nodes are at revision 8","reason":"NodeInstaller","status":"True","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:01:03.953922866Z I0109 05:01:03.953792       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nClusterMemberControllerDegraded: unhealthy members found during reconciling members\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T05:01:03.954363038Z I0109 05:01:03.954338       1 quorumguardcleanupcontroller.go:134] 2/3 guard pods ready. Waiting until all new guard pods are ready
2023-01-09T05:01:05.334183898Z I0109 05:01:05.334145       1 request.go:601] Waited for 1.096420693s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T05:01:05.938139533Z I0109 05:01:05.938097       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:01:06.232128870Z I0109 05:01:06.232078       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 57.82 %, dbSize: 134139904
2023-01-09T05:01:06.232311365Z I0109 05:01:06.232274       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-199-219.us-east-2.compute.internal, memberID: cebc4266beb3f2a1, dbSize: 134139904, dbInUse: 56582144, leader ID: 12187074142216075386
2023-01-09T05:01:06.686561290Z I0109 05:01:06.686506       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-199-219.us-east-2.compute.internal, memberID: 14896854676488319649
2023-01-09T05:01:07.937600247Z I0109 05:01:07.937558       1 installer_controller.go:512] "ip-10-0-145-4.us-east-2.compute.internal" is in transition to 8, but has not made progress because static pod is pending
2023-01-09T05:01:09.736770439Z I0109 05:01:09.736728       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T05:01:09.736770439Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T05:01:09.736770439Z  CurrentRevision: (int32) 8,
2023-01-09T05:01:09.736770439Z  TargetRevision: (int32) 0,
2023-01-09T05:01:09.736770439Z  LastFailedRevision: (int32) 0,
2023-01-09T05:01:09.736770439Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T05:01:09.736770439Z  LastFailedReason: (string) "",
2023-01-09T05:01:09.736770439Z  LastFailedCount: (int) 0,
2023-01-09T05:01:09.736770439Z  LastFallbackCount: (int) 0,
2023-01-09T05:01:09.736770439Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T05:01:09.736770439Z }
2023-01-09T05:01:09.736770439Z  because static pod is ready
2023-01-09T05:01:09.745984664Z I0109 05:01:09.745927       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'NodeCurrentRevisionChanged' Updated node "ip-10-0-145-4.us-east-2.compute.internal" from revision 7 to 8 because static pod is ready
2023-01-09T05:01:09.750326571Z I0109 05:01:09.750291       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T05:01:09Z","message":"NodeInstallerProgressing: 3 nodes are at revision 8\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:01:09.762356154Z I0109 05:01:09.762304       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Progressing changed from True to False ("NodeInstallerProgressing: 3 nodes are at revision 8\nEtcdMembersProgressing: No unstarted etcd members found"),Available message changed from "StaticPodsAvailable: 3 nodes are active; 1 nodes are at revision 7; 2 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy"
2023-01-09T05:01:10.334176128Z I0109 05:01:10.334132       1 request.go:601] Waited for 1.046999015s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T05:01:11.534106530Z I0109 05:01:11.534066       1 request.go:601] Waited for 1.784542268s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T05:01:12.534417803Z I0109 05:01:12.534376       1 request.go:601] Waited for 1.59812536s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/installer-8-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:01:12.979238614Z I0109 05:01:12.979195       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.1815355002756096 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.003995,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.004593,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.007439. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T05:01:13.008104609Z I0109 05:01:13.008062       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T05:01:09Z","message":"NodeInstallerProgressing: 3 nodes are at revision 8\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:01:13.021738134Z I0109 05:01:13.021693       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Degraded message changed from "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found"
2023-01-09T05:01:13.025280895Z I0109 05:01:13.025233       1 status_controller.go:211] clusteroperator/etcd diff {"status":{"conditions":[{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"The etcd backup controller is starting, and will decide if recent backups are available or if a backup is required","reason":"ControllerStarted","status":"Unknown","type":"RecentBackup"},{"lastTransitionTime":"2023-01-09T04:49:14Z","message":"NodeControllerDegraded: All master nodes are ready\nEtcdMembersDegraded: No unhealthy members found","reason":"AsExpected","status":"False","type":"Degraded"},{"lastTransitionTime":"2023-01-09T05:01:09Z","message":"NodeInstallerProgressing: 3 nodes are at revision 8\nEtcdMembersProgressing: No unstarted etcd members found","reason":"AsExpected","status":"False","type":"Progressing"},{"lastTransitionTime":"2023-01-09T04:43:40Z","message":"StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available","reason":"AsExpected","status":"True","type":"Available"},{"lastTransitionTime":"2023-01-09T04:41:03Z","message":"All is well","reason":"AsExpected","status":"True","type":"Upgradeable"}]}}
2023-01-09T05:01:13.036450704Z I0109 05:01:13.036379       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'OperatorStatusChanged' Status for clusteroperator/etcd changed: Available message changed from "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 2 of 3 members are available, ip-10-0-145-4.us-east-2.compute.internal is unhealthy" to "StaticPodsAvailable: 3 nodes are active; 3 nodes are at revision 8\nEtcdMembersAvailable: 3 members are available"
2023-01-09T05:01:13.734615429Z I0109 05:01:13.734578       1 request.go:601] Waited for 1.39693751s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:01:13.937276706Z I0109 05:01:13.937229       1 installer_controller.go:500] "ip-10-0-145-4.us-east-2.compute.internal" moving to (v1.NodeStatus) {
2023-01-09T05:01:13.937276706Z  NodeName: (string) (len=40) "ip-10-0-145-4.us-east-2.compute.internal",
2023-01-09T05:01:13.937276706Z  CurrentRevision: (int32) 8,
2023-01-09T05:01:13.937276706Z  TargetRevision: (int32) 0,
2023-01-09T05:01:13.937276706Z  LastFailedRevision: (int32) 0,
2023-01-09T05:01:13.937276706Z  LastFailedTime: (*v1.Time)(<nil>),
2023-01-09T05:01:13.937276706Z  LastFailedReason: (string) "",
2023-01-09T05:01:13.937276706Z  LastFailedCount: (int) 0,
2023-01-09T05:01:13.937276706Z  LastFallbackCount: (int) 0,
2023-01-09T05:01:13.937276706Z  LastFailedRevisionErrors: ([]string) <nil>
2023-01-09T05:01:13.937276706Z }
2023-01-09T05:01:13.937276706Z  because static pod is ready
2023-01-09T05:01:14.934623837Z I0109 05:01:14.934583       1 request.go:601] Waited for 1.596945811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T05:01:16.134633779Z I0109 05:01:16.134584       1 request.go:601] Waited for 1.79559322s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T05:01:17.334172583Z I0109 05:01:17.334130       1 request.go:601] Waited for 1.597774239s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:01:18.334405984Z I0109 05:01:18.334363       1 request.go:601] Waited for 1.393374683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:01:19.534136919Z I0109 05:01:19.534090       1 request.go:601] Waited for 1.194978667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:01:44.725859973Z I0109 05:01:44.725825       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 57.50 %, dbSize: 133242880
2023-01-09T05:01:44.725938934Z I0109 05:01:44.725914       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentAttempt' Attempting defrag on member: ip-10-0-145-4.us-east-2.compute.internal, memberID: a9212f5cca23387a, dbSize: 133242880, dbInUse: 56623104, leader ID: 12187074142216075386
2023-01-09T05:01:45.185832299Z I0109 05:01:45.185776       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Normal' reason: 'DefragControllerDefragmentSuccess' etcd member has been defragmented: ip-10-0-145-4.us-east-2.compute.internal, memberID: 12187074142216075386
2023-01-09T05:02:12.989063242Z I0109 05:02:12.989011       1 event.go:285] Event(v1.ObjectReference{Kind:"Deployment", Namespace:"openshift-etcd-operator", Name:"etcd-operator", UID:"d39bb38e-c33a-46b3-9e3d-2ea15c94dde1", APIVersion:"apps/v1", ResourceVersion:"", FieldPath:""}): type: 'Warning' reason: 'EtcdLeaderChangeMetrics' Detected leader change increase of 2.1055882311655134 over 5 minutes on "AWS"; disk metrics are: etcd-ip-10-0-145-4.us-east-2.compute.internal=0.004472,etcd-ip-10-0-160-211.us-east-2.compute.internal=0.003986,etcd-ip-10-0-199-219.us-east-2.compute.internal=0.007979. Most often this is as a result of inadequate storage or sometimes due to networking issues.
2023-01-09T05:02:23.300386258Z I0109 05:02:23.300347       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.23 %, dbSize: 61079552
2023-01-09T05:02:59.361300300Z I0109 05:02:59.361261       1 request.go:601] Waited for 1.074549979s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:03:00.361732130Z I0109 05:03:00.361691       1 request.go:601] Waited for 1.392193657s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T05:03:01.361825460Z I0109 05:03:01.361785       1 request.go:601] Waited for 1.35920435s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T05:03:02.563187605Z I0109 05:03:02.563152       1 request.go:601] Waited for 1.198452072s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T05:03:03.761381953Z I0109 05:03:03.761341       1 request.go:601] Waited for 1.153442116s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T05:04:13.107240945Z I0109 05:04:13.107204       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 0.05 %, dbSize: 63406080
2023-01-09T05:04:13.107240945Z I0109 05:04:13.107220       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 0.03 %, dbSize: 63225856
2023-01-09T05:04:13.107240945Z I0109 05:04:13.107225       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 0.04 %, dbSize: 63287296
2023-01-09T05:04:18.108791133Z I0109 05:04:18.106873       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T05:04:18.199282561Z I0109 05:04:18.199247       1 etcdcli_pool.go:157] closing cached client
2023-01-09T05:12:59.369931073Z I0109 05:12:59.369888       1 request.go:601] Waited for 1.08172557s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T05:13:00.369972924Z I0109 05:13:00.369932       1 request.go:601] Waited for 1.396515149s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:13:01.370751058Z I0109 05:13:01.370713       1 request.go:601] Waited for 1.174520542s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T05:13:02.970504045Z I0109 05:13:02.970462       1 request.go:601] Waited for 1.054929997s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T05:15:13.110104922Z I0109 05:15:13.110065       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 14.59 %, dbSize: 64446464
2023-01-09T05:15:13.110104922Z I0109 05:15:13.110083       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 14.27 %, dbSize: 64270336
2023-01-09T05:15:13.110104922Z I0109 05:15:13.110087       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 14.37 %, dbSize: 64323584
2023-01-09T05:22:59.368201569Z I0109 05:22:59.368164       1 request.go:601] Waited for 1.079466572s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T05:23:00.568189516Z I0109 05:23:00.568152       1 request.go:601] Waited for 1.3970109s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:23:01.768592280Z I0109 05:23:01.768545       1 request.go:601] Waited for 1.397803494s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T05:23:02.968521972Z I0109 05:23:02.968480       1 request.go:601] Waited for 1.052396137s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T05:26:13.108141128Z I0109 05:26:13.108102       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 13.85 %, dbSize: 64925696
2023-01-09T05:26:13.108141128Z I0109 05:26:13.108120       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 13.58 %, dbSize: 64749568
2023-01-09T05:26:13.108141128Z I0109 05:26:13.108125       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 13.73 %, dbSize: 64802816
2023-01-09T05:31:14.830122181Z I0109 05:31:14.830066       1 request.go:601] Waited for 1.167067788s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:32:59.335097736Z I0109 05:32:59.335050       1 request.go:601] Waited for 1.045446371s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:33:00.534819773Z I0109 05:33:00.534781       1 request.go:601] Waited for 1.39653746s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:33:01.734982508Z I0109 05:33:01.734943       1 request.go:601] Waited for 1.592713112s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T05:33:02.934505917Z I0109 05:33:02.934471       1 request.go:601] Waited for 1.017861694s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T05:33:03.935063342Z I0109 05:33:03.935021       1 request.go:601] Waited for 1.1957299s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T05:33:05.135421863Z I0109 05:33:05.135371       1 request.go:601] Waited for 1.195697692s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T05:33:06.334544255Z I0109 05:33:06.334506       1 request.go:601] Waited for 1.194745033s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T05:37:13.108519461Z I0109 05:37:13.108480       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 2.08 %, dbSize: 89829376
2023-01-09T05:37:13.108519461Z I0109 05:37:13.108498       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 1.45 %, dbSize: 89296896
2023-01-09T05:37:13.108519461Z I0109 05:37:13.108503       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 1.63 %, dbSize: 89436160
2023-01-09T05:42:59.369273604Z I0109 05:42:59.369234       1 request.go:601] Waited for 1.080064599s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:43:00.369669627Z I0109 05:43:00.369626       1 request.go:601] Waited for 1.397279164s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:43:01.568872559Z I0109 05:43:01.568834       1 request.go:601] Waited for 1.392949908s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T05:43:02.569706989Z I0109 05:43:02.569667       1 request.go:601] Waited for 1.195958684s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T05:43:03.768873689Z I0109 05:43:03.768833       1 request.go:601] Waited for 1.15613898s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T05:48:13.108381346Z I0109 05:48:13.108338       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 27.21 %, dbSize: 91754496
2023-01-09T05:48:13.108381346Z I0109 05:48:13.108355       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 27.14 %, dbSize: 91652096
2023-01-09T05:48:13.108381346Z I0109 05:48:13.108360       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 27.10 %, dbSize: 91590656
2023-01-09T05:52:59.372188516Z I0109 05:52:59.372150       1 request.go:601] Waited for 1.082242248s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T05:53:00.571458365Z I0109 05:53:00.571419       1 request.go:601] Waited for 1.396649212s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T05:53:01.571573509Z I0109 05:53:01.571537       1 request.go:601] Waited for 1.394371187s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T05:53:02.572389477Z I0109 05:53:02.572351       1 request.go:601] Waited for 1.197351329s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T05:53:03.771506066Z I0109 05:53:03.771463       1 request.go:601] Waited for 1.157510098s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T05:59:13.108002469Z I0109 05:59:13.107952       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 26.44 %, dbSize: 91754496
2023-01-09T05:59:13.108002469Z I0109 05:59:13.107968       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 26.40 %, dbSize: 91652096
2023-01-09T05:59:13.108002469Z I0109 05:59:13.107973       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 26.33 %, dbSize: 91590656
2023-01-09T06:02:59.365715664Z I0109 06:02:59.365675       1 request.go:601] Waited for 1.0764894s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:03:00.565627187Z I0109 06:03:00.565581       1 request.go:601] Waited for 1.395929811s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:03:01.566258375Z I0109 06:03:01.566222       1 request.go:601] Waited for 1.393412737s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T06:03:02.566314219Z I0109 06:03:02.566270       1 request.go:601] Waited for 1.194869061s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T06:03:03.765949383Z I0109 06:03:03.765912       1 request.go:601] Waited for 1.154722562s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T06:10:13.110280721Z I0109 06:10:13.110236       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 30.07 %, dbSize: 91754496
2023-01-09T06:10:13.110280721Z I0109 06:10:13.110253       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 30.01 %, dbSize: 91652096
2023-01-09T06:10:13.110280721Z I0109 06:10:13.110258       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 29.96 %, dbSize: 91590656
2023-01-09T06:12:59.323475002Z I0109 06:12:59.323435       1 request.go:601] Waited for 1.033695393s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:13:00.323771795Z I0109 06:13:00.323731       1 request.go:601] Waited for 1.395123607s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T06:13:01.523812323Z I0109 06:13:01.523776       1 request.go:601] Waited for 1.597857326s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:13:02.723770424Z I0109 06:13:02.723731       1 request.go:601] Waited for 1.394920756s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T06:13:03.924102919Z I0109 06:13:03.924052       1 request.go:601] Waited for 1.195479633s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T06:21:13.112953062Z I0109 06:21:13.112909       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 27.65 %, dbSize: 91754496
2023-01-09T06:21:13.112953062Z I0109 06:21:13.112926       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 27.57 %, dbSize: 91652096
2023-01-09T06:21:13.112953062Z I0109 06:21:13.112931       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 27.53 %, dbSize: 91590656
2023-01-09T06:22:59.373565337Z I0109 06:22:59.373523       1 request.go:601] Waited for 1.082923063s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:23:00.573426737Z I0109 06:23:00.573360       1 request.go:601] Waited for 1.397026859s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:23:01.773425419Z I0109 06:23:01.773388       1 request.go:601] Waited for 1.196478507s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T06:23:02.973411478Z I0109 06:23:02.973372       1 request.go:601] Waited for 1.055129369s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T06:32:13.111671875Z I0109 06:32:13.111619       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 24.36 %, dbSize: 91754496
2023-01-09T06:32:13.111671875Z I0109 06:32:13.111640       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 24.35 %, dbSize: 91652096
2023-01-09T06:32:13.111671875Z I0109 06:32:13.111648       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 24.26 %, dbSize: 91590656
2023-01-09T06:32:59.377594055Z I0109 06:32:59.377546       1 request.go:601] Waited for 1.086488211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:33:00.381113859Z I0109 06:33:00.381067       1 request.go:601] Waited for 1.398461221s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T06:33:01.577503978Z I0109 06:33:01.577462       1 request.go:601] Waited for 1.393803135s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T06:33:02.577910983Z I0109 06:33:02.577866       1 request.go:601] Waited for 1.196596822s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T06:33:03.578024544Z I0109 06:33:03.577953       1 request.go:601] Waited for 1.196839904s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T06:42:59.372717779Z I0109 06:42:59.372671       1 request.go:601] Waited for 1.081191971s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:43:00.572097494Z I0109 06:43:00.572061       1 request.go:601] Waited for 1.396639953s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:43:01.572351520Z I0109 06:43:01.572306       1 request.go:601] Waited for 1.39461306s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T06:43:02.972105949Z I0109 06:43:02.972069       1 request.go:601] Waited for 1.052704525s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T06:43:13.111629063Z I0109 06:43:13.111587       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 23.91 %, dbSize: 91754496
2023-01-09T06:43:13.111629063Z I0109 06:43:13.111603       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 23.91 %, dbSize: 91652096
2023-01-09T06:43:13.111629063Z I0109 06:43:13.111607       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 23.81 %, dbSize: 91590656
2023-01-09T06:52:59.371190907Z I0109 06:52:59.371148       1 request.go:601] Waited for 1.079048761s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T06:53:00.570559999Z I0109 06:53:00.570522       1 request.go:601] Waited for 1.396506957s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T06:53:01.571309524Z I0109 06:53:01.571270       1 request.go:601] Waited for 1.562594004s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T06:53:02.571348785Z I0109 06:53:02.571304       1 request.go:601] Waited for 1.19682038s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T06:53:03.771115037Z I0109 06:53:03.771071       1 request.go:601] Waited for 1.197388073s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T06:54:13.110955319Z I0109 06:54:13.110911       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 21.55 %, dbSize: 91754496
2023-01-09T06:54:13.110955319Z I0109 06:54:13.110928       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 22.35 %, dbSize: 92618752
2023-01-09T06:54:13.110955319Z I0109 06:54:13.110935       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 21.50 %, dbSize: 91590656
2023-01-09T07:02:59.329122387Z I0109 07:02:59.329073       1 request.go:601] Waited for 1.037102436s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T07:03:00.528621601Z I0109 07:03:00.528581       1 request.go:601] Waited for 1.39389751s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T07:03:01.529181088Z I0109 07:03:01.529139       1 request.go:601] Waited for 1.393091794s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T07:03:02.928496226Z I0109 07:03:02.928457       1 request.go:601] Waited for 1.007770493s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:03:03.928949425Z I0109 07:03:03.928911       1 request.go:601] Waited for 1.197547038s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-guard-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:05:13.125156734Z I0109 07:05:13.125120       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 26.04 %, dbSize: 91754496
2023-01-09T07:05:13.125156734Z I0109 07:05:13.125136       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 26.78 %, dbSize: 92618752
2023-01-09T07:05:13.125156734Z I0109 07:05:13.125141       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 26.01 %, dbSize: 91590656
2023-01-09T07:12:59.373901785Z I0109 07:12:59.373864       1 request.go:601] Waited for 1.081344477s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T07:13:00.573975825Z I0109 07:13:00.573933       1 request.go:601] Waited for 1.396413882s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:13:01.773509526Z I0109 07:13:01.773471       1 request.go:601] Waited for 1.19457739s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:13:02.974428945Z I0109 07:13:02.974386       1 request.go:601] Waited for 1.053137295s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:16:13.116354775Z I0109 07:16:13.116315       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 26.54 %, dbSize: 91754496
2023-01-09T07:16:13.116354775Z I0109 07:16:13.116333       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 28.41 %, dbSize: 94089216
2023-01-09T07:16:13.116354775Z I0109 07:16:13.116338       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 26.81 %, dbSize: 92078080
2023-01-09T07:22:59.373727500Z I0109 07:22:59.373687       1 request.go:601] Waited for 1.080578549s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T07:23:00.573090999Z I0109 07:23:00.573051       1 request.go:601] Waited for 1.396508468s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:23:01.573798327Z I0109 07:23:01.573742       1 request.go:601] Waited for 1.393448617s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T07:23:02.973615060Z I0109 07:23:02.973575       1 request.go:601] Waited for 1.051774208s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:27:13.117447304Z I0109 07:27:13.117402       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 23.72 %, dbSize: 91754496
2023-01-09T07:27:13.117447304Z I0109 07:27:13.117420       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 25.64 %, dbSize: 94089216
2023-01-09T07:27:13.117447304Z I0109 07:27:13.117427       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 24.07 %, dbSize: 92078080
2023-01-09T07:32:59.370335174Z I0109 07:32:59.370289       1 request.go:601] Waited for 1.076406356s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T07:33:00.370737637Z I0109 07:33:00.370691       1 request.go:601] Waited for 1.392690879s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T07:33:01.371172458Z I0109 07:33:01.371133       1 request.go:601] Waited for 1.358593148s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T07:33:02.570666643Z I0109 07:33:02.570624       1 request.go:601] Waited for 1.194811164s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T07:33:03.571049375Z I0109 07:33:03.571005       1 request.go:601] Waited for 1.198092622s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T07:38:13.120429558Z I0109 07:38:13.120388       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 22.73 %, dbSize: 92729344
2023-01-09T07:38:13.120429558Z I0109 07:38:13.120407       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 23.87 %, dbSize: 94089216
2023-01-09T07:38:13.120429558Z I0109 07:38:13.120414       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 22.67 %, dbSize: 92569600
2023-01-09T07:42:59.375486148Z I0109 07:42:59.375449       1 request.go:601] Waited for 1.08036796s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:43:00.574953996Z I0109 07:43:00.574913       1 request.go:601] Waited for 1.394447507s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:43:01.575270044Z I0109 07:43:01.575233       1 request.go:601] Waited for 1.196778433s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T07:43:02.974968892Z I0109 07:43:02.974928       1 request.go:601] Waited for 1.051655583s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:43:03.975454037Z I0109 07:43:03.975412       1 request.go:601] Waited for 1.198016805s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T07:49:13.120322890Z I0109 07:49:13.120276       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 21.82 %, dbSize: 93224960
2023-01-09T07:49:13.120322890Z I0109 07:49:13.120293       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 22.59 %, dbSize: 94089216
2023-01-09T07:49:13.120322890Z I0109 07:49:13.120297       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 22.12 %, dbSize: 93540352
2023-01-09T07:52:59.375693844Z I0109 07:52:59.375652       1 request.go:601] Waited for 1.080013497s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T07:53:00.375889897Z I0109 07:53:00.375838       1 request.go:601] Waited for 1.394882908s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T07:53:01.575611800Z I0109 07:53:01.575571       1 request.go:601] Waited for 1.394487031s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T07:53:02.975419553Z I0109 07:53:02.975381       1 request.go:601] Waited for 1.051568406s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T07:53:03.975431861Z I0109 07:53:03.975396       1 request.go:601] Waited for 1.195877812s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:00:13.123362114Z I0109 08:00:13.123319       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 28.53 %, dbSize: 93720576
2023-01-09T08:00:13.123362114Z I0109 08:00:13.123335       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 29.23 %, dbSize: 94584832
2023-01-09T08:00:13.123362114Z I0109 08:00:13.123340       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 28.48 %, dbSize: 93540352
2023-01-09T08:02:59.380782592Z I0109 08:02:59.380746       1 request.go:601] Waited for 1.082912844s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:03:00.579771427Z I0109 08:03:00.579732       1 request.go:601] Waited for 1.396061437s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:03:01.580498580Z I0109 08:03:01.580460       1 request.go:601] Waited for 1.394529681s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T08:03:02.980244255Z I0109 08:03:02.980206       1 request.go:601] Waited for 1.055395961s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T08:03:03.980395236Z I0109 08:03:03.980353       1 request.go:601] Waited for 1.197512411s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:11:13.127025125Z I0109 08:11:13.126966       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.70 %, dbSize: 95285248
2023-01-09T08:11:13.127025125Z I0109 08:11:13.127005       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.97 %, dbSize: 95584256
2023-01-09T08:11:13.127025125Z I0109 08:11:13.127013       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.91 %, dbSize: 94040064
2023-01-09T08:12:59.375535061Z I0109 08:12:59.375495       1 request.go:601] Waited for 1.078357184s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:13:00.375818109Z I0109 08:13:00.375773       1 request.go:601] Waited for 1.394805311s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T08:13:01.575554244Z I0109 08:13:01.575514       1 request.go:601] Waited for 1.394205759s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T08:13:02.576029243Z I0109 08:13:02.575975       1 request.go:601] Waited for 1.197386083s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T08:13:03.775920776Z I0109 08:13:03.775877       1 request.go:601] Waited for 1.159367491s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T08:13:04.975557867Z I0109 08:13:04.975510       1 request.go:601] Waited for 1.194884609s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T08:22:13.122514055Z I0109 08:22:13.122463       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.06 %, dbSize: 95285248
2023-01-09T08:22:13.122514055Z I0109 08:22:13.122486       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.30 %, dbSize: 95584256
2023-01-09T08:22:13.122514055Z I0109 08:22:13.122493       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.27 %, dbSize: 94040064
2023-01-09T08:22:59.376096491Z I0109 08:22:59.376052       1 request.go:601] Waited for 1.078340964s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:23:00.575486531Z I0109 08:23:00.575446       1 request.go:601] Waited for 1.3971825s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:23:01.576050883Z I0109 08:23:01.576012       1 request.go:601] Waited for 1.369830776s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T08:23:02.975110912Z I0109 08:23:02.975075       1 request.go:601] Waited for 1.048873968s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T08:23:03.975582604Z I0109 08:23:03.975546       1 request.go:601] Waited for 1.196965988s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:32:59.375764178Z I0109 08:32:59.375725       1 request.go:601] Waited for 1.077099837s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T08:33:00.376489052Z I0109 08:33:00.376448       1 request.go:601] Waited for 1.397553109s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:33:01.575785926Z I0109 08:33:01.575748       1 request.go:601] Waited for 1.196483305s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:33:02.976106133Z I0109 08:33:02.976066       1 request.go:601] Waited for 1.04929606s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T08:33:03.976404008Z I0109 08:33:03.976362       1 request.go:601] Waited for 1.195853398s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:33:13.127267861Z I0109 08:33:13.127230       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 33.53 %, dbSize: 96276480
2023-01-09T08:33:13.127267861Z I0109 08:33:13.127246       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 34.15 %, dbSize: 97091584
2023-01-09T08:33:13.127267861Z I0109 08:33:13.127251       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 32.76 %, dbSize: 95035392
2023-01-09T08:42:59.376861926Z I0109 08:42:59.376821       1 request.go:601] Waited for 1.078222263s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:43:00.376906930Z I0109 08:43:00.376874       1 request.go:601] Waited for 1.396719262s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:43:01.576163763Z I0109 08:43:01.576131       1 request.go:601] Waited for 1.388968384s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T08:43:02.576339198Z I0109 08:43:02.576294       1 request.go:601] Waited for 1.197120637s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T08:43:03.576762897Z I0109 08:43:03.576728       1 request.go:601] Waited for 1.198122896s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T08:44:13.123020566Z I0109 08:44:13.122975       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.79 %, dbSize: 97251328
2023-01-09T08:44:13.123076572Z I0109 08:44:13.123020       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.34 %, dbSize: 98050048
2023-01-09T08:44:13.123076572Z I0109 08:44:13.123027       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 33.30 %, dbSize: 95035392
2023-01-09T08:52:59.380170888Z I0109 08:52:59.380133       1 request.go:601] Waited for 1.081074484s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T08:53:00.580109565Z I0109 08:53:00.580068       1 request.go:601] Waited for 1.396123382s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T08:53:01.779887732Z I0109 08:53:01.779846       1 request.go:601] Waited for 1.193806878s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T08:53:02.980254299Z I0109 08:53:02.980216       1 request.go:601] Waited for 1.053092559s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T08:55:13.124950046Z I0109 08:55:13.124909       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 41.18 %, dbSize: 97251328
2023-01-09T08:55:13.124950046Z I0109 08:55:13.124925       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 41.65 %, dbSize: 98050048
2023-01-09T08:55:13.124950046Z I0109 08:55:13.124930       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 40.18 %, dbSize: 95551488
2023-01-09T09:02:59.381201657Z I0109 09:02:59.381155       1 request.go:601] Waited for 1.08202745s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:03:00.381486801Z I0109 09:03:00.381445       1 request.go:601] Waited for 1.395336758s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T09:03:01.381667885Z I0109 09:03:01.381632       1 request.go:601] Waited for 1.362263432s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T09:03:02.581119386Z I0109 09:03:02.581082       1 request.go:601] Waited for 1.196079029s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T09:03:03.581364177Z I0109 09:03:03.581326       1 request.go:601] Waited for 1.195100665s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T09:06:13.125126499Z I0109 09:06:13.125082       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.66 %, dbSize: 97251328
2023-01-09T09:06:13.125126499Z I0109 09:06:13.125105       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.20 %, dbSize: 98050048
2023-01-09T09:06:13.125126499Z I0109 09:06:13.125112       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.86 %, dbSize: 96030720
2023-01-09T09:12:59.417969426Z I0109 09:12:59.417928       1 request.go:601] Waited for 1.117816135s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T09:13:00.617816078Z I0109 09:13:00.617775       1 request.go:601] Waited for 1.397877776s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:13:01.621338189Z I0109 09:13:01.619224       1 request.go:601] Waited for 1.397001295s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T09:13:02.619291187Z I0109 09:13:02.619251       1 request.go:601] Waited for 1.199420998s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T09:13:03.817447267Z I0109 09:13:03.817410       1 request.go:601] Waited for 1.194260115s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T09:17:13.125489151Z I0109 09:17:13.125451       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 37.51 %, dbSize: 98209792
2023-01-09T09:17:13.125489151Z I0109 09:17:13.125469       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 37.69 %, dbSize: 98529280
2023-01-09T09:17:13.125489151Z I0109 09:17:13.125474       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 36.44 %, dbSize: 96509952
2023-01-09T09:22:59.385945381Z I0109 09:22:59.385911       1 request.go:601] Waited for 1.084719195s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T09:23:00.585416628Z I0109 09:23:00.585383       1 request.go:601] Waited for 1.39672008s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:23:01.785664709Z I0109 09:23:01.785627       1 request.go:601] Waited for 1.197618089s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T09:23:02.985552764Z I0109 09:23:02.985515       1 request.go:601] Waited for 1.056789391s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T09:23:03.986200160Z I0109 09:23:03.986159       1 request.go:601] Waited for 1.194259116s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:28:13.119108991Z I0109 09:28:13.119068       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 36.30 %, dbSize: 98209792
2023-01-09T09:28:13.119108991Z I0109 09:28:13.119085       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.49 %, dbSize: 98529280
2023-01-09T09:28:13.119108991Z I0109 09:28:13.119090       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 35.20 %, dbSize: 96509952
2023-01-09T09:32:59.382961441Z I0109 09:32:59.382917       1 request.go:601] Waited for 1.082076069s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:33:00.582306943Z I0109 09:33:00.582268       1 request.go:601] Waited for 1.394006288s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T09:33:01.582811582Z I0109 09:33:01.582772       1 request.go:601] Waited for 1.393698683s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T09:33:02.582967576Z I0109 09:33:02.582923       1 request.go:601] Waited for 1.19709316s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T09:33:03.583022439Z I0109 09:33:03.582957       1 request.go:601] Waited for 1.197769593s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T09:39:13.127309579Z I0109 09:39:13.127265       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.16 %, dbSize: 98209792
2023-01-09T09:39:13.127309579Z I0109 09:39:13.127289       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 34.35 %, dbSize: 98529280
2023-01-09T09:39:13.127309579Z I0109 09:39:13.127296       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 33.36 %, dbSize: 97001472
2023-01-09T09:42:59.383578943Z I0109 09:42:59.383538       1 request.go:601] Waited for 1.082042452s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:43:00.582898257Z I0109 09:43:00.582855       1 request.go:601] Waited for 1.39741487s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T09:43:01.582929968Z I0109 09:43:01.582876       1 request.go:601] Waited for 1.391464654s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T09:43:02.583214793Z I0109 09:43:02.583173       1 request.go:601] Waited for 1.196500048s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T09:43:03.583546097Z I0109 09:43:03.583501       1 request.go:601] Waited for 1.197732836s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T09:50:13.124033992Z I0109 09:50:13.123971       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 41.19 %, dbSize: 98209792
2023-01-09T09:50:13.124033992Z I0109 09:50:13.124019       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 41.36 %, dbSize: 98529280
2023-01-09T09:50:13.124033992Z I0109 09:50:13.124027       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 40.47 %, dbSize: 97001472
2023-01-09T09:52:59.384821179Z I0109 09:52:59.384786       1 request.go:601] Waited for 1.082720939s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T09:53:00.385113351Z I0109 09:53:00.384985       1 request.go:601] Waited for 1.396102842s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T09:53:01.584656822Z I0109 09:53:01.584619       1 request.go:601] Waited for 1.391535633s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T09:53:02.984569551Z I0109 09:53:02.984532       1 request.go:601] Waited for 1.054944977s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T09:53:03.984883076Z I0109 09:53:03.984843       1 request.go:601] Waited for 1.196115335s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:01:13.128600833Z I0109 10:01:13.128561       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 38.76 %, dbSize: 98689024
2023-01-09T10:01:13.128600833Z I0109 10:01:13.128579       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 38.69 %, dbSize: 98529280
2023-01-09T10:01:13.128600833Z I0109 10:01:13.128584       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 38.01 %, dbSize: 97480704
2023-01-09T10:02:59.382427542Z I0109 10:02:59.382392       1 request.go:601] Waited for 1.07977055s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:03:00.582333132Z I0109 10:03:00.582298       1 request.go:601] Waited for 1.394915937s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:03:01.781980007Z I0109 10:03:01.781937       1 request.go:601] Waited for 1.196997504s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:03:02.982103803Z I0109 10:03:02.982065       1 request.go:601] Waited for 1.051694778s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:03:03.982380581Z I0109 10:03:03.982341       1 request.go:601] Waited for 1.196807796s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:12:13.126708101Z I0109 10:12:13.126670       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.72 %, dbSize: 99188736
2023-01-09T10:12:13.126708101Z I0109 10:12:13.126686       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.29 %, dbSize: 98529280
2023-01-09T10:12:13.126708101Z I0109 10:12:13.126693       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.98 %, dbSize: 97980416
2023-01-09T10:12:59.382981623Z I0109 10:12:59.382933       1 request.go:601] Waited for 1.080025382s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:13:00.582707883Z I0109 10:13:00.582662       1 request.go:601] Waited for 1.396289091s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:13:01.783133238Z I0109 10:13:01.783093       1 request.go:601] Waited for 1.197787459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:13:02.982530825Z I0109 10:13:02.982483       1 request.go:601] Waited for 1.051845018s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:22:59.405563916Z I0109 10:22:59.405521       1 request.go:601] Waited for 1.102175431s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:23:00.407241746Z I0109 10:23:00.407200       1 request.go:601] Waited for 1.394895286s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T10:23:01.606491679Z I0109 10:23:01.606450       1 request.go:601] Waited for 1.394735436s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T10:23:03.005835660Z I0109 10:23:03.005791       1 request.go:601] Waited for 1.074604869s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:23:06.006254673Z I0109 10:23:06.006214       1 request.go:601] Waited for 1.172647384s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:23:13.124792981Z I0109 10:23:13.124753       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.47 %, dbSize: 99188736
2023-01-09T10:23:13.124792981Z I0109 10:23:13.124770       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.10 %, dbSize: 98529280
2023-01-09T10:23:13.124792981Z I0109 10:23:13.124777       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 35.03 %, dbSize: 98467840
2023-01-09T10:32:16.651537269Z I0109 10:32:16.650969       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T10:32:16.688412878Z I0109 10:32:16.688376       1 etcdcli_pool.go:157] closing cached client
2023-01-09T10:32:59.384529001Z I0109 10:32:59.384492       1 request.go:601] Waited for 1.080331792s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:33:00.584891452Z I0109 10:33:00.584850       1 request.go:601] Waited for 1.394345404s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:33:01.784377587Z I0109 10:33:01.784336       1 request.go:601] Waited for 1.194740701s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:33:02.984247050Z I0109 10:33:02.984207       1 request.go:601] Waited for 1.052606462s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:34:13.131597802Z I0109 10:34:13.131556       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.94 %, dbSize: 99680256
2023-01-09T10:34:13.131597802Z I0109 10:34:13.131575       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 34.20 %, dbSize: 98529280
2023-01-09T10:34:13.131597802Z I0109 10:34:13.131584       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.18 %, dbSize: 98467840
2023-01-09T10:42:59.386188146Z I0109 10:42:59.386152       1 request.go:601] Waited for 1.081102901s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:43:00.586773647Z I0109 10:43:00.586736       1 request.go:601] Waited for 1.395435188s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T10:43:01.786107578Z I0109 10:43:01.786065       1 request.go:601] Waited for 1.396752324s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T10:43:02.986332374Z I0109 10:43:02.986290       1 request.go:601] Waited for 1.053659748s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T10:43:03.986419941Z I0109 10:43:03.986378       1 request.go:601] Waited for 1.193649847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T10:45:13.129862492Z I0109 10:45:13.129825       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 40.96 %, dbSize: 100159488
2023-01-09T10:45:13.129862492Z I0109 10:45:13.129841       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 39.98 %, dbSize: 98529280
2023-01-09T10:45:13.129862492Z I0109 10:45:13.129846       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 40.26 %, dbSize: 98947072
2023-01-09T10:52:59.390873428Z I0109 10:52:59.390832       1 request.go:601] Waited for 1.083393884s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T10:53:00.391301380Z I0109 10:53:00.391261       1 request.go:601] Waited for 1.395279846s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T10:53:01.590980549Z I0109 10:53:01.590936       1 request.go:601] Waited for 1.393425037s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T10:53:02.591443792Z I0109 10:53:02.591407       1 request.go:601] Waited for 1.195521985s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T10:53:03.791066062Z I0109 10:53:03.791017       1 request.go:601] Waited for 1.160104044s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T10:56:13.127973416Z I0109 10:56:13.127931       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 40.21 %, dbSize: 100638720
2023-01-09T10:56:13.127973416Z I0109 10:56:13.127949       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 38.91 %, dbSize: 98529280
2023-01-09T10:56:13.127973416Z I0109 10:56:13.127955       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 39.49 %, dbSize: 99426304
2023-01-09T11:02:59.500722158Z I0109 11:02:59.500683       1 request.go:601] Waited for 1.146595834s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T11:03:00.501418327Z I0109 11:03:00.501373       1 request.go:601] Waited for 1.39477656s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T11:03:01.501494070Z I0109 11:03:01.501458       1 request.go:601] Waited for 1.396837415s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:03:02.701107437Z I0109 11:03:02.701067       1 request.go:601] Waited for 1.194150959s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:03:03.900781883Z I0109 11:03:03.900743       1 request.go:601] Waited for 1.196453708s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:07:13.132788510Z I0109 11:07:13.132750       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.30 %, dbSize: 100638720
2023-01-09T11:07:13.132788510Z I0109 11:07:13.132767       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 32.95 %, dbSize: 98529280
2023-01-09T11:07:13.132788510Z I0109 11:07:13.132772       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 33.53 %, dbSize: 99426304
2023-01-09T11:12:59.385219891Z I0109 11:12:59.385161       1 request.go:601] Waited for 1.078164255s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:13:00.385223331Z I0109 11:13:00.385179       1 request.go:601] Waited for 1.395306945s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T11:13:01.585067025Z I0109 11:13:01.585023       1 request.go:601] Waited for 1.394498519s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T11:13:02.984715716Z I0109 11:13:02.984675       1 request.go:601] Waited for 1.050664133s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T11:13:03.985402919Z I0109 11:13:03.985361       1 request.go:601] Waited for 1.197396943s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:18:13.133623869Z I0109 11:18:13.133584       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 35.67 %, dbSize: 100638720
2023-01-09T11:18:13.133623869Z I0109 11:18:13.133602       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 35.31 %, dbSize: 100061184
2023-01-09T11:18:13.133623869Z I0109 11:18:13.133607       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 34.93 %, dbSize: 99426304
2023-01-09T11:22:59.394713422Z I0109 11:22:59.394658       1 request.go:601] Waited for 1.084245509s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:23:00.395183019Z I0109 11:23:00.395140       1 request.go:601] Waited for 1.391908334s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T11:23:01.395482912Z I0109 11:23:01.395428       1 request.go:601] Waited for 1.359255779s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T11:23:02.595120084Z I0109 11:23:02.595078       1 request.go:601] Waited for 1.194689333s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T11:23:03.595427507Z I0109 11:23:03.595380       1 request.go:601] Waited for 1.198090322s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T11:23:04.794745637Z I0109 11:23:04.794699       1 request.go:601] Waited for 1.196098988s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd
2023-01-09T11:29:13.133428178Z I0109 11:29:13.133385       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.05 %, dbSize: 100638720
2023-01-09T11:29:13.133428178Z I0109 11:29:13.133405       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 33.67 %, dbSize: 100061184
2023-01-09T11:29:13.133428178Z I0109 11:29:13.133411       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 33.26 %, dbSize: 99426304
2023-01-09T11:32:59.394124284Z I0109 11:32:59.394081       1 request.go:601] Waited for 1.084935099s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:33:00.594013748Z I0109 11:33:00.593947       1 request.go:601] Waited for 1.394565191s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T11:33:01.594304170Z I0109 11:33:01.594259       1 request.go:601] Waited for 1.393053312s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T11:33:02.994447680Z I0109 11:33:02.994400       1 request.go:601] Waited for 1.058991741s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T11:40:13.135680974Z I0109 11:40:13.135640       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 41.50 %, dbSize: 100638720
2023-01-09T11:40:13.135680974Z I0109 11:40:13.135658       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 41.20 %, dbSize: 100061184
2023-01-09T11:40:13.135680974Z I0109 11:40:13.135663       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 40.78 %, dbSize: 99426304
2023-01-09T11:42:59.391811429Z I0109 11:42:59.391770       1 request.go:601] Waited for 1.082584847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:43:00.591556132Z I0109 11:43:00.591518       1 request.go:601] Waited for 1.396903186s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T11:43:01.791515420Z I0109 11:43:01.791471       1 request.go:601] Waited for 1.197009192s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T11:43:02.992317460Z I0109 11:43:02.992279       1 request.go:601] Waited for 1.056645999s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T11:51:13.131741702Z I0109 11:51:13.131698       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 39.33 %, dbSize: 100638720
2023-01-09T11:51:13.131741702Z I0109 11:51:13.131721       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 38.98 %, dbSize: 100061184
2023-01-09T11:51:13.131741702Z I0109 11:51:13.131728       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 38.93 %, dbSize: 99913728
2023-01-09T11:52:59.397899353Z I0109 11:52:59.397848       1 request.go:601] Waited for 1.087600042s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T11:53:00.398522914Z I0109 11:53:00.398466       1 request.go:601] Waited for 1.393073299s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T11:53:01.598459306Z I0109 11:53:01.598415       1 request.go:601] Waited for 1.394727865s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T11:53:02.998101380Z I0109 11:53:02.998048       1 request.go:601] Waited for 1.062277545s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T11:53:03.998162444Z I0109 11:53:03.998125       1 request.go:601] Waited for 1.197527048s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:02:13.135436992Z I0109 12:02:13.135395       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 36.61 %, dbSize: 100638720
2023-01-09T12:02:13.135436992Z I0109 12:02:13.135414       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.27 %, dbSize: 100061184
2023-01-09T12:02:13.135436992Z I0109 12:02:13.135419       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 36.14 %, dbSize: 99913728
2023-01-09T12:02:59.388359551Z I0109 12:02:59.388312       1 request.go:601] Waited for 1.07711892s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:03:00.588815678Z I0109 12:03:00.588774       1 request.go:601] Waited for 1.397456292s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:03:01.788663370Z I0109 12:03:01.788618       1 request.go:601] Waited for 1.197048397s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:03:02.988293028Z I0109 12:03:02.988244       1 request.go:601] Waited for 1.052110667s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:03:03.988443476Z I0109 12:03:03.988398       1 request.go:601] Waited for 1.196059936s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:12:59.392283763Z I0109 12:12:59.392236       1 request.go:601] Waited for 1.081276136s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:13:00.591846809Z I0109 12:13:00.591805       1 request.go:601] Waited for 1.396581446s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:13:01.791886439Z I0109 12:13:01.791840       1 request.go:601] Waited for 1.19724375s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:13:02.992162183Z I0109 12:13:02.992117       1 request.go:601] Waited for 1.054936407s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:13:13.134451817Z I0109 12:13:13.134404       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 33.41 %, dbSize: 100638720
2023-01-09T12:13:13.134451817Z I0109 12:13:13.134424       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 33.42 %, dbSize: 100540416
2023-01-09T12:13:13.134451817Z I0109 12:13:13.134429       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 32.97 %, dbSize: 99913728
2023-01-09T12:22:59.395290040Z I0109 12:22:59.395250       1 request.go:601] Waited for 1.083901119s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:23:00.395403136Z I0109 12:23:00.395345       1 request.go:601] Waited for 1.39324211s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T12:23:01.595204972Z I0109 12:23:01.595160       1 request.go:601] Waited for 1.55820409s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T12:23:02.595378328Z I0109 12:23:02.595319       1 request.go:601] Waited for 1.198453766s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T12:23:03.794594947Z I0109 12:23:03.794550       1 request.go:601] Waited for 1.196126371s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T12:24:13.137191058Z I0109 12:24:13.137150       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 31.80 %, dbSize: 100638720
2023-01-09T12:24:13.137191058Z I0109 12:24:13.137168       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 31.76 %, dbSize: 100540416
2023-01-09T12:24:13.137191058Z I0109 12:24:13.137173       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 31.62 %, dbSize: 100417536
2023-01-09T12:32:59.390743185Z I0109 12:32:59.390699       1 request.go:601] Waited for 1.077775503s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:33:00.590127395Z I0109 12:33:00.590081       1 request.go:601] Waited for 1.394965777s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:33:01.590363166Z I0109 12:33:01.590314       1 request.go:601] Waited for 1.1969108s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:33:02.990106631Z I0109 12:33:02.990059       1 request.go:601] Waited for 1.051776471s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:33:03.990324193Z I0109 12:33:03.990276       1 request.go:601] Waited for 1.197072793s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:35:13.137508351Z I0109 12:35:13.137456       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 40.86 %, dbSize: 101117952
2023-01-09T12:35:13.137508351Z I0109 12:35:13.137474       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 40.56 %, dbSize: 100540416
2023-01-09T12:35:13.137508351Z I0109 12:35:13.137479       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 40.50 %, dbSize: 100417536
2023-01-09T12:42:59.395724755Z I0109 12:42:59.395676       1 request.go:601] Waited for 1.08294466s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T12:43:00.396122680Z I0109 12:43:00.396079       1 request.go:601] Waited for 1.397470411s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:43:01.396253050Z I0109 12:43:01.396212       1 request.go:601] Waited for 1.35952265s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T12:43:02.595859584Z I0109 12:43:02.595819       1 request.go:601] Waited for 1.19699847s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T12:43:03.596210868Z I0109 12:43:03.596145       1 request.go:601] Waited for 1.198035228s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T12:46:13.135939297Z I0109 12:46:13.135895       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 39.24 %, dbSize: 101117952
2023-01-09T12:46:13.135939297Z I0109 12:46:13.135912       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 38.89 %, dbSize: 100540416
2023-01-09T12:46:13.135939297Z I0109 12:46:13.135917       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 38.77 %, dbSize: 100417536
2023-01-09T12:52:59.396648429Z I0109 12:52:59.396610       1 request.go:601] Waited for 1.083083246s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:53:00.596754638Z I0109 12:53:00.596702       1 request.go:601] Waited for 1.396599115s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T12:53:01.795845312Z I0109 12:53:01.795807       1 request.go:601] Waited for 1.194388053s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T12:53:02.996191344Z I0109 12:53:02.996145       1 request.go:601] Waited for 1.057212276s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T12:57:13.137581763Z I0109 12:57:13.137543       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 36.78 %, dbSize: 101117952
2023-01-09T12:57:13.137581763Z I0109 12:57:13.137561       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.41 %, dbSize: 100540416
2023-01-09T12:57:13.137581763Z I0109 12:57:13.137568       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 36.32 %, dbSize: 100417536
2023-01-09T13:02:59.396757444Z I0109 13:02:59.396717       1 request.go:601] Waited for 1.082995708s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T13:03:00.596908568Z I0109 13:03:00.596870       1 request.go:601] Waited for 1.397320247s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:03:01.796531442Z I0109 13:03:01.796494       1 request.go:601] Waited for 1.196921868s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T13:03:02.996561409Z I0109 13:03:02.996522       1 request.go:601] Waited for 1.05726181s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:03:03.997094991Z I0109 13:03:03.997057       1 request.go:601] Waited for 1.196165003s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:08:13.135514361Z I0109 13:08:13.135474       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 30.78 %, dbSize: 101617664
2023-01-09T13:08:13.135514361Z I0109 13:08:13.135491       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 29.96 %, dbSize: 100540416
2023-01-09T13:08:13.135514361Z I0109 13:08:13.135496       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 29.87 %, dbSize: 100417536
2023-01-09T13:12:59.403066536Z I0109 13:12:59.403027       1 request.go:601] Waited for 1.088383297s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T13:13:00.403170045Z I0109 13:13:00.403130       1 request.go:601] Waited for 1.395429676s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T13:13:01.602747791Z I0109 13:13:01.602700       1 request.go:601] Waited for 1.393205166s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T13:13:03.003012699Z I0109 13:13:03.002951       1 request.go:601] Waited for 1.063220427s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:19:13.141576644Z I0109 13:19:13.141538       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 32.26 %, dbSize: 102117376
2023-01-09T13:19:13.141576644Z I0109 13:19:13.141556       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 31.13 %, dbSize: 100540416
2023-01-09T13:19:13.141576644Z I0109 13:19:13.141561       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 31.39 %, dbSize: 100917248
2023-01-09T13:22:59.395814405Z I0109 13:22:59.395754       1 request.go:601] Waited for 1.079811741s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:23:00.395904780Z I0109 13:23:00.395870       1 request.go:601] Waited for 1.397449669s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:23:01.396183777Z I0109 13:23:01.396142       1 request.go:601] Waited for 1.191955447s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T13:23:02.396516200Z I0109 13:23:02.396479       1 request.go:601] Waited for 1.197130454s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T13:23:03.596194234Z I0109 13:23:03.596150       1 request.go:601] Waited for 1.194928566s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T13:30:13.137894385Z I0109 13:30:13.137858       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 39.76 %, dbSize: 102117376
2023-01-09T13:30:13.137894385Z I0109 13:30:13.137875       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 39.12 %, dbSize: 101019648
2023-01-09T13:30:13.137894385Z I0109 13:30:13.137882       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 39.00 %, dbSize: 100917248
2023-01-09T13:32:59.398555322Z I0109 13:32:59.398518       1 request.go:601] Waited for 1.082002058s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:33:00.398600800Z I0109 13:33:00.398554       1 request.go:601] Waited for 1.396993332s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:33:01.398932428Z I0109 13:33:01.398891       1 request.go:601] Waited for 1.191403113s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T13:33:02.998347206Z I0109 13:33:02.998308       1 request.go:601] Waited for 1.056973059s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:33:03.998382865Z I0109 13:33:03.998342       1 request.go:601] Waited for 1.196936588s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:41:13.142542466Z I0109 13:41:13.142503       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 39.13 %, dbSize: 102117376
2023-01-09T13:41:13.142542466Z I0109 13:41:13.142520       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 39.38 %, dbSize: 102473728
2023-01-09T13:41:13.142542466Z I0109 13:41:13.142526       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 38.40 %, dbSize: 100917248
2023-01-09T13:42:59.394753739Z I0109 13:42:59.394705       1 request.go:601] Waited for 1.078265575s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T13:43:00.395481782Z I0109 13:43:00.395440       1 request.go:601] Waited for 1.394916488s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:43:01.594738861Z I0109 13:43:01.594701       1 request.go:601] Waited for 1.39172149s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T13:43:02.594814827Z I0109 13:43:02.594775       1 request.go:601] Waited for 1.197131647s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T13:43:03.595298599Z I0109 13:43:03.595258       1 request.go:601] Waited for 1.195481033s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T13:52:13.138437027Z I0109 13:52:13.138392       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 36.11 %, dbSize: 102117376
2023-01-09T13:52:13.138437027Z I0109 13:52:13.138409       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.33 %, dbSize: 102473728
2023-01-09T13:52:13.138437027Z I0109 13:52:13.138413       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 35.64 %, dbSize: 101412864
2023-01-09T13:52:59.395676902Z I0109 13:52:59.395632       1 request.go:601] Waited for 1.077925157s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:53:00.395795564Z I0109 13:53:00.395760       1 request.go:601] Waited for 1.397298579s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T13:53:01.396153598Z I0109 13:53:01.396100       1 request.go:601] Waited for 1.195188337s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T13:53:02.995788775Z I0109 13:53:02.995752       1 request.go:601] Waited for 1.053123193s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T13:53:03.997949425Z I0109 13:53:03.997912       1 request.go:601] Waited for 1.199267186s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:02:59.398623085Z I0109 14:02:59.398580       1 request.go:601] Waited for 1.080801473s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:03:00.398807028Z I0109 14:03:00.398767       1 request.go:601] Waited for 1.397814246s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:03:01.598084346Z I0109 14:03:01.598030       1 request.go:601] Waited for 1.196441006s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:03:02.998227397Z I0109 14:03:02.998189       1 request.go:601] Waited for 1.054549662s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:03:03.998340857Z I0109 14:03:03.998296       1 request.go:601] Waited for 1.194974532s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:03:13.140376078Z I0109 14:03:13.140333       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 33.09 %, dbSize: 102117376
2023-01-09T14:03:13.140376078Z I0109 14:03:13.140351       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 33.35 %, dbSize: 102473728
2023-01-09T14:03:13.140376078Z I0109 14:03:13.140355       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 32.64 %, dbSize: 101412864
2023-01-09T14:06:39.443620653Z I0109 14:06:39.443570       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T14:06:39.514193645Z I0109 14:06:39.514156       1 etcdcli_pool.go:157] closing cached client
2023-01-09T14:12:59.403387259Z I0109 14:12:59.403348       1 request.go:601] Waited for 1.084945831s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:13:00.403771465Z I0109 14:13:00.403727       1 request.go:601] Waited for 1.397243576s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:13:01.602916935Z I0109 14:13:01.602883       1 request.go:601] Waited for 1.195610846s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:13:03.003322756Z I0109 14:13:03.003279       1 request.go:601] Waited for 1.058855673s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:14:13.145734835Z I0109 14:14:13.145694       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 29.80 %, dbSize: 102117376
2023-01-09T14:14:13.145734835Z I0109 14:14:13.145710       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 29.99 %, dbSize: 102473728
2023-01-09T14:14:13.145734835Z I0109 14:14:13.145715       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 29.25 %, dbSize: 101412864
2023-01-09T14:22:59.359758778Z I0109 14:22:59.359719       1 request.go:601] Waited for 1.041440165s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:23:00.559068369Z I0109 14:23:00.559028       1 request.go:601] Waited for 1.393749477s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T14:23:01.559541669Z I0109 14:23:01.559494       1 request.go:601] Waited for 1.393829357s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T14:23:02.559921518Z I0109 14:23:02.559877       1 request.go:601] Waited for 1.197099567s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/etcd-sa
2023-01-09T14:23:03.759049968Z I0109 14:23:03.759005       1 request.go:601] Waited for 1.193167834s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/services/etcd
2023-01-09T14:25:13.163978567Z I0109 14:25:13.163940       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 40.25 %, dbSize: 102596608
2023-01-09T14:25:13.163978567Z I0109 14:25:13.163958       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 40.15 %, dbSize: 102473728
2023-01-09T14:25:13.163978567Z I0109 14:25:13.163963       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 39.51 %, dbSize: 101412864
2023-01-09T14:32:59.516117844Z I0109 14:32:59.516079       1 request.go:601] Waited for 1.195779243s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T14:33:00.516570094Z I0109 14:33:00.516528       1 request.go:601] Waited for 1.396514041s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:33:01.715793070Z I0109 14:33:01.715759       1 request.go:601] Waited for 1.196290482s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:33:03.116128878Z I0109 14:33:03.116092       1 request.go:601] Waited for 1.170551001s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:36:13.183275718Z I0109 14:36:13.183236       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 38.75 %, dbSize: 103555072
2023-01-09T14:36:13.183275718Z I0109 14:36:13.183254       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 38.40 %, dbSize: 102952960
2023-01-09T14:36:13.183275718Z I0109 14:36:13.183259       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 36.93 %, dbSize: 101412864
2023-01-09T14:42:59.400615845Z I0109 14:42:59.400573       1 request.go:601] Waited for 1.080456918s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:43:00.401209779Z I0109 14:43:00.401171       1 request.go:601] Waited for 1.390663789s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T14:43:01.601133655Z I0109 14:43:01.601085       1 request.go:601] Waited for 1.394662651s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T14:43:03.000727687Z I0109 14:43:03.000687       1 request.go:601] Waited for 1.054530058s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:43:04.000907895Z I0109 14:43:04.000867       1 request.go:601] Waited for 1.194988932s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:47:13.150668462Z I0109 14:47:13.150626       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 36.82 %, dbSize: 103555072
2023-01-09T14:47:13.150668462Z I0109 14:47:13.150643       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 36.41 %, dbSize: 102952960
2023-01-09T14:47:13.150668462Z I0109 14:47:13.150648       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 35.42 %, dbSize: 101412864
2023-01-09T14:47:26.970348694Z I0109 14:47:26.970312       1 etcdcli_pool.go:70] creating a new cached client
2023-01-09T14:47:27.058929389Z I0109 14:47:27.058892       1 etcdcli_pool.go:157] closing cached client
2023-01-09T14:52:59.405243954Z I0109 14:52:59.405205       1 request.go:601] Waited for 1.082674935s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T14:53:00.406120835Z I0109 14:53:00.406077       1 request.go:601] Waited for 1.396441657s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:53:01.605961390Z I0109 14:53:01.605922       1 request.go:601] Waited for 1.193217666s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T14:53:03.005569462Z I0109 14:53:03.005531       1 request.go:601] Waited for 1.058915473s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T14:53:04.005642833Z I0109 14:53:04.005607       1 request.go:601] Waited for 1.196891019s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T14:58:13.145151578Z I0109 14:58:13.145109       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 34.66 %, dbSize: 103555072
2023-01-09T14:58:13.145151578Z I0109 14:58:13.145132       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 34.64 %, dbSize: 103522304
2023-01-09T14:58:13.145151578Z I0109 14:58:13.145140       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 33.24 %, dbSize: 101412864
2023-01-09T15:02:59.479871784Z I0109 15:02:59.479834       1 request.go:601] Waited for 1.157743608s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T15:03:00.680090321Z I0109 15:03:00.680051       1 request.go:601] Waited for 1.397614777s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:03:01.879703022Z I0109 15:03:01.879664       1 request.go:601] Waited for 1.19703374s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:03:03.079532950Z I0109 15:03:03.079496       1 request.go:601] Waited for 1.132593459s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T15:09:13.149592390Z I0109 15:09:13.149547       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 27.79 %, dbSize: 103555072
2023-01-09T15:09:13.149592390Z I0109 15:09:13.149569       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 27.76 %, dbSize: 103522304
2023-01-09T15:09:13.149592390Z I0109 15:09:13.149576       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 26.27 %, dbSize: 101412864
2023-01-09T15:12:59.400973753Z I0109 15:12:59.400934       1 request.go:601] Waited for 1.077725887s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T15:13:00.401109719Z I0109 15:13:00.401069       1 request.go:601] Waited for 1.39517114s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T15:13:01.401200104Z I0109 15:13:01.401163       1 request.go:601] Waited for 1.356657645s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T15:13:02.601055558Z I0109 15:13:02.601014       1 request.go:601] Waited for 1.196327677s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/restore-etcd-pod
2023-01-09T15:13:03.801510916Z I0109 15:13:03.801468       1 request.go:601] Waited for 1.159240345s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-pod
2023-01-09T15:20:13.153551351Z I0109 15:20:13.153514       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 39.71 %, dbSize: 103555072
2023-01-09T15:20:13.153551351Z I0109 15:20:13.153530       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 39.67 %, dbSize: 103522304
2023-01-09T15:20:13.153551351Z I0109 15:20:13.153535       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 38.49 %, dbSize: 101412864
2023-01-09T15:22:59.406560686Z I0109 15:22:59.406501       1 request.go:601] Waited for 1.081552589s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T15:23:00.406610547Z I0109 15:23:00.406567       1 request.go:601] Waited for 1.396960298s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:23:01.606290906Z I0109 15:23:01.606254       1 request.go:601] Waited for 1.195918771s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-145-4.us-east-2.compute.internal
2023-01-09T15:23:03.006704629Z I0109 15:23:03.006659       1 request.go:601] Waited for 1.058396993s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
2023-01-09T15:23:04.006799397Z I0109 15:23:04.006758       1 request.go:601] Waited for 1.195896224s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-160-211.us-east-2.compute.internal
2023-01-09T15:31:13.148380700Z I0109 15:31:13.148339       1 defragcontroller.go:289] etcd member "ip-10-0-160-211.us-east-2.compute.internal" backend store fragmented: 37.85 %, dbSize: 103555072
2023-01-09T15:31:13.148380700Z I0109 15:31:13.148356       1 defragcontroller.go:289] etcd member "ip-10-0-145-4.us-east-2.compute.internal" backend store fragmented: 37.83 %, dbSize: 103522304
2023-01-09T15:31:13.148380700Z I0109 15:31:13.148362       1 defragcontroller.go:289] etcd member "ip-10-0-199-219.us-east-2.compute.internal" backend store fragmented: 36.48 %, dbSize: 101412864
2023-01-09T15:32:59.369292693Z I0109 15:32:59.369250       1 request.go:601] Waited for 1.044767198s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods/etcd-ip-10-0-199-219.us-east-2.compute.internal
2023-01-09T15:33:00.568760387Z I0109 15:33:00.568715       1 request.go:601] Waited for 1.395228721s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/pods?labelSelector=app%3Dinstaller
2023-01-09T15:33:01.572200137Z I0109 15:33:01.572150       1 request.go:601] Waited for 1.398552559s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/serviceaccounts/installer-sa
2023-01-09T15:33:02.969353576Z I0109 15:33:02.969315       1 request.go:601] Waited for 1.021061936s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/api/v1/namespaces/openshift-etcd/configmaps/etcd-scripts
